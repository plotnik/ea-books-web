= 23-08 Deep Learning with Python - 2nd Edition
Fran√ßois Chollet
:toc:

== 1 What is deep learning?

=== 1.1 Artificial intelligence, machine learning, and deep learning

The layer's function in processing input data is determined by its _weights_, which are numerical values. Technically, the layer's operation is _parameterized_ by these weights, which can also be referred to as the layer's _parameters_.

_Learning_ in this context involves identifying the proper values for the weights across all layers in a network so that the network accurately associates input examples with their corresponding targets.

To manage the output of a neural network, it's essential to monitor it, which is done through a _loss function_. The loss function evaluates the network's predictions against the desired targets by calculating a distance score, indicating the network's performance on a given example. This function is also known as the _objective function_ or _cost function_.

=== 1.2 Before deep learning - A brief history of machine learning

_Probabilistic modeling_ uses statistical principles for data analysis and is foundational in machine learning. The Naive Bayes algorithm is a well-known example that applies Bayes' theorem with the assumption that all input features are independent. This technique has been in use since before the advent of computers, with its principles dating back to the 18th century. _Logistic regression_ is another related model which, despite its name, is used for classification tasks. It is also an old but still relevant technique, commonly used as an initial approach by data scientists for its simplicity and versatility.

Deep learning has gained popularity not only because of its superior performance on many tasks but also because it simplifies problem-solving by eliminating the need for _feature engineering_. In traditional machine learning methods, called shallow learning, humans had to manually create and fine-tune features from the data for the algorithms to work effectively. Deep learning, however, automates this process by learning all necessary features in a single pass through a multi-layered network structure.

Shallow learning methods like SVMs or decision trees only transform data into one or two layers of representation, which often isn't sufficient for complex problems. Attempting to apply shallow methods in succession to mimic deep learning is inefficient because each layer's optimal representation isn't the same across different model complexities. Deep learning's transformative approach is its ability to _learn multiple layers of representation jointly_. This means that all layers adjust together in response to a single feedback signal, allowing the model to develop increasingly complex and abstract representations through a series of simple transformations across layers.

The two key aspects of how deep learning learns from data are the _incremental development of more complex representations through multiple layers_ and the _joint learning of these representations_, which enables each layer to be refined in coordination with the others. These properties have made deep learning much more effective than previous machine learning methods.

From 2016 to 2020, the machine learning and data science field has been primarily focused on two methodologies: deep learning for perceptual tasks like image classification, and gradient boosted trees for structured data problems. Practitioners use Python libraries such as Scikit-learn, XGBoost, and LightGBM for gradient boosted trees and Keras, often with TensorFlow, for deep learning. Proficiency in these techniques and tools is considered crucial for success in applied machine learning, and they are frequently used in Kaggle competitions.

=== 1.3 Why deep learning - Why now

Deep learning is considered a significant advancement in AI with several properties that make it a robust and long-lasting approach, likely to influence future technologies. These properties include:

1. _Simplicity:_ Deep learning simplifies model building by eliminating the need for feature engineering, allowing for end-to-end training with a limited set of operations.

2. _Scalability:_ It benefits from GPU or TPU parallelization and can handle vast datasets thanks to batch-based training, making it compatible with the ongoing improvements in computational power.

3. _Versatility and Reusability:_ Deep learning models can be continuously updated with new data and repurposed for different tasks, making them adaptable and efficient for various applications, even with smaller datasets.

While deep learning has been prominent for only a few years, its potential is still unfolding with new use cases and improvements. The field saw rapid progress initially, which is now stabilizing into more incremental advancements. Innovations like Transformer-based models in natural language processing have marked significant milestones. As of 2021, deep learning may be entering a phase of more steady and less explosive growth, but significant developments are still anticipated in the coming years. The deployment of deep learning across numerous applications is expected to continue, as its full potential has yet to be fully realized.

