= 23-08 Deep Learning with Python - 2nd Edition
Fran√ßois Chollet
:toc:

== 1 What is deep learning?

=== 1.1 Artificial intelligence, machine learning, and deep learning

The layer's function in processing input data is determined by its _weights_, which are numerical values. Technically, the layer's operation is _parameterized_ by these weights, which can also be referred to as the layer's _parameters_.

_Learning_ in this context involves identifying the proper values for the weights across all layers in a network so that the network accurately associates input examples with their corresponding targets.

To manage the output of a neural network, it's essential to monitor it, which is done through a _loss function_. The loss function evaluates the network's predictions against the desired targets by calculating a distance score, indicating the network's performance on a given example. This function is also known as the _objective function_ or _cost function_.

=== 1.2 Before deep learning - A brief history of machine learning

_Probabilistic modeling_ uses statistical principles for data analysis and is foundational in machine learning. The Naive Bayes algorithm is a well-known example that applies Bayes' theorem with the assumption that all input features are independent. This technique has been in use since before the advent of computers, with its principles dating back to the 18th century. _Logistic regression_ is another related model which, despite its name, is used for classification tasks. It is also an old but still relevant technique, commonly used as an initial approach by data scientists for its simplicity and versatility.

Deep learning has gained popularity not only because of its superior performance on many tasks but also because it simplifies problem-solving by eliminating the need for _feature engineering_. In traditional machine learning methods, called shallow learning, humans had to manually create and fine-tune features from the data for the algorithms to work effectively. Deep learning, however, automates this process by learning all necessary features in a single pass through a multi-layered network structure.

Shallow learning methods like SVMs or decision trees only transform data into one or two layers of representation, which often isn't sufficient for complex problems. Attempting to apply shallow methods in succession to mimic deep learning is inefficient because each layer's optimal representation isn't the same across different model complexities. Deep learning's transformative approach is its ability to _learn multiple layers of representation jointly_. This means that all layers adjust together in response to a single feedback signal, allowing the model to develop increasingly complex and abstract representations through a series of simple transformations across layers.

The two key aspects of how deep learning learns from data are the _incremental development of more complex representations through multiple layers_ and the _joint learning of these representations_, which enables each layer to be refined in coordination with the others. These properties have made deep learning much more effective than previous machine learning methods.

From 2016 to 2020, the machine learning and data science field has been primarily focused on two methodologies: deep learning for perceptual tasks like image classification, and gradient boosted trees for structured data problems. Practitioners use Python libraries such as Scikit-learn, XGBoost, and LightGBM for gradient boosted trees and Keras, often with TensorFlow, for deep learning. Proficiency in these techniques and tools is considered crucial for success in applied machine learning, and they are frequently used in Kaggle competitions.

