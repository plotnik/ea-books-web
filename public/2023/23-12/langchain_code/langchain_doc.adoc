= LangChain Docs
:source-highlighter: coderay
:toc: right
:icons: font


== Get started

=== Introduction

> https://python.langchain.com/docs/get_started/introduction

LangChain is a framework designed for creating applications that utilize language models. It focuses on developing applications that are context-aware, capable of reasoning, and can be connected to various contextual sources.

The framework is composed of:

- **LangChain Libraries**: These are available in Python and JavaScript and include interfaces, integrations, and built-in components for creating chains and agents.
- **LangChain Templates**: A set of reference architectures for different tasks that can be deployed easily.
- **LangServe**: A tool for deploying LangChain chains as REST APIs.
- **LangSmith**: A platform that assists developers in debugging, testing, evaluating, and monitoring LangChain-based chains.

The framework offers the following benefits:

1. Components: These are modular tools that can be used with language models, whether within the LangChain framework or independently.
2. Off-the-shelf chains: Pre-built configurations designed to perform higher-level tasks, allowing for quick setup and customization.

The LangChain libraries consist of several packages, including `langchain-core`, `langchain-community`, and `langchain` itself, which contain the core abstractions, third-party integrations, and cognitive architecture components respectively.

To get started with LangChain, the documentation provides guidance on installation, a Quickstart guide, and security best practices. The documentation is focused on the Python library of LangChain, with a separate link provided for the JavaScript version.

LangChain also introduces the LangChain Expression Language (LCEL), which is a declarative method to compose chains that can scale from simple to complex applications without code changes.

Furthermore, LangChain provides interfaces and integrations for modules such as Model I/O, Retrieval, and Agents, which cover interacting with language models, accessing data, and enabling models to choose tools based on directives.

The ecosystem includes use cases with walkthroughs for common applications like question answering and chatbots, integrations with other tools, guides for best practices, an API reference for detailed documentation, a developer's guide, and a community section for engagement with other developers and contributors.

=== Quickstart

The provided text outlines the process of setting up and using the OpenAI API for various tasks, including creating chatbots and retrieval systems using Python. Here is a summary of the steps and concepts discussed:

1. **Installation and Initialization**:
    - Install the OpenAI Python package.
    - Obtain an API key and set it as an environment variable or pass it directly to the `ChatOpenAI` class upon instantiation.
    - Initialize the `ChatOpenAI` model and use it to interact with the OpenAI API.

2. **Using Prompts and Output Parsers**:
    - Create prompt templates to guide the language model's responses, ensuring they match the tone of a technical documentation writer.
    - Combine prompt templates with the language model to form a chain.
    - Parse the chat model's output into a string using `StrOutputParser`.

3. **Retrieval Chain**:
    - Set up a retrieval system to provide the language model with relevant context for answering questions.
    - Load and index documents into a vector store using an embedding model and a retriever.
    - Create a retrieval chain that takes a question, retrieves relevant documents, and generates an answer from the language model.

4. **Conversation Retrieval Chain**:
    - Adapt the retrieval system to handle conversations and follow-up questions.
    - Implement a retriever that considers the entire conversation history to generate a search query.
    - Update the language model chain to use the retriever and provide coherent answers in a chatbot context.

5. **Agent**:
    - Build an agent that decides the steps to take when interacting with a user.
    - Set up tools for the agent to use, such as a retriever and a search tool.
    - Deploy the agent using predefined prompts and an executor to handle user inputs and provide responses.

Throughout the text, the reader is encouraged to refer to the LangChain documentation for a deeper understanding of the concepts and for more advanced configurations. The examples provided demonstrate the creation of a system capable of answering questions with context, handling conversations, and building a functional chatbot using the OpenAI API and additional tools and libraries.

== LangChain Expression Language

> https://python.langchain.com/docs/expression_language/get_started

LCEL is a framework that allows for the easy construction of complex workflows by chaining together basic components, which can include features like streaming, parallelism, and logging.

The first example provided demonstrates how to generate a joke on a specified topic by chaining a prompt template, a language model, and an output parser. This is done using the `|` operator, akin to a Unix pipe, to pass the output of one component as the input to the next. The process involves creating a prompt using the user's topic, invoking a model (such as ChatOpenAI) to generate a response based on the prompt, and then parsing the model's output into a string.

In the second example, a more complex chain is built for retrieval-augmented generation (RAG). This involves setting up an in-memory document store with relevant texts, using a retriever to fetch documents based on a query, and then constructing a prompt that combines the retrieved documents (as context) with the user's question. This prompt is then fed into a language model to generate a relevant answer, which is finally parsed into a string. The `RunnableParallel` and `RunnablePassthrough` components are used to manage the parallel inputs (context and question) for the prompt template.

Both examples illustrate how LCEL can streamline the process of creating sophisticated workflows by integrating different components and managing the flow of data between them.

