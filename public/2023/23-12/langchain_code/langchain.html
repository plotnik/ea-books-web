<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Asciidoctor 2.0.17">
<meta name="author" content="Ben Auffarth">
<title>23-12 Generative AI with LangChain</title>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700">
<style>
/*! Asciidoctor default stylesheet | MIT License | https://asciidoctor.org */
/* Uncomment the following line when using as a custom stylesheet */
/* @import "https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700"; */
html{font-family:sans-serif;-webkit-text-size-adjust:100%}
a{background:none}
a:focus{outline:thin dotted}
a:active,a:hover{outline:0}
h1{font-size:2em;margin:.67em 0}
b,strong{font-weight:bold}
abbr{font-size:.9em}
abbr[title]{cursor:help;border-bottom:1px dotted #dddddf;text-decoration:none}
dfn{font-style:italic}
hr{height:0}
mark{background:#ff0;color:#000}
code,kbd,pre,samp{font-family:monospace;font-size:1em}
pre{white-space:pre-wrap}
q{quotes:"\201C" "\201D" "\2018" "\2019"}
small{font-size:80%}
sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}
sup{top:-.5em}
sub{bottom:-.25em}
img{border:0}
svg:not(:root){overflow:hidden}
figure{margin:0}
audio,video{display:inline-block}
audio:not([controls]){display:none;height:0}
fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}
legend{border:0;padding:0}
button,input,select,textarea{font-family:inherit;font-size:100%;margin:0}
button,input{line-height:normal}
button,select{text-transform:none}
button,html input[type=button],input[type=reset],input[type=submit]{-webkit-appearance:button;cursor:pointer}
button[disabled],html input[disabled]{cursor:default}
input[type=checkbox],input[type=radio]{padding:0}
button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}
textarea{overflow:auto;vertical-align:top}
table{border-collapse:collapse;border-spacing:0}
*,::before,::after{box-sizing:border-box}
html,body{font-size:100%}
body{background:#fff;color:rgba(0,0,0,.8);padding:0;margin:0;font-family:"Noto Serif","DejaVu Serif",serif;line-height:1;position:relative;cursor:auto;-moz-tab-size:4;-o-tab-size:4;tab-size:4;word-wrap:anywhere;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased}
a:hover{cursor:pointer}
img,object,embed{max-width:100%;height:auto}
object,embed{height:100%}
img{-ms-interpolation-mode:bicubic}
.left{float:left!important}
.right{float:right!important}
.text-left{text-align:left!important}
.text-right{text-align:right!important}
.text-center{text-align:center!important}
.text-justify{text-align:justify!important}
.hide{display:none}
img,object,svg{display:inline-block;vertical-align:middle}
textarea{height:auto;min-height:50px}
select{width:100%}
.subheader,.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{line-height:1.45;color:#7a2518;font-weight:400;margin-top:0;margin-bottom:.25em}
div,dl,dt,dd,ul,ol,li,h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6,pre,form,p,blockquote,th,td{margin:0;padding:0}
a{color:#2156a5;text-decoration:underline;line-height:inherit}
a:hover,a:focus{color:#1d4b8f}
a img{border:0}
p{line-height:1.6;margin-bottom:1.25em;text-rendering:optimizeLegibility}
p aside{font-size:.875em;line-height:1.35;font-style:italic}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{font-family:"Open Sans","DejaVu Sans",sans-serif;font-weight:300;font-style:normal;color:#ba3925;text-rendering:optimizeLegibility;margin-top:1em;margin-bottom:.5em;line-height:1.0125em}
h1 small,h2 small,h3 small,#toctitle small,.sidebarblock>.content>.title small,h4 small,h5 small,h6 small{font-size:60%;color:#e99b8f;line-height:0}
h1{font-size:2.125em}
h2{font-size:1.6875em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.375em}
h4,h5{font-size:1.125em}
h6{font-size:1em}
hr{border:solid #dddddf;border-width:1px 0 0;clear:both;margin:1.25em 0 1.1875em}
em,i{font-style:italic;line-height:inherit}
strong,b{font-weight:bold;line-height:inherit}
small{font-size:60%;line-height:inherit}
code{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;font-weight:400;color:rgba(0,0,0,.9)}
ul,ol,dl{line-height:1.6;margin-bottom:1.25em;list-style-position:outside;font-family:inherit}
ul,ol{margin-left:1.5em}
ul li ul,ul li ol{margin-left:1.25em;margin-bottom:0}
ul.square li ul,ul.circle li ul,ul.disc li ul{list-style:inherit}
ul.square{list-style-type:square}
ul.circle{list-style-type:circle}
ul.disc{list-style-type:disc}
ol li ul,ol li ol{margin-left:1.25em;margin-bottom:0}
dl dt{margin-bottom:.3125em;font-weight:bold}
dl dd{margin-bottom:1.25em}
blockquote{margin:0 0 1.25em;padding:.5625em 1.25em 0 1.1875em;border-left:1px solid #ddd}
blockquote,blockquote p{line-height:1.6;color:rgba(0,0,0,.85)}
@media screen and (min-width:768px){h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2}
h1{font-size:2.75em}
h2{font-size:2.3125em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.6875em}
h4{font-size:1.4375em}}
table{background:#fff;margin-bottom:1.25em;border:1px solid #dedede;word-wrap:normal}
table thead,table tfoot{background:#f7f8f7}
table thead tr th,table thead tr td,table tfoot tr th,table tfoot tr td{padding:.5em .625em .625em;font-size:inherit;color:rgba(0,0,0,.8);text-align:left}
table tr th,table tr td{padding:.5625em .625em;font-size:inherit;color:rgba(0,0,0,.8)}
table tr.even,table tr.alt{background:#f8f8f7}
table thead tr th,table tfoot tr th,table tbody tr td,table tr td,table tfoot tr td{line-height:1.6}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2;word-spacing:-.05em}
h1 strong,h2 strong,h3 strong,#toctitle strong,.sidebarblock>.content>.title strong,h4 strong,h5 strong,h6 strong{font-weight:400}
.center{margin-left:auto;margin-right:auto}
.stretch{width:100%}
.clearfix::before,.clearfix::after,.float-group::before,.float-group::after{content:" ";display:table}
.clearfix::after,.float-group::after{clear:both}
:not(pre).nobreak{word-wrap:normal}
:not(pre).nowrap{white-space:nowrap}
:not(pre).pre-wrap{white-space:pre-wrap}
:not(pre):not([class^=L])>code{font-size:.9375em;font-style:normal!important;letter-spacing:0;padding:.1em .5ex;word-spacing:-.15em;background:#f7f7f8;border-radius:4px;line-height:1.45;text-rendering:optimizeSpeed}
pre{color:rgba(0,0,0,.9);font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;line-height:1.45;text-rendering:optimizeSpeed}
pre code,pre pre{color:inherit;font-size:inherit;line-height:inherit}
pre>code{display:block}
pre.nowrap,pre.nowrap pre{white-space:pre;word-wrap:normal}
em em{font-style:normal}
strong strong{font-weight:400}
.keyseq{color:rgba(51,51,51,.8)}
kbd{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;display:inline-block;color:rgba(0,0,0,.8);font-size:.65em;line-height:1.45;background:#f7f7f7;border:1px solid #ccc;border-radius:3px;box-shadow:0 1px 0 rgba(0,0,0,.2),inset 0 0 0 .1em #fff;margin:0 .15em;padding:.2em .5em;vertical-align:middle;position:relative;top:-.1em;white-space:nowrap}
.keyseq kbd:first-child{margin-left:0}
.keyseq kbd:last-child{margin-right:0}
.menuseq,.menuref{color:#000}
.menuseq b:not(.caret),.menuref{font-weight:inherit}
.menuseq{word-spacing:-.02em}
.menuseq b.caret{font-size:1.25em;line-height:.8}
.menuseq i.caret{font-weight:bold;text-align:center;width:.45em}
b.button::before,b.button::after{position:relative;top:-1px;font-weight:400}
b.button::before{content:"[";padding:0 3px 0 2px}
b.button::after{content:"]";padding:0 2px 0 3px}
p a>code:hover{color:rgba(0,0,0,.9)}
#header,#content,#footnotes,#footer{width:100%;margin:0 auto;max-width:62.5em;*zoom:1;position:relative;padding-left:.9375em;padding-right:.9375em}
#header::before,#header::after,#content::before,#content::after,#footnotes::before,#footnotes::after,#footer::before,#footer::after{content:" ";display:table}
#header::after,#content::after,#footnotes::after,#footer::after{clear:both}
#content{margin-top:1.25em}
#content::before{content:none}
#header>h1:first-child{color:rgba(0,0,0,.85);margin-top:2.25rem;margin-bottom:0}
#header>h1:first-child+#toc{margin-top:8px;border-top:1px solid #dddddf}
#header>h1:only-child,body.toc2 #header>h1:nth-last-child(2){border-bottom:1px solid #dddddf;padding-bottom:8px}
#header .details{border-bottom:1px solid #dddddf;line-height:1.45;padding-top:.25em;padding-bottom:.25em;padding-left:.25em;color:rgba(0,0,0,.6);display:flex;flex-flow:row wrap}
#header .details span:first-child{margin-left:-.125em}
#header .details span.email a{color:rgba(0,0,0,.85)}
#header .details br{display:none}
#header .details br+span::before{content:"\00a0\2013\00a0"}
#header .details br+span.author::before{content:"\00a0\22c5\00a0";color:rgba(0,0,0,.85)}
#header .details br+span#revremark::before{content:"\00a0|\00a0"}
#header #revnumber{text-transform:capitalize}
#header #revnumber::after{content:"\00a0"}
#content>h1:first-child:not([class]){color:rgba(0,0,0,.85);border-bottom:1px solid #dddddf;padding-bottom:8px;margin-top:0;padding-top:1rem;margin-bottom:1.25rem}
#toc{border-bottom:1px solid #e7e7e9;padding-bottom:.5em}
#toc>ul{margin-left:.125em}
#toc ul.sectlevel0>li>a{font-style:italic}
#toc ul.sectlevel0 ul.sectlevel1{margin:.5em 0}
#toc ul{font-family:"Open Sans","DejaVu Sans",sans-serif;list-style-type:none}
#toc li{line-height:1.3334;margin-top:.3334em}
#toc a{text-decoration:none}
#toc a:active{text-decoration:underline}
#toctitle{color:#7a2518;font-size:1.2em}
@media screen and (min-width:768px){#toctitle{font-size:1.375em}
body.toc2{padding-left:15em;padding-right:0}
#toc.toc2{margin-top:0!important;background:#f8f8f7;position:fixed;width:15em;left:0;top:0;border-right:1px solid #e7e7e9;border-top-width:0!important;border-bottom-width:0!important;z-index:1000;padding:1.25em 1em;height:100%;overflow:auto}
#toc.toc2 #toctitle{margin-top:0;margin-bottom:.8rem;font-size:1.2em}
#toc.toc2>ul{font-size:.9em;margin-bottom:0}
#toc.toc2 ul ul{margin-left:0;padding-left:1em}
#toc.toc2 ul.sectlevel0 ul.sectlevel1{padding-left:0;margin-top:.5em;margin-bottom:.5em}
body.toc2.toc-right{padding-left:0;padding-right:15em}
body.toc2.toc-right #toc.toc2{border-right-width:0;border-left:1px solid #e7e7e9;left:auto;right:0}}
@media screen and (min-width:1280px){body.toc2{padding-left:20em;padding-right:0}
#toc.toc2{width:20em}
#toc.toc2 #toctitle{font-size:1.375em}
#toc.toc2>ul{font-size:.95em}
#toc.toc2 ul ul{padding-left:1.25em}
body.toc2.toc-right{padding-left:0;padding-right:20em}}
#content #toc{border:1px solid #e0e0dc;margin-bottom:1.25em;padding:1.25em;background:#f8f8f7;border-radius:4px}
#content #toc>:first-child{margin-top:0}
#content #toc>:last-child{margin-bottom:0}
#footer{max-width:none;background:rgba(0,0,0,.8);padding:1.25em}
#footer-text{color:hsla(0,0%,100%,.8);line-height:1.44}
#content{margin-bottom:.625em}
.sect1{padding-bottom:.625em}
@media screen and (min-width:768px){#content{margin-bottom:1.25em}
.sect1{padding-bottom:1.25em}}
.sect1:last-child{padding-bottom:0}
.sect1+.sect1{border-top:1px solid #e7e7e9}
#content h1>a.anchor,h2>a.anchor,h3>a.anchor,#toctitle>a.anchor,.sidebarblock>.content>.title>a.anchor,h4>a.anchor,h5>a.anchor,h6>a.anchor{position:absolute;z-index:1001;width:1.5ex;margin-left:-1.5ex;display:block;text-decoration:none!important;visibility:hidden;text-align:center;font-weight:400}
#content h1>a.anchor::before,h2>a.anchor::before,h3>a.anchor::before,#toctitle>a.anchor::before,.sidebarblock>.content>.title>a.anchor::before,h4>a.anchor::before,h5>a.anchor::before,h6>a.anchor::before{content:"\00A7";font-size:.85em;display:block;padding-top:.1em}
#content h1:hover>a.anchor,#content h1>a.anchor:hover,h2:hover>a.anchor,h2>a.anchor:hover,h3:hover>a.anchor,#toctitle:hover>a.anchor,.sidebarblock>.content>.title:hover>a.anchor,h3>a.anchor:hover,#toctitle>a.anchor:hover,.sidebarblock>.content>.title>a.anchor:hover,h4:hover>a.anchor,h4>a.anchor:hover,h5:hover>a.anchor,h5>a.anchor:hover,h6:hover>a.anchor,h6>a.anchor:hover{visibility:visible}
#content h1>a.link,h2>a.link,h3>a.link,#toctitle>a.link,.sidebarblock>.content>.title>a.link,h4>a.link,h5>a.link,h6>a.link{color:#ba3925;text-decoration:none}
#content h1>a.link:hover,h2>a.link:hover,h3>a.link:hover,#toctitle>a.link:hover,.sidebarblock>.content>.title>a.link:hover,h4>a.link:hover,h5>a.link:hover,h6>a.link:hover{color:#a53221}
details,.audioblock,.imageblock,.literalblock,.listingblock,.stemblock,.videoblock{margin-bottom:1.25em}
details{margin-left:1.25rem}
details>summary{cursor:pointer;display:block;position:relative;line-height:1.6;margin-bottom:.625rem;outline:none;-webkit-tap-highlight-color:transparent}
details>summary::-webkit-details-marker{display:none}
details>summary::before{content:"";border:solid transparent;border-left:solid;border-width:.3em 0 .3em .5em;position:absolute;top:.5em;left:-1.25rem;transform:translateX(15%)}
details[open]>summary::before{border:solid transparent;border-top:solid;border-width:.5em .3em 0;transform:translateY(15%)}
details>summary::after{content:"";width:1.25rem;height:1em;position:absolute;top:.3em;left:-1.25rem}
.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{text-rendering:optimizeLegibility;text-align:left;font-family:"Noto Serif","DejaVu Serif",serif;font-size:1rem;font-style:italic}
table.tableblock.fit-content>caption.title{white-space:nowrap;width:0}
.paragraph.lead>p,#preamble>.sectionbody>[class=paragraph]:first-of-type p{font-size:1.21875em;line-height:1.6;color:rgba(0,0,0,.85)}
.admonitionblock>table{border-collapse:separate;border:0;background:none;width:100%}
.admonitionblock>table td.icon{text-align:center;width:80px}
.admonitionblock>table td.icon img{max-width:none}
.admonitionblock>table td.icon .title{font-weight:bold;font-family:"Open Sans","DejaVu Sans",sans-serif;text-transform:uppercase}
.admonitionblock>table td.content{padding-left:1.125em;padding-right:1.25em;border-left:1px solid #dddddf;color:rgba(0,0,0,.6);word-wrap:anywhere}
.admonitionblock>table td.content>:last-child>:last-child{margin-bottom:0}
.exampleblock>.content{border:1px solid #e6e6e6;margin-bottom:1.25em;padding:1.25em;background:#fff;border-radius:4px}
.exampleblock>.content>:first-child{margin-top:0}
.exampleblock>.content>:last-child{margin-bottom:0}
.sidebarblock{border:1px solid #dbdbd6;margin-bottom:1.25em;padding:1.25em;background:#f3f3f2;border-radius:4px}
.sidebarblock>:first-child{margin-top:0}
.sidebarblock>:last-child{margin-bottom:0}
.sidebarblock>.content>.title{color:#7a2518;margin-top:0;text-align:center}
.exampleblock>.content>:last-child>:last-child,.exampleblock>.content .olist>ol>li:last-child>:last-child,.exampleblock>.content .ulist>ul>li:last-child>:last-child,.exampleblock>.content .qlist>ol>li:last-child>:last-child,.sidebarblock>.content>:last-child>:last-child,.sidebarblock>.content .olist>ol>li:last-child>:last-child,.sidebarblock>.content .ulist>ul>li:last-child>:last-child,.sidebarblock>.content .qlist>ol>li:last-child>:last-child{margin-bottom:0}
.literalblock pre,.listingblock>.content>pre{border-radius:4px;overflow-x:auto;padding:1em;font-size:.8125em}
@media screen and (min-width:768px){.literalblock pre,.listingblock>.content>pre{font-size:.90625em}}
@media screen and (min-width:1280px){.literalblock pre,.listingblock>.content>pre{font-size:1em}}
.literalblock pre,.listingblock>.content>pre:not(.highlight),.listingblock>.content>pre[class=highlight],.listingblock>.content>pre[class^="highlight "]{background:#f7f7f8}
.literalblock.output pre{color:#f7f7f8;background:rgba(0,0,0,.9)}
.listingblock>.content{position:relative}
.listingblock code[data-lang]::before{display:none;content:attr(data-lang);position:absolute;font-size:.75em;top:.425rem;right:.5rem;line-height:1;text-transform:uppercase;color:inherit;opacity:.5}
.listingblock:hover code[data-lang]::before{display:block}
.listingblock.terminal pre .command::before{content:attr(data-prompt);padding-right:.5em;color:inherit;opacity:.5}
.listingblock.terminal pre .command:not([data-prompt])::before{content:"$"}
.listingblock pre.highlightjs{padding:0}
.listingblock pre.highlightjs>code{padding:1em;border-radius:4px}
.listingblock pre.prettyprint{border-width:0}
.prettyprint{background:#f7f7f8}
pre.prettyprint .linenums{line-height:1.45;margin-left:2em}
pre.prettyprint li{background:none;list-style-type:inherit;padding-left:0}
pre.prettyprint li code[data-lang]::before{opacity:1}
pre.prettyprint li:not(:first-child) code[data-lang]::before{display:none}
table.linenotable{border-collapse:separate;border:0;margin-bottom:0;background:none}
table.linenotable td[class]{color:inherit;vertical-align:top;padding:0;line-height:inherit;white-space:normal}
table.linenotable td.code{padding-left:.75em}
table.linenotable td.linenos,pre.pygments .linenos{border-right:1px solid;opacity:.35;padding-right:.5em;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}
pre.pygments span.linenos{display:inline-block;margin-right:.75em}
.quoteblock{margin:0 1em 1.25em 1.5em;display:table}
.quoteblock:not(.excerpt)>.title{margin-left:-1.5em;margin-bottom:.75em}
.quoteblock blockquote,.quoteblock p{color:rgba(0,0,0,.85);font-size:1.15rem;line-height:1.75;word-spacing:.1em;letter-spacing:0;font-style:italic;text-align:justify}
.quoteblock blockquote{margin:0;padding:0;border:0}
.quoteblock blockquote::before{content:"\201c";float:left;font-size:2.75em;font-weight:bold;line-height:.6em;margin-left:-.6em;color:#7a2518;text-shadow:0 1px 2px rgba(0,0,0,.1)}
.quoteblock blockquote>.paragraph:last-child p{margin-bottom:0}
.quoteblock .attribution{margin-top:.75em;margin-right:.5ex;text-align:right}
.verseblock{margin:0 1em 1.25em}
.verseblock pre{font-family:"Open Sans","DejaVu Sans",sans-serif;font-size:1.15rem;color:rgba(0,0,0,.85);font-weight:300;text-rendering:optimizeLegibility}
.verseblock pre strong{font-weight:400}
.verseblock .attribution{margin-top:1.25rem;margin-left:.5ex}
.quoteblock .attribution,.verseblock .attribution{font-size:.9375em;line-height:1.45;font-style:italic}
.quoteblock .attribution br,.verseblock .attribution br{display:none}
.quoteblock .attribution cite,.verseblock .attribution cite{display:block;letter-spacing:-.025em;color:rgba(0,0,0,.6)}
.quoteblock.abstract blockquote::before,.quoteblock.excerpt blockquote::before,.quoteblock .quoteblock blockquote::before{display:none}
.quoteblock.abstract blockquote,.quoteblock.abstract p,.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{line-height:1.6;word-spacing:0}
.quoteblock.abstract{margin:0 1em 1.25em;display:block}
.quoteblock.abstract>.title{margin:0 0 .375em;font-size:1.15em;text-align:center}
.quoteblock.excerpt>blockquote,.quoteblock .quoteblock{padding:0 0 .25em 1em;border-left:.25em solid #dddddf}
.quoteblock.excerpt,.quoteblock .quoteblock{margin-left:0}
.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{color:inherit;font-size:1.0625rem}
.quoteblock.excerpt .attribution,.quoteblock .quoteblock .attribution{color:inherit;font-size:.85rem;text-align:left;margin-right:0}
p.tableblock:last-child{margin-bottom:0}
td.tableblock>.content{margin-bottom:1.25em;word-wrap:anywhere}
td.tableblock>.content>:last-child{margin-bottom:-1.25em}
table.tableblock,th.tableblock,td.tableblock{border:0 solid #dedede}
table.grid-all>*>tr>*{border-width:1px}
table.grid-cols>*>tr>*{border-width:0 1px}
table.grid-rows>*>tr>*{border-width:1px 0}
table.frame-all{border-width:1px}
table.frame-ends{border-width:1px 0}
table.frame-sides{border-width:0 1px}
table.frame-none>colgroup+*>:first-child>*,table.frame-sides>colgroup+*>:first-child>*{border-top-width:0}
table.frame-none>:last-child>:last-child>*,table.frame-sides>:last-child>:last-child>*{border-bottom-width:0}
table.frame-none>*>tr>:first-child,table.frame-ends>*>tr>:first-child{border-left-width:0}
table.frame-none>*>tr>:last-child,table.frame-ends>*>tr>:last-child{border-right-width:0}
table.stripes-all>*>tr,table.stripes-odd>*>tr:nth-of-type(odd),table.stripes-even>*>tr:nth-of-type(even),table.stripes-hover>*>tr:hover{background:#f8f8f7}
th.halign-left,td.halign-left{text-align:left}
th.halign-right,td.halign-right{text-align:right}
th.halign-center,td.halign-center{text-align:center}
th.valign-top,td.valign-top{vertical-align:top}
th.valign-bottom,td.valign-bottom{vertical-align:bottom}
th.valign-middle,td.valign-middle{vertical-align:middle}
table thead th,table tfoot th{font-weight:bold}
tbody tr th{background:#f7f8f7}
tbody tr th,tbody tr th p,tfoot tr th,tfoot tr th p{color:rgba(0,0,0,.8);font-weight:bold}
p.tableblock>code:only-child{background:none;padding:0}
p.tableblock{font-size:1em}
ol{margin-left:1.75em}
ul li ol{margin-left:1.5em}
dl dd{margin-left:1.125em}
dl dd:last-child,dl dd:last-child>:last-child{margin-bottom:0}
li p,ul dd,ol dd,.olist .olist,.ulist .ulist,.ulist .olist,.olist .ulist{margin-bottom:.625em}
ul.checklist,ul.none,ol.none,ul.no-bullet,ol.no-bullet,ol.unnumbered,ul.unstyled,ol.unstyled{list-style-type:none}
ul.no-bullet,ol.no-bullet,ol.unnumbered{margin-left:.625em}
ul.unstyled,ol.unstyled{margin-left:0}
li>p:empty:only-child::before{content:"";display:inline-block}
ul.checklist>li>p:first-child{margin-left:-1em}
ul.checklist>li>p:first-child>.fa-square-o:first-child,ul.checklist>li>p:first-child>.fa-check-square-o:first-child{width:1.25em;font-size:.8em;position:relative;bottom:.125em}
ul.checklist>li>p:first-child>input[type=checkbox]:first-child{margin-right:.25em}
ul.inline{display:flex;flex-flow:row wrap;list-style:none;margin:0 0 .625em -1.25em}
ul.inline>li{margin-left:1.25em}
.unstyled dl dt{font-weight:400;font-style:normal}
ol.arabic{list-style-type:decimal}
ol.decimal{list-style-type:decimal-leading-zero}
ol.loweralpha{list-style-type:lower-alpha}
ol.upperalpha{list-style-type:upper-alpha}
ol.lowerroman{list-style-type:lower-roman}
ol.upperroman{list-style-type:upper-roman}
ol.lowergreek{list-style-type:lower-greek}
.hdlist>table,.colist>table{border:0;background:none}
.hdlist>table>tbody>tr,.colist>table>tbody>tr{background:none}
td.hdlist1,td.hdlist2{vertical-align:top;padding:0 .625em}
td.hdlist1{font-weight:bold;padding-bottom:1.25em}
td.hdlist2{word-wrap:anywhere}
.literalblock+.colist,.listingblock+.colist{margin-top:-.5em}
.colist td:not([class]):first-child{padding:.4em .75em 0;line-height:1;vertical-align:top}
.colist td:not([class]):first-child img{max-width:none}
.colist td:not([class]):last-child{padding:.25em 0}
.thumb,.th{line-height:0;display:inline-block;border:4px solid #fff;box-shadow:0 0 0 1px #ddd}
.imageblock.left{margin:.25em .625em 1.25em 0}
.imageblock.right{margin:.25em 0 1.25em .625em}
.imageblock>.title{margin-bottom:0}
.imageblock.thumb,.imageblock.th{border-width:6px}
.imageblock.thumb>.title,.imageblock.th>.title{padding:0 .125em}
.image.left,.image.right{margin-top:.25em;margin-bottom:.25em;display:inline-block;line-height:0}
.image.left{margin-right:.625em}
.image.right{margin-left:.625em}
a.image{text-decoration:none;display:inline-block}
a.image object{pointer-events:none}
sup.footnote,sup.footnoteref{font-size:.875em;position:static;vertical-align:super}
sup.footnote a,sup.footnoteref a{text-decoration:none}
sup.footnote a:active,sup.footnoteref a:active{text-decoration:underline}
#footnotes{padding-top:.75em;padding-bottom:.75em;margin-bottom:.625em}
#footnotes hr{width:20%;min-width:6.25em;margin:-.25em 0 .75em;border-width:1px 0 0}
#footnotes .footnote{padding:0 .375em 0 .225em;line-height:1.3334;font-size:.875em;margin-left:1.2em;margin-bottom:.2em}
#footnotes .footnote a:first-of-type{font-weight:bold;text-decoration:none;margin-left:-1.05em}
#footnotes .footnote:last-of-type{margin-bottom:0}
#content #footnotes{margin-top:-.625em;margin-bottom:0;padding:.75em 0}
div.unbreakable{page-break-inside:avoid}
.big{font-size:larger}
.small{font-size:smaller}
.underline{text-decoration:underline}
.overline{text-decoration:overline}
.line-through{text-decoration:line-through}
.aqua{color:#00bfbf}
.aqua-background{background:#00fafa}
.black{color:#000}
.black-background{background:#000}
.blue{color:#0000bf}
.blue-background{background:#0000fa}
.fuchsia{color:#bf00bf}
.fuchsia-background{background:#fa00fa}
.gray{color:#606060}
.gray-background{background:#7d7d7d}
.green{color:#006000}
.green-background{background:#007d00}
.lime{color:#00bf00}
.lime-background{background:#00fa00}
.maroon{color:#600000}
.maroon-background{background:#7d0000}
.navy{color:#000060}
.navy-background{background:#00007d}
.olive{color:#606000}
.olive-background{background:#7d7d00}
.purple{color:#600060}
.purple-background{background:#7d007d}
.red{color:#bf0000}
.red-background{background:#fa0000}
.silver{color:#909090}
.silver-background{background:#bcbcbc}
.teal{color:#006060}
.teal-background{background:#007d7d}
.white{color:#bfbfbf}
.white-background{background:#fafafa}
.yellow{color:#bfbf00}
.yellow-background{background:#fafa00}
span.icon>.fa{cursor:default}
a span.icon>.fa{cursor:inherit}
.admonitionblock td.icon [class^="fa icon-"]{font-size:2.5em;text-shadow:1px 1px 2px rgba(0,0,0,.5);cursor:default}
.admonitionblock td.icon .icon-note::before{content:"\f05a";color:#19407c}
.admonitionblock td.icon .icon-tip::before{content:"\f0eb";text-shadow:1px 1px 2px rgba(155,155,0,.8);color:#111}
.admonitionblock td.icon .icon-warning::before{content:"\f071";color:#bf6900}
.admonitionblock td.icon .icon-caution::before{content:"\f06d";color:#bf3400}
.admonitionblock td.icon .icon-important::before{content:"\f06a";color:#bf0000}
.conum[data-value]{display:inline-block;color:#fff!important;background:rgba(0,0,0,.8);border-radius:50%;text-align:center;font-size:.75em;width:1.67em;height:1.67em;line-height:1.67em;font-family:"Open Sans","DejaVu Sans",sans-serif;font-style:normal;font-weight:bold}
.conum[data-value] *{color:#fff!important}
.conum[data-value]+b{display:none}
.conum[data-value]::after{content:attr(data-value)}
pre .conum[data-value]{position:relative;top:-.125em}
b.conum *{color:inherit!important}
.conum:not([data-value]):empty{display:none}
dt,th.tableblock,td.content,div.footnote{text-rendering:optimizeLegibility}
h1,h2,p,td.content,span.alt,summary{letter-spacing:-.01em}
p strong,td.content strong,div.footnote strong{letter-spacing:-.005em}
p,blockquote,dt,td.content,span.alt,summary{font-size:1.0625rem}
p{margin-bottom:1.25rem}
.sidebarblock p,.sidebarblock dt,.sidebarblock td.content,p.tableblock{font-size:1em}
.exampleblock>.content{background:#fffef7;border-color:#e0e0dc;box-shadow:0 1px 4px #e0e0dc}
.print-only{display:none!important}
@page{margin:1.25cm .75cm}
@media print{*{box-shadow:none!important;text-shadow:none!important}
html{font-size:80%}
a{color:inherit!important;text-decoration:underline!important}
a.bare,a[href^="#"],a[href^="mailto:"]{text-decoration:none!important}
a[href^="http:"]:not(.bare)::after,a[href^="https:"]:not(.bare)::after{content:"(" attr(href) ")";display:inline-block;font-size:.875em;padding-left:.25em}
abbr[title]{border-bottom:1px dotted}
abbr[title]::after{content:" (" attr(title) ")"}
pre,blockquote,tr,img,object,svg{page-break-inside:avoid}
thead{display:table-header-group}
svg{max-width:100%}
p,blockquote,dt,td.content{font-size:1em;orphans:3;widows:3}
h2,h3,#toctitle,.sidebarblock>.content>.title{page-break-after:avoid}
#header,#content,#footnotes,#footer{max-width:none}
#toc,.sidebarblock,.exampleblock>.content{background:none!important}
#toc{border-bottom:1px solid #dddddf!important;padding-bottom:0!important}
body.book #header{text-align:center}
body.book #header>h1:first-child{border:0!important;margin:2.5em 0 1em}
body.book #header .details{border:0!important;display:block;padding:0!important}
body.book #header .details span:first-child{margin-left:0!important}
body.book #header .details br{display:block}
body.book #header .details br+span::before{content:none!important}
body.book #toc{border:0!important;text-align:left!important;padding:0!important;margin:0!important}
body.book #toc,body.book #preamble,body.book h1.sect0,body.book .sect1>h2{page-break-before:always}
.listingblock code[data-lang]::before{display:block}
#footer{padding:0 .9375em}
.hide-on-print{display:none!important}
.print-only{display:block!important}
.hide-for-print{display:none!important}
.show-for-print{display:inherit!important}}
@media amzn-kf8,print{#header>h1:first-child{margin-top:1.25rem}
.sect1{padding:0!important}
.sect1+.sect1{border:0}
#footer{background:none}
#footer-text{color:rgba(0,0,0,.6);font-size:.9em}}
@media amzn-kf8{#header,#content,#footnotes,#footer{padding:0}}
</style>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
</head>
<body class="article toc2 toc-right">
<div id="header">
<h1>23-12 Generative AI with LangChain</h1>
<div class="details">
<span id="author" class="author">Ben Auffarth</span><br>
</div>
<div id="toc" class="toc2">
<div id="toctitle">Table of Contents</div>
<ul class="sectlevel1">
<li><a href="#_1_what_is_generative_ai">1. What Is Generative AI?</a></li>
<li><a href="#_2_langchain_for_llm_apps">2. LangChain for LLM Apps</a></li>
<li><a href="#_3_getting_started_with_langchain">3. Getting Started with LangChain</a>
<ul class="sectlevel2">
<li><a href="#_3_4_building_an_application_for_customer_service">3.4. Building an application for customer service</a>
<ul class="sectlevel3">
<li><a href="#_summarizing_with_chain_of_density">Summarizing with Chain of density</a></li>
<li><a href="#_summarizing_with_langchain_map_reduce">Summarizing with LangChain Map-Reduce</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#_4_building_capable_assistants">4. Building Capable Assistants</a>
<ul class="sectlevel2">
<li><a href="#_4_1_mitigating_hallucinations_through_fact_checking">4.1. Mitigating hallucinations through fact-checking</a></li>
<li><a href="#_4_2_summarizing_information">4.2. Summarizing information</a>
<ul class="sectlevel3">
<li><a href="#_4_2_4_map_reduce_pipelines">4.2.4. Map-Reduce pipelines</a></li>
<li><a href="#_4_2_5_monitoring_token_usage">4.2.5. Monitoring token usage</a></li>
</ul>
</li>
<li><a href="#_4_3_extracting_information_from_documents">4.3. Extracting information from documents</a></li>
<li><a href="#_4_4_answering_questions_with_tools">4.4. Answering questions with tools</a></li>
<li><a href="#_4_5_exploring_reasoning_strategies">4.5. Exploring reasoning strategies</a></li>
</ul>
</li>
<li><a href="#_5_building_a_chatbot_like_chatgpt">5. Building a Chatbot like ChatGPT</a></li>
<li><a href="#_6_developing_software_with_generative_ai">6. Developing Software with Generative AI</a></li>
</ul>
</div>
</div>
<div id="content">
<div class="sect1">
<h2 id="_1_what_is_generative_ai">1. What Is Generative AI?</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Text generation models such as GPT-4 are advanced tools capable of producing coherent and grammatically correct text across various languages and formats. These models have significant applications in content creation and natural language processing (NLP), striving to achieve algorithms that understand and generate human-like text.</p>
</div>
<div class="paragraph">
<p>Language models predict the next element in a sequence, helping encode the rules of language in a machine-readable format. They are vital for NLP tasks including content creation, translation, summarization, and text editing.</p>
</div>
<div class="paragraph">
<p>Representation learning is a key concept where models autonomously learn to represent data to perform tasks without explicit feature engineering. This approach is used in various applications like image recognition, where models learn to identify visual features.</p>
</div>
<div class="paragraph">
<p>Language models like GPT-4 have been employed in diverse tasks such as essay writing, coding, translation, and genetic sequence analysis. They are utilized in:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Question answering for efficient customer support through AI chatbots.</p>
</li>
<li>
<p>Automatic summarization for quick understanding of lengthy texts.</p>
</li>
<li>
<p>Sentiment analysis to gauge customer opinions for businesses.</p>
</li>
<li>
<p>Topic modeling to uncover themes in document collections.</p>
</li>
<li>
<p>Semantic search to enhance search relevance through NLP.</p>
</li>
<li>
<p>Machine translation to aid businesses in international markets, with some models reaching the efficiency of commercial products like Google Translate.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Despite their advancements, language models have limitations in complex reasoning tasks and are prone to generating plausible but false information, known as hallucinations. The potential of increasing model scale to achieve new reasoning capabilities is still uncertain.</p>
</div>
<hr>
<div class="paragraph">
<p>Large Language Models (LLMs), such as ChatGPT, are sophisticated deep neural networks that excel in language comprehension and generation. These models, including transformers and Generative Pre-Trained Transformers (GPTs), have been trained on vast amounts of text data to learn language patterns through both unsupervised and supervised learning methods. The most recent models, like GPT-4, are capable of multiple modalities, including image processing, and have been trained on trillions of tokens. Despite their abilities, LLMs can produce incorrect or nonsensical answers, a phenomenon known as hallucination.</p>
</div>
<div class="paragraph">
<p>The article mentions the evolution of LLMs from BERT to GPT-4 in terms of size, training budget, and the organizations involved. OpenAI&#8217;s GPT series has been at the forefront of LLM development, with GPT-3 having 175 billion parameters and GPT-4 rumored to have between 200 and 500 billion. The cost of training such models is substantial, with GPT-4&#8217;s training allegedly exceeding $100 million. OpenAI has utilized a Mixture of Experts model to keep costs reasonable and potentially applied speculative decoding to speed up processing, though this could affect quality.</p>
</div>
<div class="paragraph">
<p>GPT-4 has been trained on a massive scale and can handle complex tasks more effectively, including avoiding harmful responses. It also has a multi-modal version capable of interpreting images and videos. The graph provided in the content shows various LLMs, indicating that there are several models available besides OpenAI&#8217;s, some of which could serve as alternatives to OpenAI&#8217;s proprietary models.</p>
</div>
<hr>
<div class="paragraph">
<p>OpenAI’s GPT-4 is a leading model in the field of generative transformer-based language models, but there are other significant models like Google DeepMind’s PaLM 2 and Meta AI&#8217;s LLaMa series that also demonstrate strong performance in various tasks.</p>
</div>
<div class="paragraph">
<p>PaLM 2, released in May 2023, focuses on better multilingual and reasoning capabilities with improved computational efficiency. Though smaller than its predecessor, PaLM 2 excels in tasks like language proficiency exams across several languages and shows enhanced abilities in multilingual common sense, mathematical reasoning, and coding.</p>
</div>
<div class="paragraph">
<p>The LLaMa and LLaMa 2 series from Meta AI, released in February and July 2023 respectively, have spurred a wave of open-source language model developments. LLaMa 2 has expanded on the original by increasing its training data, context length, and adopting new attention mechanisms. It offers multiple model sizes and is available for both research and commercial use.</p>
</div>
<div class="paragraph">
<p>Anthropic&#8217;s Claude and Claude 2 AI assistants are also notable, with Claude 2 emerging as a strong competitor to GPT-4. Released in July 2023, it has shown improvements in helpfulness and a reduction in bias, and it performs well in areas like coding and summarization.</p>
</div>
<div class="paragraph">
<p>Although all these models have made significant strides, they still share limitations like potential biases, factual inaccuracies, and the capacity for misuse, which their developers are actively working to mitigate. The evolution of large language models remains concentrated among a few key players due to the high computational demands of developing and training such models.</p>
</div>
<hr>
<div class="paragraph">
<p>This passage discusses the development and deployment of large language models (LLMs) by various companies and institutions. Training such models requires immense computational resources and expertise, with costs ranging from $10 million to over $100 million. Meta&#8217;s LLaMa 2 model, with 70 billion parameters, and Google&#8217;s PaLM 2 model, with 340 billion parameters, are examples of LLMs trained on extensive datasets in multiple languages.</p>
</div>
<div class="paragraph">
<p>Few organizations have the capability to train and deploy very large models; notable contributors include tech giants like Microsoft and Google, as well as universities like KAUST and Carnegie Mellon. Collaborations between companies and universities have resulted in projects like Stable Diffusion, Soundify, and DreamFusion.</p>
</div>
<div class="paragraph">
<p>Several entities are developing generative AI and LLMs, with varied approaches to sharing their work:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>OpenAI</strong> released GPT-2 as open source but has since restricted access to later models, offering them through an API.</p>
</li>
<li>
<p><strong>Google</strong> and <strong>DeepMind</strong> have created models such as BERT and PaLM, with some initially open-sourced but more recent development being more secretive.</p>
</li>
<li>
<p><strong>Anthropic</strong> offers public usage of its Claude models through their website, with API in private beta.</p>
</li>
<li>
<p><strong>Meta</strong> has released models like RoBERTa and LLaMa 2, including parameters and setup code, often under non-commercial licenses.</p>
</li>
<li>
<p><strong>Microsoft</strong> has developed its own models but also integrates OpenAI models into its products, releasing some parameters for research.</p>
</li>
<li>
<p><strong>Stability AI</strong>, the creator of Stable Diffusion, released model weights under a non-commercial license.</p>
</li>
<li>
<p><strong>Mistral</strong>, a French startup, offers a free, open-license 7B model.</p>
</li>
<li>
<p><strong>EleutherAI</strong> provides fully open-source models like GPT-Neo and GPT-J to the public.</p>
</li>
<li>
<p>Companies like <strong>Aleph Alpha</strong>, <strong>Alibaba</strong>, and <strong>Baidu</strong> prefer providing API access or integrating models into products rather than releasing their training code or parameters.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Additionally, the <strong>Technology Innovation Institute</strong> has open-sourced its Falcon LLM for research and commercial use.</p>
</div>
<div class="paragraph">
<p>Despite the high computational costs, the release of models like LLaMa has enabled smaller companies to make significant advancements, particularly in coding capabilities.</p>
</div>
<hr>
<div class="paragraph">
<p>The provided text discusses the transformative impact of the Transformer deep neural network architecture on natural language processing (NLP), particularly with the advent of models like BERT and GPT. This architecture, introduced in the paper "Attention Is All You Need" by Vaswani et al. in 2017, differs from previous models by processing words in parallel rather than sequentially, allowing for more efficient computation.</p>
</div>
<div class="paragraph">
<p>Transformers consist of an encoder and decoder, each comprising multiple layers with attention mechanisms and feed-forward networks. These models use positional encoding to retain information about word order, layer normalization for stable learning, and multi-head attention to capture different aspects of information simultaneously.</p>
</div>
<div class="paragraph">
<p>Attention mechanisms, a key feature of transformers, involve computing weighted sums of values based on the similarity between positions in the input sequence. Multi-Query Attention (MQA) is an extension that enhances efficiency, used in models such as OpenAI&#8217;s GPT series.</p>
</div>
<div class="paragraph">
<p>Grouped-Query Attention (GQA) is another technique used to speed up attention computation by caching key and value pairs, although it has memory cost issues with larger contexts or batch sizes.</p>
</div>
<div class="paragraph">
<p>Other efficiency-increasing methods include sparse and low-rank attention, latent bottlenecks, and architectures like transformer-XL which use recursion to store and leverage hidden states of previously encoded sentences.</p>
</div>
<div class="paragraph">
<p>The majority of large language models (LLMs) are based on the Transformer architecture due to its effectiveness in understanding and generating human language, as well as applications in other domains like image, sound, and 3D object processing.</p>
</div>
<div class="paragraph">
<p>The text concludes by mentioning that GPT models, which dominate the landscape of LLMs, are characterized by their pre-training process, setting the stage for a discussion on how these models are trained.</p>
</div>
<hr>
<div class="paragraph">
<p>The transformer model is trained in two stages: unsupervised pre-training and task-specific fine-tuning. Pre-training&#8217;s objective is to learn a universal representation for various tasks. Masked Language Modeling (MLM) is a pre-training method where the model predicts missing words in a sentence. The model&#8217;s parameters are updated to minimize the difference between its predictions and the actual tokens.</p>
</div>
<div class="paragraph">
<p>Two key metrics for training and evaluating language models are Negative Log-Likelihood (NLL) and Perplexity (PPL). NLL measures the probability of correct predictions, with lower values indicating better learning. PPL, which is the exponentiation of NLL, provides a more intuitive measure of model performance; a lower PPL suggests a model that accurately predicts words and is "less surprised" by the next word.</p>
</div>
<div class="paragraph">
<p>Perplexity is used to compare performance across different language models, where a lower value signifies a more effective model. The training process begins with tokenization, which converts words to numerical representations necessary for the model to process the input.</p>
</div>
<hr>
<div class="paragraph">
<p>Tokenization is the process of breaking down text into smaller units called tokens, which can be words, subwords, punctuation marks, or numbers. These tokens are then converted into unique numerical IDs through a mapping dictionary. The dictionary is created from the training data before training a Large Language Model (LLM) and remains unchanged afterward.</p>
</div>
<div class="paragraph">
<p>The numerical IDs assigned to tokens are not random; they are within a specific range, determined by the size of the tokenizer&#8217;s vocabulary. Tokens are essential for constructing sequences of text during the processing of natural language.</p>
</div>
<div class="paragraph">
<p>Different tokenization methods like Byte-Pair Encoding (BPE), WordPiece, and SentencePiece are used in various models. For instance, LLaMa 2&#8217;s BPE tokenizer breaks numbers into single digits and decomposes unknown UTF-8 characters using bytes, with a total vocabulary size of 32,000 tokens.</p>
</div>
<div class="paragraph">
<p>LLMs have a context window that limits the length of the token sequence they can process, usually ranging from 1,000 to 10,000 tokens. The large scale of these models is briefly mentioned as a topic for further discussion.</p>
</div>
<hr>
<div class="paragraph">
<p>The content discusses the trend of increasing language model sizes in machine learning, referencing a figure that shows their growth over time. This trend is linked to the decrease in computing costs and the pursuit of higher performance. Key findings from various research papers are highlighted:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>A 2020 paper by Kaplan et al. from OpenAI analyzed scaling laws for neural language models and found that transformers outperform LSTMs in handling long contexts, which leads to better performance and efficiency.</p>
</li>
<li>
<p>The paper also established a power-law relationship between a model&#8217;s performance and the dataset size, model size, and computational resources, suggesting that these factors should be scaled together to avoid performance bottlenecks.</p>
</li>
<li>
<p>DeepMind researchers in 2022 suggested that large language models (LLMs) are undertrained relative to what scaling laws would recommend for compute budget and dataset size. They showed that a smaller model (Chinchilla) could outperform a larger one (Gopher) if trained longer with a proportional dataset.</p>
</li>
<li>
<p>Contrary to the trend of larger models, Microsoft Research&#8217;s recent study found that a smaller network (350M parameters) trained on high-quality data can perform competitively, challenging the notion that bigger is always better.</p>
</li>
<li>
<p>Future chapters of the source will explore the implications of scaling laws for generative models and the potential for new scaling laws related to data quality.</p>
</li>
<li>
<p>Lastly, the content mentions that after pre-training, models are prepared for specific tasks through fine-tuning or prompting, which will be discussed in the context of task conditioning.</p>
</li>
</ul>
</div>
<hr>
<div class="paragraph">
<p>Conditioning Large Language Models (LLMs) involves adapting them for specific purposes, and it can be achieved through fine-tuning and prompting:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Fine-tuning</strong> is the process of further training a pre-trained LLM on a specific dataset to improve its performance on a particular task. This can include instruction tuning, where the model learns to follow natural language instructions, and Reinforcement Learning from Human Feedback (RLHF), which aims to make the model more helpful and safe.</p>
</li>
<li>
<p><strong>Prompting techniques</strong> involve providing the model with text-based problems to solve. These can range from simple questions to complex instructions, and may or may not include examples. Zero-shot prompting doesn&#8217;t use examples, while few-shot prompting provides a few example problems and solutions to guide the model.</p>
</li>
</ul>
</div>
<hr>
<div class="paragraph">
<p>The provided content explains how to access OpenAI&#8217;s model and other language models (LLMs) through their website, API, or platforms like Hugging Face. Open-source LLMs can be downloaded, fine-tuned, or fully trained, with a guide to fine-tuning provided in Chapter 8 of the referenced book. It also mentions the use of generative AI in creating 3D images, avatars, and other graphical content, with a focus on text-to-image generation. The book will primarily discuss LLMs due to their wide-ranging applications but will also touch upon image models. Upcoming sections will review state-of-the-art methods for text-conditioned image generation, including progress, challenges, and future directions.</p>
</div>
<hr>
<div class="paragraph">
<p>Text-to-image models are AI systems that generate images from textual descriptions. They are used in various fields, such as art, design, and advertising, to create visuals based on textual prompts. The models employ techniques like diffusion processes, where they start with a random noise and refine it into an image. They also use text encoders to convert text into embeddings, which are then processed in successive stages to produce images.</p>
</div>
<div class="paragraph">
<p>There are two main types of models: Generative Adversarial Networks (GANs) and diffusion models. GANs consist of two competing networks, a generator and a discriminator, which improve over time to create realistic images. Diffusion models work by gradually denoising a noisy image until it becomes a coherent picture corresponding to the text prompt.</p>
</div>
<div class="paragraph">
<p>Stable Diffusion is a notable example that operates in latent space, which is more computationally efficient than pixel space. It uses a Variational Autoencoder (VAE) for compression and a U-Net architecture for denoising. Stable Diffusion has been made available publicly under an open license, allowing wide access and use on consumer-grade hardware.</p>
</div>
<div class="paragraph">
<p>The training for these models is done on large datasets, and images are generated through a series of steps, including encoding, denoising, and decoding. The models can also be conditioned with specific inputs like depth maps or outlines to create images that closely match the text prompts.</p>
</div>
<div class="paragraph">
<p>These AI capabilities also extend to other domains beyond image generation, but the provided content focuses on the text-to-image context.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_2_langchain_for_llm_apps">2. LangChain for LLM Apps</h2>
<div class="sectionbody">
<div class="paragraph">
<p>LLMs (Large Language Models) are powerful tools for language processing but have notable limitations, which need to be understood when they are employed in applications:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Outdated Knowledge</strong>: LLMs are trained on historical data and cannot update their knowledge without new training, leaving them unaware of recent events or developments.</p>
</li>
<li>
<p><strong>Inability to Take Action</strong>: LLMs are not capable of performing interactive actions such as web searches or data retrieval, which limits their practical use.</p>
</li>
<li>
<p><strong>Lack of Real-Time Context</strong>: They struggle with understanding context from previous interactions, and cannot incorporate new context without external data sources.</p>
</li>
<li>
<p><strong>Hallucination Risks</strong>: LLMs may generate inaccurate or nonsensical responses when they lack concrete information on a topic.</p>
</li>
<li>
<p><strong>Biases and Discrimination</strong>: The biases present in their training data can lead to biased outputs, which reflect religious, ideological, or political prejudices.</p>
</li>
<li>
<p><strong>Lack of Transparency</strong>: The complexity of LLMs can make their decision-making process opaque and not easily understandable.</p>
</li>
<li>
<p><strong>Memory Limitations</strong>: LLMs may not remember details from earlier parts of a conversation or struggle to provide relevant additional information.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>To illustrate these limitations, the author provides examples where an LLM:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Lacks up-to-date information about a query concerning LangChain, potentially leading to incorrect responses about a different entity with the same name.</p>
</li>
<li>
<p>Performs inconsistently in solving math problems, correctly answering one question but failing another, highlighting the LLM&#8217;s reliance on training data rather than computational ability.</p>
</li>
<li>
<p>Could face problems with reasoning, such as determining whether a fruit would float based on its density compared to water, due to difficulties in synthesizing information.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The challenges posed by these limitations can be addressed by integrating LLMs with external data sources, analytical tools, and other applications to provide real-world context and enhance functionality. However, careful design and monitoring are required to mitigate risks such as bias and inappropriate content.</p>
</div>
<hr>
<div class="paragraph">
<p>The excerpt discusses various techniques to improve the performance and reliability of large language models (LLMs), which include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Retrieval augmentation</strong>: Enhancing model responses with information from knowledge bases to provide current context and reduce false information.</p>
</li>
<li>
<p><strong>Chaining</strong>: Allowing the model to perform searches and calculations as part of its response process.</p>
</li>
<li>
<p><strong>Prompt engineering</strong>: Designing prompts that include critical context to steer the model towards appropriate responses.</p>
</li>
<li>
<p><strong>Monitoring, filtering, and reviews</strong>: Implementing continuous oversight to identify and correct issues with the model’s inputs and outputs through:</p>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Automated <strong>filters</strong> like block lists and sensitivity classifiers.</p>
</li>
<li>
<p>Monitoring based on <strong>constitutional principles</strong> to ensure ethical content.</p>
</li>
<li>
<p><strong>Human reviews</strong> to gain insights into the model’s behavior and outputs.</p>
</li>
</ol>
</div>
</li>
<li>
<p><strong>Memory</strong>: Maintaining the context of conversations over time.</p>
</li>
<li>
<p><strong>Fine-tuning</strong>: Adapting the model with data that&#8217;s more relevant to its intended use to align with application-specific requirements.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The text emphasizes that merely increasing a model&#8217;s size does not grant it advanced reasoning skills. Instead, explicit strategies like prompting and chain-of-thought reasoning are necessary for compositional tasks. Techniques like self-ask prompting encourage the model to break down complex problems methodically.</p>
</div>
<div class="paragraph">
<p>The integration of these tools into training helps bridge gaps in the model’s abilities, where prompting provides context, chaining allows for logical inference, and retrieval adds factual data. This turns basic LLMs into more sophisticated reasoning tools.</p>
</div>
<div class="paragraph">
<p>Proper prompt engineering and fine-tuning are essential for preparing models for practical applications, while continuous monitoring ensures any problems are promptly addressed. Filters serve as an initial safeguard, and adherence to AI constitutional principles aims to ensure ethical behavior.</p>
</div>
<div class="paragraph">
<p>Connecting LLMs to external data sources is important for maintaining accuracy and reducing the generation of false information (hallucination), although it adds complexity to the system. Frameworks like LangChain offer a structured approach to responsibly use LLMs by enabling the combination of model queries with data sources, thus overcoming the limitations of standalone LLMs. The text suggests that with these enhancements, it is possible to create AI systems that were not feasible before due to inherent model limitations, setting the stage for further discussion on the topic.</p>
</div>
<hr>
<div class="paragraph">
<p>Large Language Models (LLMs), when integrated with specialized tools into applications, can significantly impact the digital landscape. These applications often involve a series of prompted interactions with LLMs, sometimes supplemented with external services or data sources to complete tasks.</p>
</div>
<div class="paragraph">
<p>Traditional software applications follow a multi-layer architecture with distinct client, frontend, backend, and database layers. In contrast, an LLM app uses an LLM to understand and respond to natural language prompts, including a client layer for user input, prompt engineering to guide the LLM, an LLM backend for processing, an output parsing layer, and optional integration with external services.</p>
</div>
<div class="paragraph">
<p>LLM apps can be enhanced with functions such as API access, advanced reasoning algorithms, and retrieval augmented generation (RAG) which weaves in external knowledge for more robust capabilities. These extensions enable LLM apps to execute complex logic chains, interact with databases conversationally, and provide dynamic responses based on up-to-date information.</p>
</div>
<div class="paragraph">
<p>The advantages of LLM applications include nuanced language processing, personalization, contextualization, and the ability to perform multi-step inferences. They facilitate natural user interactions and can be developed more efficiently since they do not require manual coding for every language scenario.</p>
</div>
<div class="paragraph">
<p>However, responsible data practices are crucial to address concerns around privacy, security, and potential misuse. LLM applications can be applied in various domains, such as chatbots, intelligent search engines, automated content creation, question answering, sentiment analysis, text summarization, data analysis, and code generation.</p>
</div>
<div class="paragraph">
<p>The effectiveness of LLMs is amplified when they are combined with other knowledge sources and computational tools. The LangChain framework is designed to integrate LLMs with other components to build complex, reasoning-based applications, addressing challenges associated with LLMs and enabling the creation of customized NLP solutions.</p>
</div>
<hr>
<div class="paragraph">
<p>LangChain is an open-source Python framework created by Harrison Chase in 2022, designed to ease the development of applications powered by large language models (LLMs). It provides a modular structure that allows developers to integrate language models with external data sources and services. Sequoia Capital and Benchmark, known for funding major tech companies, have invested in LangChain.</p>
</div>
<div class="paragraph">
<p>The framework offers reusable components and pre-assembled chains to streamline the creation of complex LLM applications. It addresses common challenges in LLM application development, such as prompt engineering, bias mitigation, and integrating external data, by providing abstracted and composable tools.</p>
</div>
<div class="paragraph">
<p>LangChain also supports advanced features like conversational context, persistence through agents and memory, and the ability to interact more sophisticatedly with the environment. Its key benefits include its modular design, chaining capabilities, memory and persistence for stateful interactions, and the open-source community.</p>
</div>
<div class="paragraph">
<p>Although LangChain is primarily a Python-based framework, there are companion projects in JavaScript (LangChain.js) and Ruby (<code>Langchain.rb</code>). Development of LLM applications can be challenging, but resources like documentation, courses, communities, and a Discord server are available to support developers.</p>
</div>
<div class="paragraph">
<p>An ecosystem is growing around LangChain, with extensions and integrations being regularly added. LangSmith offers debugging, testing, and monitoring tools for LLM apps. LlamaHub and LangChainHub provide libraries for building LLM systems, with LlamaHub focusing on data integration and LangChainHub serving as a repository for sharing LangChain artifacts.</p>
</div>
<div class="paragraph">
<p>Additionally, LangFlow and Flowise are UIs that facilitate the visual assembly of LangChain components into executable workflows. LangChain can be deployed locally or on various platforms, and <code>langchain-serve</code> streamlines deployment on the Jina AI cloud.</p>
</div>
<div class="paragraph">
<p>The framework aims to simplify the development process for more advanced LLM applications by leveraging its modular components, including memory, chaining, and agents.</p>
</div>
<hr>
<div class="paragraph">
<p>The passage discusses the concept of "chains" in LangChain, which are sequences of calls to components that can be used to build complex applications. Chains can include various components, such as language model calls, mathematical tools, and database queries, and are designed to be modular, composable, and reusable. They can be used to improve LangChain application performance by chaining prompts together or integrating specific tools, and they can enforce policies to moderate content or align with ethical standards.</p>
</div>
<div class="paragraph">
<p>For example, the <code>LLMCheckerChain</code> is used to verify statements and reduce inaccurate responses, a technique supported by a research paper which showed a 20% improvement in task performance. Router chains can autonomously decide which tool to use for a given task.</p>
</div>
<div class="paragraph">
<p>Benefits of using chains include modularity, composability, readability, maintainability, reusability, easy tool integration, and productivity. Creating a chain typically involves breaking down a workflow into logical steps and ensuring that components are single-responsibility and stateless for maximum reusability. Customizable configurations, robust error handling, and monitoring/logging are essential for creating reliable chains.</p>
</div>
<hr>
<div class="paragraph">
<p>Agents in LangChain are self-governing software entities designed to perform tasks and achieve specific goals through interaction with users and environments. They are distinct from chains, which are sequences of components that execute logical steps. Agents use chains by orchestrating them to take actions based on goals. They make decisions on actions by using large language models (LLMs) as reasoning engines, which process the available tools, user input, and past actions to determine the next step or final response.</p>
</div>
<div class="paragraph">
<p>Tools are essential functions that agents utilize to interact with the real world, and the agent executor runtime manages the continuous cycle of querying the agent, performing tool actions, and incorporating feedback from the environment, while handling technical details like error management and parsing.</p>
</div>
<div class="paragraph">
<p>The main advantages of agents include goal-driven behavior, the ability to dynamically adjust to environmental changes, maintaining context through statefulness, robust error handling through alternatives, and the composition of reusable chains.</p>
</div>
<div class="paragraph">
<p>Agents enable complex, multi-step tasks and interactive applications such as chatbots. They are designed to select and use the appropriate tools, as exemplified by an agent choosing to use a calculator or Python interpreter for calculations, indicating that sometimes simpler tools are more effective than complex LLMs for specific tasks.</p>
</div>
<div class="paragraph">
<p>However, agents and chains typically operate without retaining context from one execution to the next, presenting a limitation in statelessness. To address this, LangChain introduces memory components that allow information to be carried over between executions, enabling agents to maintain state and context.</p>
</div>
<hr>
<div class="paragraph">
<p>LangChain&#8217;s concept of memory allows for the persistence of state between executions of a chain or agent, enhancing the development of conversational and interactive applications. Memory enables the storage of conversational contexts, facts, relationships, and task progress, which improves response coherence and relevance, provides consistency, and maintains contextual information across sessions. This memory system reduces redundant LLM calls, saving on API costs and maintaining necessary context for the agent or chain.</p>
</div>
<div class="paragraph">
<p>LangChain offers a standard memory interface and various storage integrations, including databases. Some of the memory options provided are:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>ConversationBufferMemory</code> for full message history storage, though it increases latency and costs.</p>
</li>
<li>
<p><code>ConversationBufferWindowMemory</code> for retaining only recent messages.</p>
</li>
<li>
<p><code>ConversationKGMemory</code> for summarizing exchanges into a knowledge graph.</p>
</li>
<li>
<p><code>EntityMemory</code> for persisting agent states and facts, often backed by a database.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>There are multiple database options available for durable storage, such as SQL databases (e.g., Postgres, SQLite), NoSQL databases (e.g., MongoDB, Cassandra), in-memory databases like Redis, and managed cloud services like AWS DynamoDB. Specialized memory servers like Remembrall and Motörhead are also available for optimized conversational context.</p>
</div>
<div class="paragraph">
<p>The choice of memory approach depends on specific requirements such as persistence needs, data relationships, scalability, and resources. Effective memory patterns are crucial for creating stateful, context-aware agents, and LangChain provides the tools and integrations necessary to build such advanced AI systems.</p>
</div>
<hr>
<div class="paragraph">
<p>LangChain provides a framework for integrating external services, such as databases and APIs, into language models, enhancing their capabilities beyond simple text processing. Tools within LangChain offer various functionalities, including document loading, indexing, and data storage, and can be organized into toolkits that share resources. These tools can be combined with language models to address a wide range of tasks:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Machine translator</strong>: Helps models understand and respond in multiple languages.</p>
</li>
<li>
<p><strong>Calculator</strong>: Performs basic arithmetic operations.</p>
</li>
<li>
<p><strong>Maps</strong>: Provides location-based services, routing, and points of interest information.</p>
</li>
<li>
<p><strong>Weather</strong>: Supplies real-time weather data for various locations.</p>
</li>
<li>
<p><strong>Stocks</strong>: Accesses stock market data for financial analysis.</p>
</li>
<li>
<p><strong>Slides</strong>: Assists in creating presentation slides based on high-level semantics.</p>
</li>
<li>
<p><strong>Table processing</strong>: Analyzes and visualizes tabular data using data manipulation APIs.</p>
</li>
<li>
<p><strong>Knowledge graphs</strong>: Facilitates querying of structured factual data.</p>
</li>
<li>
<p><strong>Search engine</strong>: Enhances web-based information retrieval.</p>
</li>
<li>
<p><strong>Wikipedia</strong>: Aids in searching and disambiguating Wikipedia content.</p>
</li>
<li>
<p><strong>Online shopping</strong>: Enables e-commerce functionalities like product searching and selection.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Additional tools include AI Painting for image generation, 3D Model Construction for creating 3D visuals, Chemical Properties for scientific inquiries, and database tools for interacting with databases using natural language.</p>
</div>
<div class="paragraph">
<p>These tools significantly expand the applications of language models, allowing them to perform various specialized tasks efficiently.</p>
</div>
<hr>
<div class="paragraph">
<p>LangChain is a framework designed to build applications using large language models (LLMs) by providing modular components for various tasks. It enables the creation of pipelines, also known as chains, to perform sequences of actions such as loading documents, embedding for retrieval, querying LLMs, parsing outputs, and writing to memory. These components can be mixed and matched to align with specific application goals.</p>
</div>
<div class="paragraph">
<p>Key components of LangChain include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Interfaces for interacting with LLMs and chat models, supporting asynchronous, streaming, and batch operations.</p>
</li>
<li>
<p>Document loaders for ingesting data from various sources into text and metadata.</p>
</li>
<li>
<p>Document transformers for adapting data through manipulation like splitting, combining, and filtering.</p>
</li>
<li>
<p>Text embedding models for creating vector representations of text to facilitate semantic search.</p>
</li>
<li>
<p>Vector stores for indexing document vectors to improve retrieval efficiency.</p>
</li>
<li>
<p>Retrievers to return relevant documents based on a query.</p>
</li>
<li>
<p>Tools for interacting with external systems such as databases or web searches.</p>
</li>
<li>
<p>Agents that are goal-driven systems using LLMs to plan and execute actions.</p>
</li>
<li>
<p>Toolkits to initialize groups of tools sharing resources.</p>
</li>
<li>
<p>Memory components to maintain conversation and workflow information across sessions.</p>
</li>
<li>
<p>Callbacks for integrating with pipeline stages for tasks like logging and monitoring.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The framework offers standardized interfaces for integrating with various language model providers, allowing for easy swapping of models depending on cost, energy efficiency, or performance needs. It also provides prompt classes for user interaction with LLMs, which can be optimized through prompt engineering, and a collection of templates and battle-tested prompts.</p>
</div>
<div class="paragraph">
<p>LangChain supports a variety of data types and includes utilities for external system interaction, with the aim to enhance LLMs' knowledge and performance in applications like question answering and summarization. It also offers numerous integrations for vector storage, facilitating efficient document retrieval even for large documents.</p>
</div>
<div class="paragraph">
<p>For more detailed information, the LangChain API reference and code examples are available online. LangChain stands out as a comprehensive and feature-rich framework for building LLM applications.</p>
</div>
<hr>
<div class="paragraph">
<p>This text discusses the landscape of application frameworks designed for large language models (LLMs), with a focus on open-source libraries in Python for building dynamic LLM applications. It compares the popularity of various frameworks using GitHub stars over time, referencing a graph that illustrates their relative growth.</p>
</div>
<div class="paragraph">
<p>The frameworks mentioned include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Haystack</strong>: The oldest framework mentioned, which started in early 2020 and is focused on creating large-scale search systems. Despite its early start, it is the least popular among those discussed.</p>
</li>
<li>
<p><strong>LangChain</strong>: A rapidly growing framework that specializes in chaining LLMs together using agents, prompt optimization, and context-aware information retrieval/generation. It is praised for its modular interface and comprehensive toolset.</p>
</li>
<li>
<p><strong>LlamaIndex (previously GPTIndex)</strong>: Aimed at advanced retrieval tasks rather than a broad range of LLM applications.</p>
</li>
<li>
<p><strong>SuperAGI</strong>: Offers features similar to LangChain, including a marketplace for tools and agents, but it is not as extensive or well-supported.</p>
</li>
<li>
<p><strong>AutoGen</strong>: A Microsoft project that facilitates the creation of workflows powered by LLMs, particularly through customizable conversational agents that automate coordination between LLMs, humans, and tools.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The text also references AutoGPT and other tools focused on prompt engineering, such as Promptify, but notes their limitations in reasoning and tendency to fall into logic loops. Additionally, it mentions frameworks in other programming languages, like Dust in Rust, which is geared towards the design and deployment of LLM apps.</p>
</div>
<div class="paragraph">
<p>The author emphasizes the importance of foundational knowledge in leveraging LLM frameworks effectively and responsibly, and suggests that investment in education is crucial to develop capable LLM applications.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_3_getting_started_with_langchain">3. Getting Started with LangChain</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The provided text describes the use of a fake LLM (Large Language Model) in testing environments to simulate responses from a real LLM without making actual API calls. This allows developers to rapidly prototype and test their applications without being constrained by rate limits or the need for a live LLM. The fake LLM can be used for mocking various responses to ensure that an application handles them correctly, thus facilitating quick iteration.</p>
</div>
<div class="paragraph">
<p>The text includes a simple example of initializing a <code>FakeLLM</code> in Python that returns a single response "Hello". It also provides a more complex example using <code>FakeListLLM</code> to mock a sequence of responses within an agent framework that leverages tools like a Python REPL. This is used to demonstrate how an agent can interact with a tool based on the fake LLM&#8217;s output. The agent in this example is set up to react to input text ("what&#8217;s 2 + 2") and, through the fake LLM&#8217;s responses, perform an action (running Python code via REPL) and return a result ("Final Answer: 4").</p>
</div>
<div class="paragraph">
<p>The text highlights that the action performed by the agent must match the <code>name</code> attribute of the tool, which in this example is "Python_REPL". The fake LLM can be programmed to return a different final answer, which would not be consistent with the actual computation.</p>
</div>
<hr>
<div class="paragraph">
<p>To use OpenAI&#8217;s API, it is necessary to obtain an API key, and the text provides a step-by-step guide on how to do this, including creating a login, setting up billing, and generating a new key on the OpenAI platform. A Python code snippet is also given, showing how to set up an OpenAI language model class and create an agent that can perform calculations. An example demonstrates the agent correctly solving a simple arithmetic problem.</p>
</div>
<hr>
<div class="paragraph">
<p>Hugging Face is a leading company in the field of natural language processing (NLP), known for its open-source contributions and machine learning hosting services. It is based in the United States and is responsible for creating the widely-used Transformers Python library, which supports NLP models like Mistral 7B, BERT, and GPT-2, while being compatible with PyTorch, TensorFlow, and JAX.</p>
</div>
<div class="paragraph">
<p>The company also operates the Hugging Face Hub, an online platform with over 120,000 models, 20,000 datasets, and 50,000 demo applications (spaces) that serves as a collaborative environment for machine learning practitioners. Their ecosystem includes other libraries such as <code>Datasets</code> for managing datasets, <code>Evaluate</code> for model evaluation, <code>Simulate</code> for running simulations, and <code>Gradio</code> for creating machine learning demos.</p>
</div>
<div class="paragraph">
<p>Hugging Face has engaged in significant research initiatives, such as the BigScience Research Workshop and the release of the BLOOM model, which has 176 billion parameters. They have secured substantial funding, with a Series C round valuing the company at $2 billion, and have formed partnerships with industry giants like Graphcore and AWS.</p>
</div>
<div class="paragraph">
<p>Users can access and integrate Hugging Face models into their applications by creating an account and obtaining API keys. For example, using the Flan-T5-XXL model developed by Google, one can run NLP tasks like answering questions, as demonstrated in the provided Python code snippet.</p>
</div>
<hr>
<div class="paragraph">
<p>Google Cloud Platform offers access to various machine learning models and functions through Vertex AI, with language models such as LaMDA, T5, and PaLM available. The Natural Language API has been updated with a new large language model for Content Classification, featuring over 1,000 labels and supporting 11 languages.</p>
</div>
<div class="paragraph">
<p>To use models on GCP, one must install the <code>gcloud</code> command-line interface and authenticate using the provided command. Vertex AI must be enabled for the project, which involves installing the Google Vertex AI SDK.</p>
</div>
<div class="paragraph">
<p>Setting up the Google Cloud project ID can be done in multiple ways, including using <code>gcloud</code>, passing a constructor argument, using <code>aiplatform.init()</code>, or setting a GCP environment variable.</p>
</div>
<div class="paragraph">
<p>Running a model involves using the VertexAI class and LLMChain with a PromptTemplate. The provided example demonstrates running a query about which NFL team won the Super Bowl in the year Justin Bieber was born, with a step-by-step reasoning approach. The response correctly identifies the San Francisco 49ers as the winners in 1994, despite a misspelling of Bieber&#8217;s name.</p>
</div>
<div class="paragraph">
<p>Vertex AI has specialized models for various tasks, such as following instructions, conversation, and code generation. Models like text-bison, chat-bison, code-bison, codechat-bison, and code-gecko have different token limits and are designed for specific use cases.</p>
</div>
<div class="paragraph">
<p>The example also shows the code-bison model generating a Python function to solve the FizzBuzz problem, suggesting the model&#8217;s capability to generate functional code for common programming tasks. The documentation provides more detailed and current information about the models and their updates.</p>
</div>
<hr>
<div class="paragraph">
<p>Jina AI is an AI company based in Berlin that provides cloud-native neural search solutions for various data types, including text, image, audio, and video. The company, founded in 2020, has developed an open-source neural search ecosystem to help developers create scalable and efficient information retrieval systems. They also introduced a tool called Finetuner for fine-tuning deep neural networks according to specific needs.</p>
</div>
<div class="paragraph">
<p>The company has raised $37.5 million through funding rounds, with significant investment from GGV Capital and Canaan Partners. Jina AI offers an API platform for setting up services like image captioning and visual question answering.</p>
</div>
<div class="paragraph">
<p>The document includes an example of setting up a Visual Question Answering API and a guide to using Jina AI&#8217;s services with LangChain, a library that facilitates working with language models. Although Jina AI APIs are not directly available through LangChain, users can integrate them by subclassing the <code>LLM</code> class. Instructions on setting up a chatbot with Jina AI are provided, along with examples of API calls for translation and food recommendation tasks.</p>
</div>
<div class="paragraph">
<p>The document distinguishes between LLMs (text completion models) and chat models (designed for conversational interactions) in LangChain, noting that both implement a base language model interface allowing for versatility in application usage.</p>
</div>
<div class="sect2">
<h3 id="_3_4_building_an_application_for_customer_service">3.4. Building an application for customer service</h3>
<div class="paragraph">
<p>Generative AI can greatly assist customer service agents by classifying customer sentiment, summarizing lengthy messages, predicting customer intent, and suggesting answers to improve response accuracy and timeliness. LangChain facilitates the use of various models, including those from Hugging Face, for tasks like sentiment analysis and summarization. For instance, sentiment analysis can identify negative or positive emotions in customer communications, while summarization tools condense lengthy texts. Popular models on Hugging Face for these tasks include <code>distilbert-base-uncased-finetuned-sst-2-english</code> for sentiment classification and <code>facebook/bart-large-cnn</code> for summarization. The use of AI in customer service can help with the quick resolution of common issues, allowing human agents to focus on complex problems, thereby enhancing customer service efficiency and effectiveness.</p>
</div>
<div class="sect3">
<h4 id="_summarizing_with_chain_of_density">Summarizing with Chain of density</h4>
<div class="dlist">
<dl>
<dt class="hdlist1">Missing entities</dt>
<dd>
<p>Generative AI; LangChain; Hugging Face integrations; sentiment analysis; summarization; intent classification; Zengzhi Wang; Financial PhraseBank; ProsusAI/finbert; Python code; Vertex AI; Prototype; Chapter 5; Chatbot; GPT-3.5; GitHub; spaCy; Cohere; NLP Cloud; LLMs; few-shot prompts; pipeline; <code>HuggingFaceHub</code>; <code>load_huggingface_tool()</code>; <code>cardiffnlp/twitter-roberta-base-sentiment</code>; emoji prediction; irony detection; hate speech detection; offensive language identification; stance detection; <code>LABEL_0</code>; facebook/bart-large-cnn; t5-small; t5-base; sshleifer/distilbart-cnn-12-6; t5-large; <code>HUGGINGFACEHUB_API_TOKEN</code>; PromptTemplate; LLMChain; graphical interface; AI automation; customer service workflows</p>
</dd>
</dl>
</div>
<div class="paragraph">
<p>Generative AI tools like LangChain can enhance customer service by offering sentiment analysis, summarization, and intent classification. Integrations with platforms like Hugging Face provide access to specialized models, such as ProsusAI/finbert for financial text. Python code examples demonstrate how to use these tools, highlighting their application in a prototype for a chatbot in Chapter 5. For instance, GPT-3.5 generated a customer email complaint, available on GitHub, which was analyzed using spaCy, Cohere, and NLP Cloud models. Using few-shot prompts, LLMs can be executed through a pipeline or via <code>HuggingFaceHub</code> and <code>load_huggingface_tool()</code> loaders. The <code>cardiffnlp/twitter-roberta-base-sentiment</code> model, capable of emoji prediction, irony detection, hate speech detection, offensive language identification, and stance detection, identified the email&#8217;s sentiment as negative (<code>LABEL_0</code>). The <code>facebook/bart-large-cnn</code> is among the most downloaded summarization models on Hugging Face, along with t5 variants. With <code>HUGGINGFACEHUB_API_TOKEN</code>, the model can summarize text remotely. Vertex AI is also showcased, where a PromptTemplate and LLMChain identified the email&#8217;s category. The potential for AI automation in customer service workflows is evident, and a graphical interface can be implemented for agents to interact with AI-enhanced systems.</p>
</div>
</div>
<div class="sect3">
<h4 id="_summarizing_with_langchain_map_reduce">Summarizing with LangChain Map-Reduce</h4>
<div class="paragraph">
<p>Generative AI can enhance customer service by assisting agents with tasks such as sentiment classification, summarization, and intent classification, leading to more personalized and efficient service. LangChain allows the use of various AI models, including those from Hugging Face, for these purposes. The text illustrates how AI can interpret customer sentiment, summarize communications, and categorize issues, suggesting that AI could manage routine inquiries and free up human agents for complex problems. The integration of AI tools into a user-friendly interface for agents is proposed for future exploration.</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_4_building_capable_assistants">4. Building Capable Assistants</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_4_1_mitigating_hallucinations_through_fact_checking">4.1. Mitigating hallucinations through fact-checking</h3>
<div class="paragraph">
<p>The text discusses the issue of hallucination in Large Language Models (LLMs), where generated text does not accurately reflect the input, leading to misinformation. It emphasizes the importance of fact-checking to maintain information integrity and mitigate societal harm caused by misinformation, such as distrust in science and damage to democratic processes.</p>
</div>
<div class="paragraph">
<p>The process of automatic fact-checking is described in three stages: claim detection, evidence retrieval, and verdict prediction. The process is demonstrated using a pipeline diagram from a GitHub repository. Pre-trained LLMs with extensive world knowledge from sources like Wikipedia can be prompted to retrieve facts for evidence verification, or external tools can be used to search knowledge bases and other corpora.</p>
</div>
<div class="paragraph">
<p>A practical application is introduced with the <code>LLMCheckerChain</code> in LangChain, which uses prompt chaining to question the assumptions behind statements and check their validity. The model sequentially lists assumptions, checks their truthfulness, and makes a final judgment on the initial question. The example provided shows how this process can be used to verify which mammal lays the largest eggs, demonstrating that while not infallible, the fact-checking approach can improve the reliability of LLMs.</p>
</div>
</div>
<div class="sect2">
<h3 id="_4_2_summarizing_information">4.2. Summarizing information</h3>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="dlist">
<dl>
<dt class="hdlist1">Example</dt>
<dd>
<p>generative_ai_with_langchain/summarize/prompts.py</p>
</dd>
<dt class="hdlist1">Web Server</dt>
<dd>
<p>generative_ai_with_langchain/webserver/chat.py</p>
</dd>
</dl>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>The provided text explains how to use LangChain, a Python library, to summarize text using OpenAI&#8217;s language models. It describes two methods: a basic approach using prompts and a more Pythonic way using LangChain decorators. The latter offers a cleaner interface, enabling developers to write natural Python code while leveraging the power of language models for tasks such as summarization. An example demonstrates summarizing a piece of text into a one-sentence summary using the <code>@llm_prompt</code> decorator.</p>
</div>
<hr>
<div class="paragraph">
<p>The provided text describes the use of prompt templates in LangChain Expression Language (LCEL) to dynamically insert text into prompts, which is useful for tasks like text summarization. The example code demonstrates how to set up a prompt template and create a chain in LCEL that includes a language model (LLM) and an output parser. The chain is then used to generate a summary of the provided text. LCEL offers benefits such as asynchronous processing, batching, streaming, and other features that enhance productivity and integration.</p>
</div>
<hr>
<div class="paragraph">
<p>Salesforce researchers have devised a method called Chain of Density (CoD) for GPT-4 that produces increasingly dense and concise summaries by iteratively including more informative entities without extending the length. Using a structured prompt, the process starts with a sparse summary and, through five rounds of editing, integrates additional entities while maintaining word count. This technique enhances the information density of summaries, but there&#8217;s a balance to strike as too many entities can reduce clarity. The effectiveness of CoD is evaluated through human studies and GPT-4 scoring, highlighting the trade-offs between detail and coherence in AI-generated text.</p>
</div>
<div class="sect3">
<h4 id="_4_2_4_map_reduce_pipelines">4.2.4. Map-Reduce pipelines</h4>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="dlist">
<dl>
<dt class="hdlist1">Jupyter</dt>
<dd>
<p>my/08_summarize.ipynb</p>
</dd>
<dt class="hdlist1">LangChain - Summarization</dt>
<dd>
<p><a href="https://python.langchain.com/docs/use_cases/summarization" class="bare">https://python.langchain.com/docs/use_cases/summarization</a></p>
</dd>
<dt class="hdlist1">Markdown Loader</dt>
<dd>
<p><a href="https://python.langchain.com/docs/modules/data_connection/document_loaders/markdown" class="bare">https://python.langchain.com/docs/modules/data_connection/document_loaders/markdown</a></p>
</dd>
</dl>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>LangChain enables efficient processing of documents using a map-reduce approach with large language models (LLMs). Documents are split into chunks, each summarized in parallel (map step), and then combined and further summarized (reduce step). This method allows for scaling summarization to any text length and can include an optional collapsing step to ensure chunks fit within token limits.</p>
</div>
<div class="paragraph">
<p>The process involves loading a document, like a PDF, summarizing each part independently, and then combining these summaries into a final, concise document. Custom prompts can be used for different steps to tailor the output, such as summarizing, translating, or rephrasing.</p>
</div>
<div class="paragraph">
<p>An example in Python demonstrates loading a PDF, summarizing it with a map-reduce chain, and outputting the summary. The approach is customizable, allows parallel processing, and can be used for various applications like literature reviews. However, when using cloud services, this method may increase computational costs due to the number of tokens processed.</p>
</div>
</div>
<div class="sect3">
<h4 id="_4_2_5_monitoring_token_usage">4.2.5. Monitoring token usage</h4>
<div class="paragraph">
<p>When using language models like those from OpenAI, it is crucial to monitor token usage to manage costs. OpenAI offers a variety of models tailored to different tasks, such as ChatGPT for dialogue and InstructGPT for instruction-following, with varying levels of speed and capability, affecting their pricing. For image generation, OpenAI has DALL·E, and for speech transcription and translation, it provides Whisper, each with different pricing structures.</p>
</div>
<div class="paragraph">
<p>To track token usage and costs, OpenAI provides a callback function in Python that displays the number of tokens used and the associated cost for each operation. Additionally, the <code>generate()</code> method and the chat completions response format offer information on token usage. Understanding these costs is essential for managing the budget in production environments. The upcoming chapter will discuss tools that offer further insights into the token usage of generative AI models.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_4_3_extracting_information_from_documents">4.3. Extracting information from documents</h3>
<div class="paragraph">
<p>OpenAI announced updates to their API in June 2023, adding <strong>function calling</strong> capabilities to enhance the interaction with GPT models, specifically <code>gpt-4-0613</code> and <code>gpt-3.5-turbo-0613</code>. This new feature allows developers to define functions in a schema format which the models can use to return structured outputs, such as JSON objects. This is particularly useful for creating chatbots, converting natural language into API or database queries, and extracting structured data from text.</p>
</div>
<div class="paragraph">
<p>Developers can define functions using the <code>functions</code> parameter in the API and describe them using JSON schema. This enables precise extraction of information, as demonstrated with an example schema for a Curriculum Vitae (CV) using the Pydantic library for parsing.</p>
</div>
<div class="paragraph">
<p>LangChain, a tool for building LLM applications, can utilize these function calls for tasks such as information extraction from documents. An example code snippet demonstrates how one might extract information from a CV using LangChain’s <code>create_extraction_chain_pydantic()</code> function and an OpenAI model.</p>
</div>
<div class="paragraph">
<p>The result of this extraction process may not be perfect, capturing only a part of the desired information, but it illustrates the potential of this approach. OpenAI&#8217;s function calling is integrated into the system message and is optimized for their models, which affects the context limit and billing.</p>
</div>
<div class="paragraph">
<p>LangChain supports function calls natively and can use models from providers other than OpenAI. The chapter also hints at further integrations, allowing LLM agents to execute function calls to connect with live data, services, and runtime environments. The next section is set to discuss how tools can augment context by retrieving external knowledge sources.</p>
</div>
</div>
<div class="sect2">
<h3 id="_4_4_answering_questions_with_tools">4.4. Answering questions with tools</h3>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="dlist">
<dl>
<dt class="hdlist1">Streamlit</dt>
<dd>
<p>st_langchain.py</p>
</dd>
</dl>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>LangChain is a platform that enhances the capabilities of large language models (LLMs) by enabling them to interact with external data sources and tools, thus allowing them to perform domain-specific tasks and access real-time information. This functionality is facilitated by a framework of agents and chains that can be developed to include tools like calculators, search engines (like DuckDuckGo and Wolfram Alpha), and information databases (like arXiv and Wikipedia). These tools help LLMs provide more accurate and relevant responses by grounding them in real-world data and reducing incorrect or hallucinated replies.</p>
</div>
<div class="paragraph">
<p>The integration of LLMs with tools can be demonstrated by setting up an agent in Python, which includes a DuckDuckGo search tool for privacy-focused searches, Wolfram Alpha for math questions, arXiv for academic research, and Wikipedia for information about notable entities. To use Wolfram Alpha, a developer account and token are required.</p>
</div>
<div class="paragraph">
<p>LangChain can also be used to build interactive web applications using Streamlit, a platform that facilitates the creation of user interfaces for machine learning workflows. An example provided in the text shows how to create a Streamlit app that enables users to interact with a chatbot powered by LangChain. This Streamlit integration allows for real-time updates, easy deployment, and sharing through Streamlit Community Cloud or Hugging Face Spaces.</p>
</div>
<div class="paragraph">
<p>The text illustrates the process of building a Streamlit app and deploying it, highlighting the advantages of a quick and intuitive user interface that can be tailored to specific use cases. Streamlit apps are responsive and can handle complex workflows, allowing users to interact with the LLM-powered agent with ease. Despite these advancements, the LLM&#8217;s reasoning abilities are limited, and the text suggests that more advanced types of agents can be implemented to overcome these limitations.</p>
</div>
</div>
<div class="sect2">
<h3 id="_4_5_exploring_reasoning_strategies">4.5. Exploring reasoning strategies</h3>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="dlist">
<dl>
<dt class="hdlist1">Streamlit</dt>
<dd>
<p>st_plan.py</p>
</dd>
</dl>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Language Large Models (LLMs) are adept at recognizing patterns but have limitations in performing complex multi-step symbolic reasoning. To enhance their capabilities, hybrid systems combining neural pattern recognition and symbolic manipulation are being developed. These advanced systems can perform multi-step deductive reasoning, mathematical problem-solving, and optimized action planning.</p>
</div>
<div class="paragraph">
<p>Hybrid systems involve various components and architectures such as action agents, which iterate based on new observations, and plan-and-execute agents, which create a full plan before taking action. Action agents use an observation-dependent approach, while plan-and-execute agents involve a Planner to create plans and a Solver to execute the final output after evidence is gathered.</p>
</div>
<div class="paragraph">
<p>The research application LangChain demonstrates how to implement these reasoning strategies, allowing users to select between zero-shot-react and plan-and-solve strategies. The application uses a combination of tools and LLMs to answer complex questions, and it can be executed using Streamlit, a tool for creating web applications.</p>
</div>
<div class="paragraph">
<p>The plan-and-solve strategy is particularly efficient as it can use specialized, smaller models for planning and solving, and it can handle more complex tasks by breaking them down into subtasks. However, challenges such as calculation errors and semantic misunderstandings can arise. Despite these issues, these strategies are valuable for improving the reasoning capabilities of LLMs and their effectiveness in problem-solving tasks.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_5_building_a_chatbot_like_chatgpt">5. Building a Chatbot like ChatGPT</h2>
<div class="sectionbody">
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="dlist">
<dl>
<dt class="hdlist1">Example</dt>
<dd>
<p>generative_ai_with_langchain/chat_with_retrieval/app.py</p>
</dd>
</dl>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Retrieval-augmented generation (RAG) is a method used to improve text generation by incorporating external knowledge into language models, referred to as Retrieval-Augmented Language Models (RALMs). Unlike traditional language models that rely solely on a given prompt, RALMs use semantic search algorithms to find and use relevant information from external sources to create more accurate and contextually appropriate text. This process involves dynamically querying and retrieving data to inform the generation process, which can lead to more nuanced, factually correct, and useful outputs. The technique relies on efficient storage and indexing of vector embeddings to perform real-time semantic searches across vast document collections. By leveraging RAG, language models can reduce incorrect or irrelevant responses, especially in specialized fields like healthcare. Vector search, a related concept, involves retrieving vectors based on similarity to enhance various applications including search engines and chatbots.</p>
</div>
<hr>
<div class="paragraph">
<p>Embeddings are numerical vectors that represent objects like words, sentences, or images in a format that machines can understand, capturing their semantic content. In OpenAI LLM an embedding consists of 1,536 numbers that encapsulate the text&#8217;s meaning. Word embeddings can be visualized in a vector space where semantic similarity corresponds to proximity. Traditional methods like the <strong>bag-of-words</strong> have been succeeded by more advanced models like <strong>word2vec</strong>, which learn embeddings from word context. For images, embeddings can be derived from convolutional neural networks.</p>
</div>
<div class="paragraph">
<p>Embeddings are used for a variety of machine learning tasks, such as measuring similarity, classification, or as input for other models. In LangChain, embeddings can be obtained using methods like <code>embed_query()</code> for single inputs or <code>embed_documents()</code> for multiple inputs. Arithmetic operations can be performed on embeddings, like calculating distances to analyze similarity.</p>
</div>
<div class="paragraph">
<p>The text also discusses how to generate and analyze embeddings using LangChain and Python code, including visualizing distances between word embeddings to confirm their semantic relationships. Additionally, LangChain offers tools for integrating embeddings into apps and systems, as well as a <code>FakeEmbeddings</code> class for testing without external calls.</p>
</div>
<hr>
<div class="paragraph">
<p>Vector search is a technique used to find similar data points in a high-dimensional space by representing data as vectors and measuring the similarity between them. This method is useful in applications such as recommendation systems and image or text search. Data points are organized through indexing, using algorithms such as k-d trees, Annoy, and product quantization for efficient retrieval.</p>
</div>
<div class="paragraph">
<p>Vector libraries, like Faiss and Annoy, offer functions for indexing and searching vectors, with some libraries being more popular than others based on GitHub stars. Vector databases like Milvus and Pinecone provide a comprehensive solution for managing and querying vector embeddings, supporting a variety of use cases such as anomaly detection, personalization, and natural language processing.</p>
</div>
<div class="paragraph">
<p>The market for vector databases is growing, with open-source options being popular for their AI and data management capabilities. They are designed for specific tasks such as similarity search and can handle high-dimensional data efficiently. Examples of vector databases include Chroma, Qdrant, and Milvus, among others, each with its unique features, business models, indexing methods, and licensing.</p>
</div>
<div class="paragraph">
<p>LangChain&#8217;s <code>vectorstores</code> module can be used to implement vector storage, with Chroma as an example backend optimized for storing and querying vectors. To use Chroma, one must import the necessary modules, create an instance with documents and an embedding method, and then query the vector store to find similar vectors. Document loaders and retrievers are also important components when building applications like chatbots.</p>
</div>
<hr>
<div class="paragraph">
<p>LangChain provides a toolchain for creating retrieval systems, including a pipeline for building a chatbot with Retrieval-Augmented Generation (RAG). The process involves data loaders to import documents, document transformers to process them, embedding models to convert text to vector representations, vector stores to maintain these embeddings, and retrievers to fetch relevant information based on queries.</p>
</div>
<div class="paragraph">
<p>Data loaders help load documents from various sources, such as text files, web pages, Arxiv, or YouTube, into the LangChain framework as Document objects with text and metadata. Examples of different loaders include <code>TextLoader</code>, <code>WebBaseLoader</code>, <code>ArxivLoader</code>, <code>YoutubeLoader</code>, and <code>ImageCaptionLoader</code>. These loaders can fetch documents either eagerly or lazily as needed.</p>
</div>
<div class="paragraph">
<p>Retrievers are components used to search and retrieve information from a vector store, where document embeddings are indexed. Different types of retrievers are available, such as BM25, TF-IDF, dense, and kNN retrievers, each with its own strengths and use cases. Specialized retrievers, like the <code>ArxivRetriever</code> and <code>WikipediaRetriever</code>, cater to specific domains like scientific literature and Wikipedia respectively.</p>
</div>
<div class="paragraph">
<p>Examples are provided for using a kNN retriever with OpenAI embeddings to retrieve documents based on text similarity, and a PubMed retriever to fetch biomedical literature relevant to queries like "COVID."</p>
</div>
<div class="paragraph">
<p>Additionally, custom retrievers can be created by inheriting from the <code>BaseRetriever</code> class and implementing the <code>get_relevant_documents()</code> method to define the retrieval logic for any specific requirements.</p>
</div>
<div class="paragraph">
<p>In summary, LangChain helps build chatbots and other retrieval systems by offering tools to load, transform, embed, store, and retrieve documents, with flexibility to handle various data sources and to customize retrievers as per the application&#8217;s needs.</p>
</div>
<hr>
<div class="paragraph">
<p>The provided content outlines how to implement a simple chatbot using the LangChain framework. The process involves setting up a document loader to read various document formats (PDF, text, EPUB, Word), storing documents in a vector store, and configuring a chatbot to retrieve information from the vector storage.</p>
</div>
<div class="paragraph">
<p>The document loader is designed to support multiple file extensions and load files as a list of documents, while the vector storage is configured using a Hugging Face model for embeddings and DocArray for in-memory storage. The retriever employs maximum marginal relevance (MMR) to retrieve diverse and relevant documents.</p>
</div>
<div class="paragraph">
<p>Contextual compression techniques are mentioned to enhance retrieval by filtering out irrelevant information, with options like LLMChainExtractor, LLMChainFilter, and EmbeddingsFilter. The chatbot chain is set up with memory for contextual conversation and a ChatOpenAI model.</p>
</div>
<div class="paragraph">
<p>The interface for the chatbot is created using Streamlit, allowing users to upload documents and interact with the chatbot. However, limitations such as input size, cost, and complexity of in-house model hosting are acknowledged, with further discussion to be found in a later chapter on customizing language models.</p>
</div>
<hr>
<div class="paragraph">
<p>Memory is crucial for chatbots to have coherent conversations by remembering previous interactions. It provides context, personalization, and the ability to learn from past exchanges. LangChain&#8217;s <code>ConversationBufferMemory</code> and <code>ConversationBufferWindowMemory</code> are examples of how to implement memory in chatbots. <code>ConversationBufferMemory</code> stores all messages, while <code>ConversationBufferWindowMemory</code> keeps only a specified number of recent interactions.</p>
</div>
<div class="paragraph">
<p>Customization of conversational memory is possible by changing prefixes and templates in LangChain. <code>ConversationSummaryMemory</code> provides a condensed version of conversation history, and <code>ConversationKGMemory</code> enables storing facts in a knowledge graph format. Multiple memory strategies can be combined with <code>CombinedMemory</code>.</p>
</div>
<div class="paragraph">
<p><code>ConversationSummaryBufferMemory</code> summarizes old interactions and keeps recent ones to manage token limits. Long-term persistence can be achieved using platforms like Zep, which offer persistent backends for storing, summarizing, and searching chat histories. This enhances AI capabilities and context awareness.</p>
</div>
<hr>
<div class="paragraph">
<p>Moderation in chatbots is crucial for ensuring interactions are appropriate and respectful, aligning with ethical standards, and protecting users from offensive content. A "constitution" for chatbots establishes guidelines for their behavior, promoting ethical engagement and safeguarding brand reputation. This framework also helps in meeting legal requirements for content moderation.</p>
</div>
<div class="paragraph">
<p>To implement moderation, developers can use pre-built moderation chains such as the <code>OpenAIModerationChain</code> in LangChain, which can be appended to the chatbot&#8217;s operational chain. If harmful content is detected, the system can either throw an error or inform the user that the content is unacceptable.</p>
</div>
<div class="paragraph">
<p>Guardrails are additional controls that provide programmable constraints to guide the chatbot&#8217;s output, preventing discussions on sensitive topics, ensuring conversations follow predefined paths, maintaining a specific language style, and extracting structured data from interactions. These measures ensure that chatbots operate within safe and desired parameters, maintaining user trust and compliance with standards.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_6_developing_software_with_generative_ai">6. Developing Software with Generative AI</h2>
<div class="sectionbody">
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="dlist">
<dl>
<dt class="hdlist1">Example</dt>
<dd>
<p>generative_ai_with_langchain/software_development/baby_dev.py</p>
</dd>
</dl>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<div id="footer">
<div id="footer-text">
Last updated 2024-01-08 15:06:20 +0200
</div>
</div>
</body>
</html>