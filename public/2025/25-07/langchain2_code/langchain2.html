<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Asciidoctor 2.0.23">
<meta name="author" content="Ben Auffarth, Leonid Kuligin">
<title>Generative AI with LangChain</title>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700">
<style>
/*! Asciidoctor default stylesheet | MIT License | https://asciidoctor.org */
/* Uncomment the following line when using as a custom stylesheet */
/* @import "https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700"; */
html{font-family:sans-serif;-webkit-text-size-adjust:100%}
a{background:none}
a:focus{outline:thin dotted}
a:active,a:hover{outline:0}
h1{font-size:2em;margin:.67em 0}
b,strong{font-weight:bold}
abbr{font-size:.9em}
abbr[title]{cursor:help;border-bottom:1px dotted #dddddf;text-decoration:none}
dfn{font-style:italic}
hr{height:0}
mark{background:#ff0;color:#000}
code,kbd,pre,samp{font-family:monospace;font-size:1em}
pre{white-space:pre-wrap}
q{quotes:"\201C" "\201D" "\2018" "\2019"}
small{font-size:80%}
sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}
sup{top:-.5em}
sub{bottom:-.25em}
img{border:0}
svg:not(:root){overflow:hidden}
figure{margin:0}
audio,video{display:inline-block}
audio:not([controls]){display:none;height:0}
fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}
legend{border:0;padding:0}
button,input,select,textarea{font-family:inherit;font-size:100%;margin:0}
button,input{line-height:normal}
button,select{text-transform:none}
button,html input[type=button],input[type=reset],input[type=submit]{-webkit-appearance:button;cursor:pointer}
button[disabled],html input[disabled]{cursor:default}
input[type=checkbox],input[type=radio]{padding:0}
button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}
textarea{overflow:auto;vertical-align:top}
table{border-collapse:collapse;border-spacing:0}
*,::before,::after{box-sizing:border-box}
html,body{font-size:100%}
body{background:#fff;color:rgba(0,0,0,.8);padding:0;margin:0;font-family:"Noto Serif","DejaVu Serif",serif;line-height:1;position:relative;cursor:auto;-moz-tab-size:4;-o-tab-size:4;tab-size:4;word-wrap:anywhere;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased}
a:hover{cursor:pointer}
img,object,embed{max-width:100%;height:auto}
object,embed{height:100%}
img{-ms-interpolation-mode:bicubic}
.left{float:left!important}
.right{float:right!important}
.text-left{text-align:left!important}
.text-right{text-align:right!important}
.text-center{text-align:center!important}
.text-justify{text-align:justify!important}
.hide{display:none}
img,object,svg{display:inline-block;vertical-align:middle}
textarea{height:auto;min-height:50px}
select{width:100%}
.subheader,.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{line-height:1.45;color:#7a2518;font-weight:400;margin-top:0;margin-bottom:.25em}
div,dl,dt,dd,ul,ol,li,h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6,pre,form,p,blockquote,th,td{margin:0;padding:0}
a{color:#2156a5;text-decoration:underline;line-height:inherit}
a:hover,a:focus{color:#1d4b8f}
a img{border:0}
p{line-height:1.6;margin-bottom:1.25em;text-rendering:optimizeLegibility}
p aside{font-size:.875em;line-height:1.35;font-style:italic}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{font-family:"Open Sans","DejaVu Sans",sans-serif;font-weight:300;font-style:normal;color:#ba3925;text-rendering:optimizeLegibility;margin-top:1em;margin-bottom:.5em;line-height:1.0125em}
h1 small,h2 small,h3 small,#toctitle small,.sidebarblock>.content>.title small,h4 small,h5 small,h6 small{font-size:60%;color:#e99b8f;line-height:0}
h1{font-size:2.125em}
h2{font-size:1.6875em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.375em}
h4,h5{font-size:1.125em}
h6{font-size:1em}
hr{border:solid #dddddf;border-width:1px 0 0;clear:both;margin:1.25em 0 1.1875em}
em,i{font-style:italic;line-height:inherit}
strong,b{font-weight:bold;line-height:inherit}
small{font-size:60%;line-height:inherit}
code{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;font-weight:400;color:rgba(0,0,0,.9)}
ul,ol,dl{line-height:1.6;margin-bottom:1.25em;list-style-position:outside;font-family:inherit}
ul,ol{margin-left:1.5em}
ul li ul,ul li ol{margin-left:1.25em;margin-bottom:0}
ul.circle{list-style-type:circle}
ul.disc{list-style-type:disc}
ul.square{list-style-type:square}
ul.circle ul:not([class]),ul.disc ul:not([class]),ul.square ul:not([class]){list-style:inherit}
ol li ul,ol li ol{margin-left:1.25em;margin-bottom:0}
dl dt{margin-bottom:.3125em;font-weight:bold}
dl dd{margin-bottom:1.25em}
blockquote{margin:0 0 1.25em;padding:.5625em 1.25em 0 1.1875em;border-left:1px solid #ddd}
blockquote,blockquote p{line-height:1.6;color:rgba(0,0,0,.85)}
@media screen and (min-width:768px){h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2}
h1{font-size:2.75em}
h2{font-size:2.3125em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.6875em}
h4{font-size:1.4375em}}
table{background:#fff;margin-bottom:1.25em;border:1px solid #dedede;word-wrap:normal}
table thead,table tfoot{background:#f7f8f7}
table thead tr th,table thead tr td,table tfoot tr th,table tfoot tr td{padding:.5em .625em .625em;font-size:inherit;color:rgba(0,0,0,.8);text-align:left}
table tr th,table tr td{padding:.5625em .625em;font-size:inherit;color:rgba(0,0,0,.8)}
table tr.even,table tr.alt{background:#f8f8f7}
table thead tr th,table tfoot tr th,table tbody tr td,table tr td,table tfoot tr td{line-height:1.6}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2;word-spacing:-.05em}
h1 strong,h2 strong,h3 strong,#toctitle strong,.sidebarblock>.content>.title strong,h4 strong,h5 strong,h6 strong{font-weight:400}
.center{margin-left:auto;margin-right:auto}
.stretch{width:100%}
.clearfix::before,.clearfix::after,.float-group::before,.float-group::after{content:" ";display:table}
.clearfix::after,.float-group::after{clear:both}
:not(pre).nobreak{word-wrap:normal}
:not(pre).nowrap{white-space:nowrap}
:not(pre).pre-wrap{white-space:pre-wrap}
:not(pre):not([class^=L])>code{font-size:.9375em;font-style:normal!important;letter-spacing:0;padding:.1em .5ex;word-spacing:-.15em;background:#f7f7f8;border-radius:4px;line-height:1.45;text-rendering:optimizeSpeed}
pre{color:rgba(0,0,0,.9);font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;line-height:1.45;text-rendering:optimizeSpeed}
pre code,pre pre{color:inherit;font-size:inherit;line-height:inherit}
pre>code{display:block}
pre.nowrap,pre.nowrap pre{white-space:pre;word-wrap:normal}
em em{font-style:normal}
strong strong{font-weight:400}
.keyseq{color:rgba(51,51,51,.8)}
kbd{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;display:inline-block;color:rgba(0,0,0,.8);font-size:.65em;line-height:1.45;background:#f7f7f7;border:1px solid #ccc;border-radius:3px;box-shadow:0 1px 0 rgba(0,0,0,.2),inset 0 0 0 .1em #fff;margin:0 .15em;padding:.2em .5em;vertical-align:middle;position:relative;top:-.1em;white-space:nowrap}
.keyseq kbd:first-child{margin-left:0}
.keyseq kbd:last-child{margin-right:0}
.menuseq,.menuref{color:#000}
.menuseq b:not(.caret),.menuref{font-weight:inherit}
.menuseq{word-spacing:-.02em}
.menuseq b.caret{font-size:1.25em;line-height:.8}
.menuseq i.caret{font-weight:bold;text-align:center;width:.45em}
b.button::before,b.button::after{position:relative;top:-1px;font-weight:400}
b.button::before{content:"[";padding:0 3px 0 2px}
b.button::after{content:"]";padding:0 2px 0 3px}
p a>code:hover{color:rgba(0,0,0,.9)}
#header,#content,#footnotes,#footer{width:100%;margin:0 auto;max-width:62.5em;*zoom:1;position:relative;padding-left:.9375em;padding-right:.9375em}
#header::before,#header::after,#content::before,#content::after,#footnotes::before,#footnotes::after,#footer::before,#footer::after{content:" ";display:table}
#header::after,#content::after,#footnotes::after,#footer::after{clear:both}
#content{margin-top:1.25em}
#content::before{content:none}
#header>h1:first-child{color:rgba(0,0,0,.85);margin-top:2.25rem;margin-bottom:0}
#header>h1:first-child+#toc{margin-top:8px;border-top:1px solid #dddddf}
#header>h1:only-child{border-bottom:1px solid #dddddf;padding-bottom:8px}
#header .details{border-bottom:1px solid #dddddf;line-height:1.45;padding-top:.25em;padding-bottom:.25em;padding-left:.25em;color:rgba(0,0,0,.6);display:flex;flex-flow:row wrap}
#header .details span:first-child{margin-left:-.125em}
#header .details span.email a{color:rgba(0,0,0,.85)}
#header .details br{display:none}
#header .details br+span::before{content:"\00a0\2013\00a0"}
#header .details br+span.author::before{content:"\00a0\22c5\00a0";color:rgba(0,0,0,.85)}
#header .details br+span#revremark::before{content:"\00a0|\00a0"}
#header #revnumber{text-transform:capitalize}
#header #revnumber::after{content:"\00a0"}
#content>h1:first-child:not([class]){color:rgba(0,0,0,.85);border-bottom:1px solid #dddddf;padding-bottom:8px;margin-top:0;padding-top:1rem;margin-bottom:1.25rem}
#toc{border-bottom:1px solid #e7e7e9;padding-bottom:.5em}
#toc>ul{margin-left:.125em}
#toc ul.sectlevel0>li>a{font-style:italic}
#toc ul.sectlevel0 ul.sectlevel1{margin:.5em 0}
#toc ul{font-family:"Open Sans","DejaVu Sans",sans-serif;list-style-type:none}
#toc li{line-height:1.3334;margin-top:.3334em}
#toc a{text-decoration:none}
#toc a:active{text-decoration:underline}
#toctitle{color:#7a2518;font-size:1.2em}
@media screen and (min-width:768px){#toctitle{font-size:1.375em}
body.toc2{padding-left:15em;padding-right:0}
body.toc2 #header>h1:nth-last-child(2){border-bottom:1px solid #dddddf;padding-bottom:8px}
#toc.toc2{margin-top:0!important;background:#f8f8f7;position:fixed;width:15em;left:0;top:0;border-right:1px solid #e7e7e9;border-top-width:0!important;border-bottom-width:0!important;z-index:1000;padding:1.25em 1em;height:100%;overflow:auto}
#toc.toc2 #toctitle{margin-top:0;margin-bottom:.8rem;font-size:1.2em}
#toc.toc2>ul{font-size:.9em;margin-bottom:0}
#toc.toc2 ul ul{margin-left:0;padding-left:1em}
#toc.toc2 ul.sectlevel0 ul.sectlevel1{padding-left:0;margin-top:.5em;margin-bottom:.5em}
body.toc2.toc-right{padding-left:0;padding-right:15em}
body.toc2.toc-right #toc.toc2{border-right-width:0;border-left:1px solid #e7e7e9;left:auto;right:0}}
@media screen and (min-width:1280px){body.toc2{padding-left:20em;padding-right:0}
#toc.toc2{width:20em}
#toc.toc2 #toctitle{font-size:1.375em}
#toc.toc2>ul{font-size:.95em}
#toc.toc2 ul ul{padding-left:1.25em}
body.toc2.toc-right{padding-left:0;padding-right:20em}}
#content #toc{border:1px solid #e0e0dc;margin-bottom:1.25em;padding:1.25em;background:#f8f8f7;border-radius:4px}
#content #toc>:first-child{margin-top:0}
#content #toc>:last-child{margin-bottom:0}
#footer{max-width:none;background:rgba(0,0,0,.8);padding:1.25em}
#footer-text{color:hsla(0,0%,100%,.8);line-height:1.44}
#content{margin-bottom:.625em}
.sect1{padding-bottom:.625em}
@media screen and (min-width:768px){#content{margin-bottom:1.25em}
.sect1{padding-bottom:1.25em}}
.sect1:last-child{padding-bottom:0}
.sect1+.sect1{border-top:1px solid #e7e7e9}
#content h1>a.anchor,h2>a.anchor,h3>a.anchor,#toctitle>a.anchor,.sidebarblock>.content>.title>a.anchor,h4>a.anchor,h5>a.anchor,h6>a.anchor{position:absolute;z-index:1001;width:1.5ex;margin-left:-1.5ex;display:block;text-decoration:none!important;visibility:hidden;text-align:center;font-weight:400}
#content h1>a.anchor::before,h2>a.anchor::before,h3>a.anchor::before,#toctitle>a.anchor::before,.sidebarblock>.content>.title>a.anchor::before,h4>a.anchor::before,h5>a.anchor::before,h6>a.anchor::before{content:"\00A7";font-size:.85em;display:block;padding-top:.1em}
#content h1:hover>a.anchor,#content h1>a.anchor:hover,h2:hover>a.anchor,h2>a.anchor:hover,h3:hover>a.anchor,#toctitle:hover>a.anchor,.sidebarblock>.content>.title:hover>a.anchor,h3>a.anchor:hover,#toctitle>a.anchor:hover,.sidebarblock>.content>.title>a.anchor:hover,h4:hover>a.anchor,h4>a.anchor:hover,h5:hover>a.anchor,h5>a.anchor:hover,h6:hover>a.anchor,h6>a.anchor:hover{visibility:visible}
#content h1>a.link,h2>a.link,h3>a.link,#toctitle>a.link,.sidebarblock>.content>.title>a.link,h4>a.link,h5>a.link,h6>a.link{color:#ba3925;text-decoration:none}
#content h1>a.link:hover,h2>a.link:hover,h3>a.link:hover,#toctitle>a.link:hover,.sidebarblock>.content>.title>a.link:hover,h4>a.link:hover,h5>a.link:hover,h6>a.link:hover{color:#a53221}
details,.audioblock,.imageblock,.literalblock,.listingblock,.stemblock,.videoblock{margin-bottom:1.25em}
details{margin-left:1.25rem}
details>summary{cursor:pointer;display:block;position:relative;line-height:1.6;margin-bottom:.625rem;outline:none;-webkit-tap-highlight-color:transparent}
details>summary::-webkit-details-marker{display:none}
details>summary::before{content:"";border:solid transparent;border-left:solid;border-width:.3em 0 .3em .5em;position:absolute;top:.5em;left:-1.25rem;transform:translateX(15%)}
details[open]>summary::before{border:solid transparent;border-top:solid;border-width:.5em .3em 0;transform:translateY(15%)}
details>summary::after{content:"";width:1.25rem;height:1em;position:absolute;top:.3em;left:-1.25rem}
.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{text-rendering:optimizeLegibility;text-align:left;font-family:"Noto Serif","DejaVu Serif",serif;font-size:1rem;font-style:italic}
table.tableblock.fit-content>caption.title{white-space:nowrap;width:0}
.paragraph.lead>p,#preamble>.sectionbody>[class=paragraph]:first-of-type p{font-size:1.21875em;line-height:1.6;color:rgba(0,0,0,.85)}
.admonitionblock>table{border-collapse:separate;border:0;background:none;width:100%}
.admonitionblock>table td.icon{text-align:center;width:80px}
.admonitionblock>table td.icon img{max-width:none}
.admonitionblock>table td.icon .title{font-weight:bold;font-family:"Open Sans","DejaVu Sans",sans-serif;text-transform:uppercase}
.admonitionblock>table td.content{padding-left:1.125em;padding-right:1.25em;border-left:1px solid #dddddf;color:rgba(0,0,0,.6);word-wrap:anywhere}
.admonitionblock>table td.content>:last-child>:last-child{margin-bottom:0}
.exampleblock>.content{border:1px solid #e6e6e6;margin-bottom:1.25em;padding:1.25em;background:#fff;border-radius:4px}
.sidebarblock{border:1px solid #dbdbd6;margin-bottom:1.25em;padding:1.25em;background:#f3f3f2;border-radius:4px}
.sidebarblock>.content>.title{color:#7a2518;margin-top:0;text-align:center}
.exampleblock>.content>:first-child,.sidebarblock>.content>:first-child{margin-top:0}
.exampleblock>.content>:last-child,.exampleblock>.content>:last-child>:last-child,.exampleblock>.content .olist>ol>li:last-child>:last-child,.exampleblock>.content .ulist>ul>li:last-child>:last-child,.exampleblock>.content .qlist>ol>li:last-child>:last-child,.sidebarblock>.content>:last-child,.sidebarblock>.content>:last-child>:last-child,.sidebarblock>.content .olist>ol>li:last-child>:last-child,.sidebarblock>.content .ulist>ul>li:last-child>:last-child,.sidebarblock>.content .qlist>ol>li:last-child>:last-child{margin-bottom:0}
.literalblock pre,.listingblock>.content>pre{border-radius:4px;overflow-x:auto;padding:1em;font-size:.8125em}
@media screen and (min-width:768px){.literalblock pre,.listingblock>.content>pre{font-size:.90625em}}
@media screen and (min-width:1280px){.literalblock pre,.listingblock>.content>pre{font-size:1em}}
.literalblock pre,.listingblock>.content>pre:not(.highlight),.listingblock>.content>pre[class=highlight],.listingblock>.content>pre[class^="highlight "]{background:#f7f7f8}
.literalblock.output pre{color:#f7f7f8;background:rgba(0,0,0,.9)}
.listingblock>.content{position:relative}
.listingblock code[data-lang]::before{display:none;content:attr(data-lang);position:absolute;font-size:.75em;top:.425rem;right:.5rem;line-height:1;text-transform:uppercase;color:inherit;opacity:.5}
.listingblock:hover code[data-lang]::before{display:block}
.listingblock.terminal pre .command::before{content:attr(data-prompt);padding-right:.5em;color:inherit;opacity:.5}
.listingblock.terminal pre .command:not([data-prompt])::before{content:"$"}
.listingblock pre.highlightjs{padding:0}
.listingblock pre.highlightjs>code{padding:1em;border-radius:4px}
.listingblock pre.prettyprint{border-width:0}
.prettyprint{background:#f7f7f8}
pre.prettyprint .linenums{line-height:1.45;margin-left:2em}
pre.prettyprint li{background:none;list-style-type:inherit;padding-left:0}
pre.prettyprint li code[data-lang]::before{opacity:1}
pre.prettyprint li:not(:first-child) code[data-lang]::before{display:none}
table.linenotable{border-collapse:separate;border:0;margin-bottom:0;background:none}
table.linenotable td[class]{color:inherit;vertical-align:top;padding:0;line-height:inherit;white-space:normal}
table.linenotable td.code{padding-left:.75em}
table.linenotable td.linenos,pre.pygments .linenos{border-right:1px solid;opacity:.35;padding-right:.5em;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}
pre.pygments span.linenos{display:inline-block;margin-right:.75em}
.quoteblock{margin:0 1em 1.25em 1.5em;display:table}
.quoteblock:not(.excerpt)>.title{margin-left:-1.5em;margin-bottom:.75em}
.quoteblock blockquote,.quoteblock p{color:rgba(0,0,0,.85);font-size:1.15rem;line-height:1.75;word-spacing:.1em;letter-spacing:0;font-style:italic;text-align:justify}
.quoteblock blockquote{margin:0;padding:0;border:0}
.quoteblock blockquote::before{content:"\201c";float:left;font-size:2.75em;font-weight:bold;line-height:.6em;margin-left:-.6em;color:#7a2518;text-shadow:0 1px 2px rgba(0,0,0,.1)}
.quoteblock blockquote>.paragraph:last-child p{margin-bottom:0}
.quoteblock .attribution{margin-top:.75em;margin-right:.5ex;text-align:right}
.verseblock{margin:0 1em 1.25em}
.verseblock pre{font-family:"Open Sans","DejaVu Sans",sans-serif;font-size:1.15rem;color:rgba(0,0,0,.85);font-weight:300;text-rendering:optimizeLegibility}
.verseblock pre strong{font-weight:400}
.verseblock .attribution{margin-top:1.25rem;margin-left:.5ex}
.quoteblock .attribution,.verseblock .attribution{font-size:.9375em;line-height:1.45;font-style:italic}
.quoteblock .attribution br,.verseblock .attribution br{display:none}
.quoteblock .attribution cite,.verseblock .attribution cite{display:block;letter-spacing:-.025em;color:rgba(0,0,0,.6)}
.quoteblock.abstract blockquote::before,.quoteblock.excerpt blockquote::before,.quoteblock .quoteblock blockquote::before{display:none}
.quoteblock.abstract blockquote,.quoteblock.abstract p,.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{line-height:1.6;word-spacing:0}
.quoteblock.abstract{margin:0 1em 1.25em;display:block}
.quoteblock.abstract>.title{margin:0 0 .375em;font-size:1.15em;text-align:center}
.quoteblock.excerpt>blockquote,.quoteblock .quoteblock{padding:0 0 .25em 1em;border-left:.25em solid #dddddf}
.quoteblock.excerpt,.quoteblock .quoteblock{margin-left:0}
.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{color:inherit;font-size:1.0625rem}
.quoteblock.excerpt .attribution,.quoteblock .quoteblock .attribution{color:inherit;font-size:.85rem;text-align:left;margin-right:0}
p.tableblock:last-child{margin-bottom:0}
td.tableblock>.content{margin-bottom:1.25em;word-wrap:anywhere}
td.tableblock>.content>:last-child{margin-bottom:-1.25em}
table.tableblock,th.tableblock,td.tableblock{border:0 solid #dedede}
table.grid-all>*>tr>*{border-width:1px}
table.grid-cols>*>tr>*{border-width:0 1px}
table.grid-rows>*>tr>*{border-width:1px 0}
table.frame-all{border-width:1px}
table.frame-ends{border-width:1px 0}
table.frame-sides{border-width:0 1px}
table.frame-none>colgroup+*>:first-child>*,table.frame-sides>colgroup+*>:first-child>*{border-top-width:0}
table.frame-none>:last-child>:last-child>*,table.frame-sides>:last-child>:last-child>*{border-bottom-width:0}
table.frame-none>*>tr>:first-child,table.frame-ends>*>tr>:first-child{border-left-width:0}
table.frame-none>*>tr>:last-child,table.frame-ends>*>tr>:last-child{border-right-width:0}
table.stripes-all>*>tr,table.stripes-odd>*>tr:nth-of-type(odd),table.stripes-even>*>tr:nth-of-type(even),table.stripes-hover>*>tr:hover{background:#f8f8f7}
th.halign-left,td.halign-left{text-align:left}
th.halign-right,td.halign-right{text-align:right}
th.halign-center,td.halign-center{text-align:center}
th.valign-top,td.valign-top{vertical-align:top}
th.valign-bottom,td.valign-bottom{vertical-align:bottom}
th.valign-middle,td.valign-middle{vertical-align:middle}
table thead th,table tfoot th{font-weight:bold}
tbody tr th{background:#f7f8f7}
tbody tr th,tbody tr th p,tfoot tr th,tfoot tr th p{color:rgba(0,0,0,.8);font-weight:bold}
p.tableblock>code:only-child{background:none;padding:0}
p.tableblock{font-size:1em}
ol{margin-left:1.75em}
ul li ol{margin-left:1.5em}
dl dd{margin-left:1.125em}
dl dd:last-child,dl dd:last-child>:last-child{margin-bottom:0}
li p,ul dd,ol dd,.olist .olist,.ulist .ulist,.ulist .olist,.olist .ulist{margin-bottom:.625em}
ul.checklist,ul.none,ol.none,ul.no-bullet,ol.no-bullet,ol.unnumbered,ul.unstyled,ol.unstyled{list-style-type:none}
ul.no-bullet,ol.no-bullet,ol.unnumbered{margin-left:.625em}
ul.unstyled,ol.unstyled{margin-left:0}
li>p:empty:only-child::before{content:"";display:inline-block}
ul.checklist>li>p:first-child{margin-left:-1em}
ul.checklist>li>p:first-child>.fa-square-o:first-child,ul.checklist>li>p:first-child>.fa-check-square-o:first-child{width:1.25em;font-size:.8em;position:relative;bottom:.125em}
ul.checklist>li>p:first-child>input[type=checkbox]:first-child{margin-right:.25em}
ul.inline{display:flex;flex-flow:row wrap;list-style:none;margin:0 0 .625em -1.25em}
ul.inline>li{margin-left:1.25em}
.unstyled dl dt{font-weight:400;font-style:normal}
ol.arabic{list-style-type:decimal}
ol.decimal{list-style-type:decimal-leading-zero}
ol.loweralpha{list-style-type:lower-alpha}
ol.upperalpha{list-style-type:upper-alpha}
ol.lowerroman{list-style-type:lower-roman}
ol.upperroman{list-style-type:upper-roman}
ol.lowergreek{list-style-type:lower-greek}
.hdlist>table,.colist>table{border:0;background:none}
.hdlist>table>tbody>tr,.colist>table>tbody>tr{background:none}
td.hdlist1,td.hdlist2{vertical-align:top;padding:0 .625em}
td.hdlist1{font-weight:bold;padding-bottom:1.25em}
td.hdlist2{word-wrap:anywhere}
.literalblock+.colist,.listingblock+.colist{margin-top:-.5em}
.colist td:not([class]):first-child{padding:.4em .75em 0;line-height:1;vertical-align:top}
.colist td:not([class]):first-child img{max-width:none}
.colist td:not([class]):last-child{padding:.25em 0}
.thumb,.th{line-height:0;display:inline-block;border:4px solid #fff;box-shadow:0 0 0 1px #ddd}
.imageblock.left{margin:.25em .625em 1.25em 0}
.imageblock.right{margin:.25em 0 1.25em .625em}
.imageblock>.title{margin-bottom:0}
.imageblock.thumb,.imageblock.th{border-width:6px}
.imageblock.thumb>.title,.imageblock.th>.title{padding:0 .125em}
.image.left,.image.right{margin-top:.25em;margin-bottom:.25em;display:inline-block;line-height:0}
.image.left{margin-right:.625em}
.image.right{margin-left:.625em}
a.image{text-decoration:none;display:inline-block}
a.image object{pointer-events:none}
sup.footnote,sup.footnoteref{font-size:.875em;position:static;vertical-align:super}
sup.footnote a,sup.footnoteref a{text-decoration:none}
sup.footnote a:active,sup.footnoteref a:active,#footnotes .footnote a:first-of-type:active{text-decoration:underline}
#footnotes{padding-top:.75em;padding-bottom:.75em;margin-bottom:.625em}
#footnotes hr{width:20%;min-width:6.25em;margin:-.25em 0 .75em;border-width:1px 0 0}
#footnotes .footnote{padding:0 .375em 0 .225em;line-height:1.3334;font-size:.875em;margin-left:1.2em;margin-bottom:.2em}
#footnotes .footnote a:first-of-type{font-weight:bold;text-decoration:none;margin-left:-1.05em}
#footnotes .footnote:last-of-type{margin-bottom:0}
#content #footnotes{margin-top:-.625em;margin-bottom:0;padding:.75em 0}
div.unbreakable{page-break-inside:avoid}
.big{font-size:larger}
.small{font-size:smaller}
.underline{text-decoration:underline}
.overline{text-decoration:overline}
.line-through{text-decoration:line-through}
.aqua{color:#00bfbf}
.aqua-background{background:#00fafa}
.black{color:#000}
.black-background{background:#000}
.blue{color:#0000bf}
.blue-background{background:#0000fa}
.fuchsia{color:#bf00bf}
.fuchsia-background{background:#fa00fa}
.gray{color:#606060}
.gray-background{background:#7d7d7d}
.green{color:#006000}
.green-background{background:#007d00}
.lime{color:#00bf00}
.lime-background{background:#00fa00}
.maroon{color:#600000}
.maroon-background{background:#7d0000}
.navy{color:#000060}
.navy-background{background:#00007d}
.olive{color:#606000}
.olive-background{background:#7d7d00}
.purple{color:#600060}
.purple-background{background:#7d007d}
.red{color:#bf0000}
.red-background{background:#fa0000}
.silver{color:#909090}
.silver-background{background:#bcbcbc}
.teal{color:#006060}
.teal-background{background:#007d7d}
.white{color:#bfbfbf}
.white-background{background:#fafafa}
.yellow{color:#bfbf00}
.yellow-background{background:#fafa00}
span.icon>.fa{cursor:default}
a span.icon>.fa{cursor:inherit}
.admonitionblock td.icon [class^="fa icon-"]{font-size:2.5em;text-shadow:1px 1px 2px rgba(0,0,0,.5);cursor:default}
.admonitionblock td.icon .icon-note::before{content:"\f05a";color:#19407c}
.admonitionblock td.icon .icon-tip::before{content:"\f0eb";text-shadow:1px 1px 2px rgba(155,155,0,.8);color:#111}
.admonitionblock td.icon .icon-warning::before{content:"\f071";color:#bf6900}
.admonitionblock td.icon .icon-caution::before{content:"\f06d";color:#bf3400}
.admonitionblock td.icon .icon-important::before{content:"\f06a";color:#bf0000}
.conum[data-value]{display:inline-block;color:#fff!important;background:rgba(0,0,0,.8);border-radius:50%;text-align:center;font-size:.75em;width:1.67em;height:1.67em;line-height:1.67em;font-family:"Open Sans","DejaVu Sans",sans-serif;font-style:normal;font-weight:bold}
.conum[data-value] *{color:#fff!important}
.conum[data-value]+b{display:none}
.conum[data-value]::after{content:attr(data-value)}
pre .conum[data-value]{position:relative;top:-.125em}
b.conum *{color:inherit!important}
.conum:not([data-value]):empty{display:none}
dt,th.tableblock,td.content,div.footnote{text-rendering:optimizeLegibility}
h1,h2,p,td.content,span.alt,summary{letter-spacing:-.01em}
p strong,td.content strong,div.footnote strong{letter-spacing:-.005em}
p,blockquote,dt,td.content,td.hdlist1,span.alt,summary{font-size:1.0625rem}
p{margin-bottom:1.25rem}
.sidebarblock p,.sidebarblock dt,.sidebarblock td.content,p.tableblock{font-size:1em}
.exampleblock>.content{background:#fffef7;border-color:#e0e0dc;box-shadow:0 1px 4px #e0e0dc}
.print-only{display:none!important}
@page{margin:1.25cm .75cm}
@media print{*{box-shadow:none!important;text-shadow:none!important}
html{font-size:80%}
a{color:inherit!important;text-decoration:underline!important}
a.bare,a[href^="#"],a[href^="mailto:"]{text-decoration:none!important}
a[href^="http:"]:not(.bare)::after,a[href^="https:"]:not(.bare)::after{content:"(" attr(href) ")";display:inline-block;font-size:.875em;padding-left:.25em}
abbr[title]{border-bottom:1px dotted}
abbr[title]::after{content:" (" attr(title) ")"}
pre,blockquote,tr,img,object,svg{page-break-inside:avoid}
thead{display:table-header-group}
svg{max-width:100%}
p,blockquote,dt,td.content{font-size:1em;orphans:3;widows:3}
h2,h3,#toctitle,.sidebarblock>.content>.title{page-break-after:avoid}
#header,#content,#footnotes,#footer{max-width:none}
#toc,.sidebarblock,.exampleblock>.content{background:none!important}
#toc{border-bottom:1px solid #dddddf!important;padding-bottom:0!important}
body.book #header{text-align:center}
body.book #header>h1:first-child{border:0!important;margin:2.5em 0 1em}
body.book #header .details{border:0!important;display:block;padding:0!important}
body.book #header .details span:first-child{margin-left:0!important}
body.book #header .details br{display:block}
body.book #header .details br+span::before{content:none!important}
body.book #toc{border:0!important;text-align:left!important;padding:0!important;margin:0!important}
body.book #toc,body.book #preamble,body.book h1.sect0,body.book .sect1>h2{page-break-before:always}
.listingblock code[data-lang]::before{display:block}
#footer{padding:0 .9375em}
.hide-on-print{display:none!important}
.print-only{display:block!important}
.hide-for-print{display:none!important}
.show-for-print{display:inherit!important}}
@media amzn-kf8,print{#header>h1:first-child{margin-top:1.25rem}
.sect1{padding:0!important}
.sect1+.sect1{border:0}
#footer{background:none}
#footer-text{color:rgba(0,0,0,.6);font-size:.9em}}
@media amzn-kf8{#header,#content,#footnotes,#footer{padding:0}}
</style>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
</head>
<body class="article toc2 toc-left">
<div id="header">
<h1>Generative AI with LangChain</h1>
<div class="details">
<span id="author" class="author">Ben Auffarth, Leonid Kuligin</span><br>
</div>
<div id="toc" class="toc2">
<div id="toctitle">Table of Contents</div>
<ul class="sectlevel1">
<li><a href="#_chapter_1_the_rise_of_generative_ai_from_language_models_to_agents">Chapter 1: The Rise of Generative AI: From Language Models to Agents</a>
<ul class="sectlevel2">
<li><a href="#_1_1_the_modern_llm_landscape">1.1 The modern LLM landscape</a></li>
<li><a href="#_1_2_from_models_to_agentic_applications">1.2 From models to agentic applications</a></li>
<li><a href="#_1_3_introducing_langchain">1.3 Introducing LangChain</a></li>
</ul>
</li>
<li><a href="#_chapter_2_first_steps_with_langchain">Chapter 2: First Steps with LangChain</a>
<ul class="sectlevel2">
<li><a href="#_2_2_exploring_langchains_building_blocks">2.2 Exploring LangChain’s building blocks</a></li>
<li><a href="#_2_3_running_local_models">2.3 Running local models</a></li>
<li><a href="#_2_4_multimodal_ai_applications">2.4 Multimodal AI applications</a>
<ul class="sectlevel3">
<li><a href="#_text_to_image_generation_with_langchain">Text-to-Image Generation with LangChain</a></li>
<li><a href="#_image_understanding_with_multimodal_models">Image Understanding with Multimodal Models</a></li>
<li><a href="#_summary">Summary</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#_chapter_3_building_workflows_with_langgraph">Chapter 3: Building Workflows with LangGraph</a>
<ul class="sectlevel2">
<li><a href="#_3_1_langgraph_fundamentals">3.1 LangGraph fundamentals</a></li>
<li><a href="#_3_2_prompt_engineering">3.2 Prompt engineering</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="content">
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>Build production ready LLM applications and advanced agents using Python and LangGraph, 2nd Edition</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_chapter_1_the_rise_of_generative_ai_from_language_models_to_agents">Chapter 1: The Rise of Generative AI: From Language Models to Agents</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_1_1_the_modern_llm_landscape">1.1 The modern LLM landscape</h3>
<div class="paragraph">
<p>The document provides a comprehensive overview of the evolution,
capabilities, and landscape of large language models (LLMs) and
generative AI, emphasizing their recent mainstream adoption and
practical applications.</p>
</div>
<div class="paragraph">
<p><strong>Key points include:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Generative AI Evolution:</strong> Starting with the 2017 transformer
architecture breakthrough, LLMs have grown from millions to billions of
parameters, unlocking emergent abilities like few-shot learning, complex
reasoning, and creative content generation. ChatGPT’s 2022 release
marked widespread public adoption.</p>
</li>
<li>
<p><strong>Limitations and Solutions:</strong> Despite their power, LLMs struggle with
tool use, complex reasoning, and maintaining context. Frameworks like
LangChain enable these models to act as agents that integrate external
tools, memory systems, and multi-step reasoning to solve real-world
problems effectively.</p>
</li>
<li>
<p><strong>Terminology:</strong></p>
<div class="ulist">
<ul>
<li>
<p><em>Tools</em> enable AI to interact with external systems.</p>
</li>
<li>
<p><em>Memory</em> supports contextual awareness across interactions.</p>
</li>
<li>
<p><em>Reinforcement Learning from Human Feedback (RLHF)</em> aligns models
with human preferences.</p>
</li>
<li>
<p><em>Agents</em> autonomously perceive, decide, and act using LLMs and tools.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Historical Timeline:</strong> From early statistical models in the 1990s to
deep learning and transformers, culminating in recent multimodal and
reasoning-enhanced models (e.g., OpenAI’s GPT-4o, Google’s Gemini,
DeepSeek’s R1).</p>
</li>
<li>
<p><strong>Model Comparison Factors:</strong></p>
<div class="ulist">
<ul>
<li>
<p><em>Open-source vs. closed-source:</em> Open-source models (e.g., Mistral,
LLaMA) offer transparency and local deployment; closed-source models
(e.g., GPT-4, Claude) provide API access with proprietary control.</p>
</li>
<li>
<p><em>Size and capabilities:</em> Larger models generally perform better but
require more resources; smaller models (SLMs) are efficient for limited
hardware.</p>
</li>
<li>
<p><em>Specialization:</em> Some models focus on tasks like code generation or
mathematical reasoning.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Scaling Laws:</strong> Empirical laws (Kaplan et al., Chinchilla) relate
model performance to size, data, and compute. Recent research suggests
architectural innovation and data quality improvements may reduce
reliance on sheer scale.</p>
</li>
<li>
<p><strong>Provider Landscape:</strong> Major providers include OpenAI, Anthropic,
Google, Cohere, Mistral AI, AWS, DeepSeek, and Together AI, each
offering models with distinct strengths in performance, efficiency,
multimodality, and cost.</p>
</li>
<li>
<p><strong>Licensing Considerations:</strong> Models vary from fully open-source
(permitting modification and local use) to proprietary API-only access.
Some, like Llama 2, offer permissive licenses with conditions. The Model
Openness Framework (MOF) helps evaluate transparency and usage rights.
Licensing impacts adoption, research, and commercial use.</p>
</li>
<li>
<p><strong>Future Outlook:</strong> The coexistence of massive general models and
smaller efficient models is expected, with ongoing advances in
architecture, training methods, and integration of external tools
enhancing AI capabilities.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Overall, the document highlights the rapid progress in generative AI,
the importance of understanding model characteristics and licensing, and
the shift toward agentic AI systems capable of autonomous, multi-step
problem-solving in practical applications.</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
<div class="olist arabic">
<ol class="arabic">
<li>
<p>How do the KM and Chinchilla scaling laws differ in their approach to optimizing the performance of large language models, and what implications do these differences have for the future development of AI architectures beyond simply increasing model size?</p>
</li>
<li>
<p>In what ways do LangChain agents leverage external tools and memory systems to overcome the inherent limitations of large language models, and how does this transformation enable practical, production-ready AI applications?</p>
</li>
<li>
<p>Considering the licensing spectrum described, how does the Model Openness Framework (MOF) evaluate language models, and what are the key trade-offs between open-source and proprietary LLM licenses in terms of accessibility, modification rights, and commercial use?</p>
</li>
</ol>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_1_2_from_models_to_agentic_applications">1.2 From models to agentic applications</h3>
<div class="paragraph">
<p>The text discusses the current state and limitations of large language
models (LLMs) and the emerging paradigm of agentic AI to overcome these
challenges.</p>
</div>
<div class="paragraph">
<p><strong>Key points:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Limitations of traditional LLMs:</strong></p>
<div class="ulist">
<ul>
<li>
<p>They are fundamentally reactive, lacking true understanding and prone
to hallucinations.</p>
</li>
<li>
<p>Struggle with complex reasoning, multi-step problem-solving, and
up-to-date knowledge due to static training data.</p>
</li>
<li>
<p>Cannot natively interact with external tools, APIs, or take
autonomous actions.</p>
</li>
<li>
<p>Exhibit biases and ethical concerns inherited from training data.</p>
</li>
<li>
<p>Require significant computational resources, impacting cost and
efficiency.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Need for agentic AI:</strong></p>
<div class="ulist">
<ul>
<li>
<p>Agentic AI systems extend LLMs by enabling planning, reasoning,
autonomous action, and interaction with external environments.</p>
</li>
<li>
<p>These AI agents integrate memory, tool use, decision-making
frameworks, and multi-step workflows to act independently with minimal
human oversight.</p>
</li>
<li>
<p>They transform passive language models into active problem solvers
and workflow automators.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Applications of LLMs and AI agents:</strong></p>
<div class="ulist">
<ul>
<li>
<p>Complex integrated applications augment human workflows with decision
support, content generation, and automation under human supervision.</p>
</li>
<li>
<p>Autonomous agents execute tasks independently, such as task
automation, information gathering, and multi-agent coordination.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Challenges for AI agents:</strong></p>
<div class="ulist">
<ul>
<li>
<p>Ensuring reliability, generalization across domains, user trust, and
effective coordination in multi-agent systems.</p>
</li>
<li>
<p>Practical issues like API rate limits, hallucination management,
cost, and scalability.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Frameworks like LangChain:</strong></p>
<div class="ulist">
<ul>
<li>
<p>Provide tools and architectures to build reliable, production-ready
AI agents.</p>
</li>
<li>
<p>Support memory management, tool integration, structured prompting,
and multi-step reasoning.</p>
</li>
<li>
<p>Help standardize agent development and address practical deployment
challenges.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Future outlook:</strong></p>
<div class="ulist">
<ul>
<li>
<p>AI agents represent a natural evolution from pattern-based models to
autonomous, reasoning-capable systems.</p>
</li>
<li>
<p>Advances in multimodal models, reinforcement learning, and
open-weight models will drive further innovation.</p>
</li>
<li>
<p>Agentic AI promises to expand AI’s impact across science,
engineering, and daily life by enabling autonomous, context-aware
decision-making and action.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>In summary, while LLMs excel at language generation, their reactive
nature and limitations necessitate the development of agentic AI systems
that can autonomously plan, reason, and act. Frameworks like LangChain
facilitate this transition, enabling the creation of sophisticated AI
agents that unlock new possibilities for automation and intelligent
decision-making.</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
<div class="olist arabic">
<ol class="arabic">
<li>
<p>How does the concept of agency differentiate AI agents from traditional LLMs in terms of autonomous decision-making and action-taking capabilities?</p>
</li>
<li>
<p>What are the specific practical challenges (e.g., rate limits, hallucination management) that production-ready AI agent systems must address, and how do frameworks like LangChain and LangSmith propose to solve them?</p>
</li>
<li>
<p>In what ways do AI agents extend LLM functionality through memory, tool use, and multi-step workflow execution to reduce human oversight and improve automation efficiency?</p>
</li>
</ol>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_1_3_introducing_langchain">1.3 Introducing LangChain</h3>
<div class="paragraph">
<p>The provided content offers a comprehensive overview of LangChain, a
leading open-source framework and company focused on accelerating the
development of applications powered by large language models (LLMs). Key
points include:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>LangChain Overview</strong></p>
<div class="ulist">
<ul>
<li>
<p>Founded by Harrison Chase in 2022, LangChain exists as both an
open-source framework and a venture-backed company based in San
Francisco.</p>
</li>
<li>
<p>It supports multiple programming languages (Python,
JavaScript/TypeScript, Go, Rust, Ruby) and has secured significant
funding, including a Series A in early 2024.</p>
</li>
<li>
<p>The core framework is open source, while the company offers enterprise
features and support.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Challenges with Raw LLMs</strong></p>
<div class="ulist">
<ul>
<li>
<p>LLMs have inherent limitations such as fixed context windows, limited
tool orchestration, and difficulty managing multi-step workflows.</p>
</li>
<li>
<p>These challenges affect reliability, resource management, and
integration complexity, necessitating frameworks like LangChain for
practical production use.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>LangChain’s Approach and Architecture</strong></p>
<div class="ulist">
<ul>
<li>
<p>Emphasizes modularity and composability, treating LLMs as components
integrated with tools and services.</p>
</li>
<li>
<p>Introduces the LangChain Expression Language (LCEL) for building
composable workflows.</p>
</li>
<li>
<p>Provides abstract interfaces for LLMs, embeddings, vector databases,
document loaders, and search engines, enabling easy switching between
providers.</p>
</li>
<li>
<p>Memory and agent management have evolved: LangGraph now handles
persistent state and agent workflows, while LangChain focuses on model
integration and workflow orchestration.</p>
</li>
<li>
<p>LangSmith offers observability tools for debugging, testing, and
monitoring.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Ecosystem and Adoption</strong></p>
<div class="ulist">
<ul>
<li>
<p>LangChain boasts over 20 million monthly downloads, 100,000+ GitHub
stars, and contributions from 4,000+ developers.</p>
</li>
<li>
<p>Core libraries include LangChain (Python and JS), LangGraph (Python
and JS), and platform services like LangSmith.</p>
</li>
<li>
<p>Numerous applications and extensions exist, such as ChatLangChain
(documentation assistant), Open Canvas (code/markdown UX), and various
AI agents.</p>
</li>
<li>
<p>Widely adopted by enterprises like Rakuten, Elastic, Ally, and Adyen
for improving LLM application development and deployment.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Modular Design and Dependency Management</strong></p>
<div class="ulist">
<ul>
<li>
<p>To handle rapid growth and numerous integrations, LangChain split its
monolithic codebase into specialized packages with lazy loading to
reduce dependency conflicts and simplify contributions.</p>
</li>
<li>
<p>The codebase is organized into core libraries, experimental features,
community integrations, and partner packages maintained both inside and
outside the main repository.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Companion Projects</strong></p>
<div class="ulist">
<ul>
<li>
<p><strong>LangGraph</strong>: Orchestration framework for stateful, multi-actor LLM
applications with support for streaming and human-in-the-loop.</p>
</li>
<li>
<p><strong>LangSmith</strong>: Platform for debugging, testing, monitoring, and
evaluating LLM applications.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Third-Party Visual Tools</strong></p>
<div class="ulist">
<ul>
<li>
<p>Tools like LangFlow and Flowise provide drag-and-drop visual
interfaces for building LangChain workflows, lowering barriers to
complex pipeline creation.</p>
</li>
<li>
<p>LangChain applications can be deployed locally or on cloud platforms.</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p><strong>Summary:</strong><br>
LangChain transforms raw LLMs into reliable, production-ready AI systems
by addressing fundamental limitations through a modular, composable
framework supported by a rich ecosystem of libraries, tools, and
services. Its architecture promotes flexibility, observability, and
vendor-agnostic development, enabling rapid, scalable, and maintainable
AI application development widely adopted across industries.</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
<div class="olist arabic">
<ol class="arabic">
<li>
<p>How does LangChain’s modular package architecture specifically address dependency conflicts and contribution bottlenecks that arise from its rapid expansion and extensive third-party integrations?</p>
</li>
<li>
<p>In what ways does LangGraph improve upon LangChain’s earlier memory and agent management approaches, particularly regarding persistent state, streaming support, and human-in-the-loop capabilities?</p>
</li>
<li>
<p>What are the unique advantages of LangChain’s vendor-agnostic integration ecosystem that enable seamless switching between LLM and embedding providers without rewriting core application logic?</p>
</li>
</ol>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_chapter_2_first_steps_with_langchain">Chapter 2: First Steps with LangChain</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_2_2_exploring_langchains_building_blocks">2.2 Exploring LangChain’s building blocks</h3>
<div class="paragraph">
<p>The document provides a comprehensive overview of working with large
language models (LLMs) and chat models using LangChain, focusing on
practical application development, model interfaces, prompt engineering,
and the new LangChain Expression Language (LCEL).</p>
</div>
<div class="paragraph">
<p><strong>Key points include:</strong></p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Model Interfaces and Usage:</strong></p>
<div class="ulist">
<ul>
<li>
<p>LangChain offers a unified interface to interact with various LLM
providers (OpenAI, Google Gemini, Anthropic Claude, etc.), enabling easy
switching between models with consistent code.</p>
</li>
<li>
<p>The traditional LLM interface (string input/output) is being
deprecated in favor of chat-based models, which handle multi-turn
conversations with structured messages (SystemMessage, HumanMessage,
AIMessage).</p>
</li>
<li>
<p>Example code demonstrates invoking jokes from OpenAI and Gemini models
using the same <code>invoke()</code> method.</p>
</li>
<li>
<p>Development testing can use <code>FakeListLLM</code> to simulate responses
without API calls.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Chat Models and Advanced Features:</strong></p>
<div class="ulist">
<ul>
<li>
<p>Chat models expect full conversation history as structured messages
each time, aligning with provider APIs.</p>
</li>
<li>
<p>Anthropic Claude 3.7 Sonnet supports "extended thinking," allowing the
model to show step-by-step reasoning before final answers, configurable
via token budgets.</p>
</li>
<li>
<p>Other providers (OpenAI, DeepSeek) offer reasoning control through
parameters like <code>reasoning_effort</code>.</p>
</li>
<li>
<p>Model behavior can be finely controlled using parameters such as
temperature, top-k, top-p, max tokens, presence/frequency penalties, and
stop sequences, with provider-specific nuances.</p>
</li>
<li>
<p>Parameter tuning depends on application needs: low temperature for
factual consistency, higher for creativity.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Prompt Engineering and Templates:</strong></p>
<div class="ulist">
<ul>
<li>
<p>LangChain’s prompt templates enable dynamic, maintainable, and
testable prompt generation with variable substitution.</p>
</li>
<li>
<p>Chat prompt templates support role-based structured messages for chat
models.</p>
</li>
<li>
<p>Templates improve consistency, maintainability, readability, and
scalability in production environments.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>LangChain Expression Language (LCEL):</strong></p>
<div class="ulist">
<ul>
<li>
<p>LCEL is a declarative, pipe-based syntax introduced in 2023 for
building complex LLM workflows by connecting components (prompts, LLMs,
parsers, functions) as Runnables.</p>
</li>
<li>
<p>The pipe operator (<code>|</code>) chains components sequentially, simplifying
workflow construction and improving readability.</p>
</li>
<li>
<p>LCEL supports synchronous/asynchronous execution, streaming, batching,
and easy integration with LangChain ecosystem tools (LangSmith,
LangServe).</p>
</li>
<li>
<p>Examples show simple joke generation and complex multi-stage workflows
(story generation + analysis) preserving context and structured outputs.</p>
</li>
<li>
<p>LCEL automatically converts functions and dictionaries into Runnable
components, enabling flexible data transformations and branching.</p>
</li>
<li>
<p>LCEL replaces older Chain classes, offering faster development, better
composability, and runtime optimization.</p>
</li>
<li>
<p>For advanced state management and branching, LangGraph is recommended
(covered in later chapters).</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Deployment Flexibility:</strong></p>
<div class="ulist">
<ul>
<li>
<p>LangChain supports both cloud-based and local model deployments
seamlessly, allowing developers to choose based on their needs.</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p><strong>Summary:</strong> This guide emphasizes modern best practices for building
LLM-powered applications with LangChain, advocating chat-based models,
prompt templates, and especially the new LCEL declarative syntax for
composing workflows. It covers practical coding examples, model behavior
tuning, reasoning capabilities, and scalable prompt management,
providing a solid foundation for developing robust, maintainable, and
flexible LLM applications.</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
<div class="olist arabic">
<ol class="arabic">
<li>
<p>How does LangChain’s LangChain Expression Language (LCEL) enable seamless chaining of multiple LLM calls while preserving and transforming data throughout the workflow, and what are the key components or utilities involved in managing structured outputs and context in such complex chains?</p>
</li>
<li>
<p>What are the differences in usage and advantages between the deprecated traditional LLM interface and the recommended chat model interface in LangChain, including how message roles and content are structured and why chat models are preferred for modern multi-turn conversational workflows?</p>
</li>
<li>
<p>How can developers control and fine-tune the behavior of different LLM providers (such as OpenAI, Anthropic, and Google Gemini) using parameters like temperature, top-k, top-p, and reasoning effort within LangChain, and what provider-specific considerations should be taken into account for achieving desired output consistency or creativity?</p>
</li>
</ol>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_2_3_running_local_models">2.3 Running local models</h3>
<div class="paragraph">
<p>The content discusses considerations and practical guidance for running
large language models (LLMs) locally versus in the cloud when building
LangChain applications.</p>
</div>
<div class="paragraph">
<p><strong>Local vs Cloud Models:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><em>Local models</em> offer full data control, privacy, no API costs, offline
use, and parameter tuning but require hardware and setup.</p>
</li>
<li>
<p><em>Cloud models</em> provide access to powerful, up-to-date models with
elastic scaling and no infrastructure management but depend on internet
and incur costs.</p>
</li>
<li>
<p>Local models are ideal for privacy-sensitive, offline, development, or
cost-sensitive high-volume use cases.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Running Local Models with LangChain:</strong></p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Ollama Integration:</strong></p>
<div class="ulist">
<ul>
<li>
<p>Ollama enables easy local use of open-source models.</p>
</li>
<li>
<p>Installation: <code>pip install langchain-ollama</code></p>
</li>
<li>
<p>Pull models via CLI (e.g., <code>ollama pull deepseek-r1:1.5b</code>) and start
server (<code>ollama serve</code>).</p>
</li>
<li>
<p>LangChain’s LCEL chains work seamlessly with Ollama models without API
keys.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Hugging Face Integration:</strong></p>
<div class="ulist">
<ul>
<li>
<p>Use <code>HuggingFacePipeline</code> for local model runs.</p>
</li>
<li>
<p>Example with TinyLlama model for text generation.</p>
</li>
<li>
<p>Initial downloads may be slow; usage is similar to other LangChain
LLMs.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Other Local Integrations:</strong></p>
<div class="ulist">
<ul>
<li>
<p>llama.cpp for efficient LLaMA model inference on consumer hardware.</p>
</li>
<li>
<p>GPT4All for lightweight local models.</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p><strong>Tips for Local Model Usage:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Resource Management:</strong></p>
<div class="ulist">
<ul>
<li>
<p>Use quantized models (e.g., 4-bit) to reduce memory footprint.</p>
</li>
<li>
<p>Configure GPU and CPU threads according to hardware.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Error Handling:</strong></p>
<div class="ulist">
<ul>
<li>
<p>Implement retry logic for common errors like out-of-memory or
timeouts.</p>
</li>
<li>
<p>Handle model loading failures and context length issues gracefully.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Example code snippets demonstrate configuring models and safe
invocation with retries.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Summary:</strong></p>
</div>
<div class="paragraph">
<p>LangChain supports flexible deployment of LLMs locally or in the cloud,
with developer-friendly integrations like Ollama and Hugging Face for
local use. Proper resource tuning and error handling are key to robust
local deployments. This foundation enables building text-based
applications and sets the stage for extending to multimodal AI
capabilities such as image generation and understanding.</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
<div class="olist arabic">
<ol class="arabic">
<li>
<p>How can you configure Ollama’s local model parameters in LangChain to optimize memory usage and processing speed on a consumer-grade desktop with a single GPU and 4 CPU cores?</p>
</li>
<li>
<p>What is a recommended Python error-handling pattern for safely invoking local LLMs in LangChain that addresses common issues like CUDA out-of-memory errors and model loading failures?</p>
</li>
<li>
<p>How does the LangChain LCEL chaining pattern maintain model-agnosticism when switching from cloud-based LLMs to local models such as Ollama or Hugging Face pipelines?</p>
</li>
</ol>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_2_4_multimodal_ai_applications">2.4 Multimodal AI applications</h3>
<div class="paragraph">
<p>The content explains the distinction between two advanced AI
capabilities:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Multimodal Understanding</strong> – AI models that simultaneously process and
reason across multiple input types (text, images, audio, video,
structured data). Examples include Gemini 2.5, GPT-4V, Sonnet 3.7, and
Llama 4. These models can analyze relationships between modalities and
perform complex reasoning, such as interpreting a chart image alongside
a text question.</p>
</li>
<li>
<p><strong>Content Generation</strong> – Specialized AI models focused on creating
specific media types with high quality, such as text-to-image
(Midjourney, DALL-E, Stable Diffusion), text-to-video (Sora, Pika), and
text-to-audio (Suno, ElevenLabs). These models excel at generating
content but have limited understanding capabilities.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>The LangChain framework supports both workflows, enabling developers to
integrate multimodal understanding and content generation into
applications easily.</p>
</div>
<div class="sect3">
<h4 id="_text_to_image_generation_with_langchain">Text-to-Image Generation with LangChain</h4>
<div class="ulist">
<ul>
<li>
<p>LangChain provides wrappers for popular image generation models like
OpenAI’s DALL-E and Stability AI’s Stable Diffusion 3.5 Large.</p>
</li>
<li>
<p>Example code demonstrates generating images from text prompts using
DALL-E and Stable Diffusion, with control over parameters like image
size, quality, prompt strength, and style.</p>
</li>
<li>
<p>Images generated illustrate the models’ ability to create detailed
technical diagrams and artistic visuals.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_image_understanding_with_multimodal_models">Image Understanding with Multimodal Models</h4>
<div class="ulist">
<ul>
<li>
<p>Modern multimodal models (e.g., Gemini 1.5 Pro, GPT-4 Vision) can
interpret images contextually, going beyond traditional computer vision
tasks.</p>
</li>
<li>
<p>LangChain uses a unified <code>ChatModel</code> interface to handle multimodal
inputs, allowing mixing of text and images in prompts.</p>
</li>
<li>
<p>Images can be sent by value (base64-encoded) or by reference (e.g.,
Google Cloud Storage URIs).</p>
</li>
<li>
<p>Examples show how to send images and videos for analysis, including
specifying video segments.</p>
</li>
<li>
<p>GPT-4 Vision integration enables detailed image analysis and answering
questions about visual content, demonstrated with a futuristic cityscape
image.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_summary">Summary</h4>
<div class="ulist">
<ul>
<li>
<p>Multimodal understanding AI models enable reasoning across diverse
data types simultaneously.</p>
</li>
<li>
<p>Content generation models specialize in producing high-quality media
from text prompts.</p>
</li>
<li>
<p>LangChain facilitates both by providing standardized interfaces and
integrations for image generation and multimodal input handling.</p>
</li>
<li>
<p>Practical examples illustrate generating images with DALL-E and Stable
Diffusion, and analyzing images with Gemini and GPT-4 Vision.</p>
</li>
<li>
<p>These capabilities empower developers to build sophisticated
applications combining visual and textual reasoning.</p>
</li>
</ul>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
<div class="olist arabic">
<ol class="arabic">
<li>
<p>How does LangChain’s unified interface handle multimodal inputs differently when sending image data by value (base64 encoding) versus by reference (e.g., Google Cloud Storage URIs), and what are the practical implications for model providers like Gemini?</p>
</li>
<li>
<p>What specific parameter settings (e.g., prompt_strength, cfg, steps, aspect_ratio) are recommended for Stable Diffusion 3.5 Large within LangChain to balance image quality and prompt adherence, and how do these parameters influence the generated image characteristics?</p>
</li>
<li>
<p>In the example of GPT-4 Vision analyzing a futuristic cityscape image, how does the model’s multimodal reasoning manifest in its detailed responses about objects, mood, and presence of people, illustrating the difference between traditional computer vision and modern multimodal understanding?</p>
</li>
</ol>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_chapter_3_building_workflows_with_langgraph">Chapter 3: Building Workflows with LangGraph</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_3_1_langgraph_fundamentals">3.1 LangGraph fundamentals</h3>
<div class="paragraph">
<p><strong>Purpose and differentiator</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>LangGraph (from LangChain) is an orchestration framework for complex
AI workflows. It supports features many others don’t: cycles in graphs
(not only DAGs), streaming, pre-built loops/components for generative AI
(e.g., human moderation), and a low-level API for granular control.</p>
</li>
<li>
<p><em>Recommendation:</em> take the free LangGraph course at
<a href="https://academy.langchain.com/" class="bare">https://academy.langchain.com/</a> for deeper learning.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Basic concepts</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><em>Graphs/workflows:</em> composed of nodes (Python functions) and edges;
START and END are reserved nodes.</p>
</li>
<li>
<p><em>State:</em> a workflow-wide context (think Python dict / TypedDict /
dataclass / Pydantic model). Nodes receive an immutable snapshot of
state and must return a dict of key-value updates. LangGraph merges
these updates into the master state—preventing side effects and making
changes explicit.</p>
</li>
<li>
<p><em>Execution model:</em> runs in discrete iterations called supersteps
(inspired by Pregel), possibly executing nodes in parallel and merging
their state updates. You can control recursion limits and max supersteps
to avoid infinite cycles.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Conditional edges and control flow</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Conditional edges allow branching based on current state. A
conditional function takes state and returns a destination (or a typed
Literal). If no type hint is present, you can pass an explicit mapping
to <code>add_conditional_edges</code>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Reducers (how state keys are merged)</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Default reducer replaces values.</p>
</li>
<li>
<p><em>Built-in/add reducers:</em> use <code>typing.Annotated</code> with an operator (e.g.,
add) to concatenate lists.</p>
</li>
<li>
<p><em>Custom reducers:</em> you can supply your own function to control how old
and new values merge.</p>
</li>
<li>
<p><code>add_messages</code> reducer and <code>MessagesState</code> exist to handle conversational
message merging (useful for chat/LLM workflows).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Configurability</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Nodes can accept a second argument: <code>RunnableConfig</code> (typed dict). It
includes execution controls (e.g., <code>recursion_limit</code>) and a configurable
dict for user parameters (e.g., switching <code>model_provider</code>/<code>model_name</code> at
runtime).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Controlled/structured generation and output parsing</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>LLM outputs are often free text; to feed downstream steps reliably,
use controlled generation and output parsers.</p>
</li>
<li>
<p>LangChain provides OutputParsers (Enum, JSON, CSV, Pydantic,
DataFrame, etc.), each implementing <code>BaseGenerationOutputParser</code>.</p>
</li>
<li>
<p>Example: <code>EnumOutputParser</code> converts text like "YES" / "NO" into Enum
values. Parsers can be chained with LLMs and embedded inside LangGraph
nodes.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Error handling and robustness</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Failures can occur anywhere (API/network/LLM outputs). Recommended
tactics:</p>
<div class="ulist">
<ul>
<li>
<p>Catch exceptions, log them, and return safe/default outputs so the
workflow can continue.</p>
</li>
<li>
<p>Convert exceptions into text for downstream LLMs to reason about
recovery.</p>
</li>
<li>
<p>Use fake chat models provided by LangChain for testing
(<code>GenericFakeChatModel</code>, <code>FakeChatModel</code>, <code>FakeListChatModel</code>).</p>
</li>
</ul>
</div>
</li>
<li>
<p>Retries:</p>
<div class="ulist">
<ul>
<li>
<p>Generic Runnable retry via <code>.with_retry</code> (configure exception types,
wait/backoff, attempts).</p>
</li>
<li>
<p>Node-level RetryPolicy in LangGraph (<code>retry_on</code>, <code>max_attempts</code>).</p>
</li>
<li>
<p>Semantic retry (<code>RetryWithErrorOutputParser</code>): when parsing fails, feed
the original prompt, completion, and parse error back to an LLM that
attempts to fix the output.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Fallbacks:</p>
<div class="ulist">
<ul>
<li>
<p>Define alternative runnables/chains to run if primary execution fails
(<code>with_fallbacks</code>).</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Takeaways</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>LangGraph provides a stateful, flexible orchestration model optimized
for generative AI workflows: explicit immutable state updates, reducers
to manage merged values, conditional edges and cycles, configurable
execution, and built-in mechanisms for parsing, retrying, and falling
back when integrating LLMs.</p>
</li>
<li>
<p>For complex agentic or multi-step workflows, refer to later chapters
(5 and 6) and LangChain docs/examples for advanced patterns.</p>
</li>
</ul>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
<div class="olist arabic">
<ol class="arabic">
<li>
<p>What are the reserved built-in nodes used to mark the beginning and end of a LangGraph workflow?</p>
</li>
<li>
<p>How does LangGraph provide state to nodes and how must nodes update the workflow state?</p>
</li>
<li>
<p>What is a "superstep" in LangGraph and which system inspired this execution model?</p>
</li>
</ol>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_3_2_prompt_engineering">3.2 Prompt engineering</h3>
<div class="ulist">
<ul>
<li>
<p>Prompt engineering vs prompt design</p>
<div class="ulist">
<ul>
<li>
<p>Prompt engineering: tuning a prompt (or template) to improve
performance on a specific task.</p>
</li>
<li>
<p>Prompt design: creating more general prompts that elicit better
responses across many tasks.</p>
</li>
<li>
<p>Both build on in-context learning: LLMs can adapt to new tasks via
natural‑language instructions and examples.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Key prompt abstractions in LangChain</p>
<div class="ulist">
<ul>
<li>
<p><strong>PromptTemplate:</strong> serializable runnable template that substitutes
variables at runtime.</p>
</li>
<li>
<p><strong>ChatPromptTemplate / message templates:</strong> build chat-style inputs
(system/human/assistant). You can pass messages as a list or as tuples
for convenience.</p>
</li>
<li>
<p><strong>Placeholders / MessagesPlaceholder:</strong> insert lists of messages at
runtime (common use: conversation history).</p>
</li>
<li>
<p><strong>Partial templates and composition:</strong> partially fill templates, add
functions as values, concatenate templates, or use
PipelinePromptTemplate to build larger prompts.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Zero-shot vs few-shot prompting</p>
<div class="ulist">
<ul>
<li>
<p><strong>Zero-shot:</strong> task description without examples. Useful tricks: set a
role/persona and be explicit about style (creative, concise, factual).</p>
</li>
<li>
<p><strong>Gemini guideline:</strong> include persona, task, relevant context, and
desired format.</p>
</li>
<li>
<p><strong>Few-shot:</strong> include input–output examples to steer behavior. For large
example sets, load examples from files/db instead of hard-coding.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Dynamic few-shot / example selection</p>
<div class="ulist">
<ul>
<li>
<p>Select examples per input (semantic similarity, length/freshness
limits). LangChain provides built-in example selectors
(<code>langchain_core.example_selectors</code>) that can be plugged into
<strong>FewShotPromptTemplate</strong>.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Chain-of-Thought (CoT)</p>
<div class="ulist">
<ul>
<li>
<p>CoT prompts ask the model to show intermediate step-by-step reasoning
(e.g., <em>“Let’s think step by step”</em>), improving complex reasoning and math
performance.</p>
</li>
<li>
<p>Often used zero-shot or with few-shot examples; many
reasoning-oriented models are trained in CoT format.</p>
</li>
<li>
<p>Practical pattern: run a CoT chain, then parse/extract a concise
final answer from the CoT output.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Self-consistency</p>
<div class="ulist">
<ul>
<li>
<p>Sample multiple outputs at higher temperature, then aggregate (e.g.,
majority vote) to improve reliability for low-dimensional outputs like
classifications or extractions.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Model/provider considerations</p>
<div class="ulist">
<ul>
<li>
<p>Providers have different prompting recommendations (e.g., Anthropic
favors XML-like tags). Reasoning models (<code>o3-mini</code>, <code>gemini-flash-thinking</code>)
may have their own best practices and sometimes don’t need CoT/few-shot.</p>
</li>
<li>
<p>When switching providers, re-evaluate prompts and end-to-end
performance; you may need multiple prompt templates and dynamic
selection by provider.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Tools &amp; references</p>
<div class="ulist">
<ul>
<li>
<p>Papers: “The Prompt Report” (Schulhoff et al.) for prompt taxonomy;
CoT paper by Jason Wei et al.</p>
</li>
<li>
<p>LangSmith Hub and LangChain utilities provide reusable prompt
artifacts and example prompt templates.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Next concern</p>
<div class="ulist">
<ul>
<li>
<p>If prompts grow too long and exceed a model’s context window,
additional strategies are needed (topic introduced as lead-in to the
next chapter).</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
<div class="olist arabic">
<ol class="arabic">
<li>
<p>In the <code>MessagesPlaceholder</code> example, after invoking <code>chat_prompt_template.invoke</code> with <code>job_description="fake"</code> and <code>history=[("human", "hi!"), ("ai", "hi!")]</code>, how many messages are in the resulting messages list?</p>
</li>
<li>
<p>In the partial <code>PromptTemplate</code> example where <code>system_template.partial(a="a")</code> is invoked with <code>{"b": "b"}</code>, what exact text is printed?</p>
</li>
<li>
<p>What concise answer does the <code>final_chain.invoke</code> return for the question "Solve equation 2*x+5=15" in the Chain-of-Thought extraction example?</p>
</li>
</ol>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
</div>
<div id="footer">
<div id="footer-text">
Last updated 2025-11-08 23:06:43 +0200
</div>
</div>
</body>
</html>