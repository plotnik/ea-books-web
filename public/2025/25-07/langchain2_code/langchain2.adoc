= Generative AI with LangChain
:source-highlighter: coderay
:icons: font
:toc: left
:toclevels: 4
:example-caption:
Ben Auffarth, Leonid Kuligin

Build production ready LLM applications and advanced agents using Python and LangGraph, 2nd Edition

== Chapter 1: The Rise of Generative AI: From Language Models to Agents

=== 1.1 The modern LLM landscape

The document provides a comprehensive overview of the evolution,
capabilities, and landscape of large language models (LLMs) and
generative AI, emphasizing their recent mainstream adoption and
practical applications.

*Key points include:*

* *Generative AI Evolution:* Starting with the 2017 transformer
architecture breakthrough, LLMs have grown from millions to billions of
parameters, unlocking emergent abilities like few-shot learning, complex
reasoning, and creative content generation. ChatGPT’s 2022 release
marked widespread public adoption.
* *Limitations and Solutions:* Despite their power, LLMs struggle with
tool use, complex reasoning, and maintaining context. Frameworks like
LangChain enable these models to act as agents that integrate external
tools, memory systems, and multi-step reasoning to solve real-world
problems effectively.
* *Terminology:*
** _Tools_ enable AI to interact with external systems.
** _Memory_ supports contextual awareness across interactions.
** _Reinforcement Learning from Human Feedback (RLHF)_ aligns models
with human preferences.
** _Agents_ autonomously perceive, decide, and act using LLMs and tools.
* *Historical Timeline:* From early statistical models in the 1990s to
deep learning and transformers, culminating in recent multimodal and
reasoning-enhanced models (e.g., OpenAI’s GPT-4o, Google’s Gemini,
DeepSeek’s R1).
* *Model Comparison Factors:*
** _Open-source vs. closed-source:_ Open-source models (e.g., Mistral,
LLaMA) offer transparency and local deployment; closed-source models
(e.g., GPT-4, Claude) provide API access with proprietary control.
** _Size and capabilities:_ Larger models generally perform better but
require more resources; smaller models (SLMs) are efficient for limited
hardware.
** _Specialization:_ Some models focus on tasks like code generation or
mathematical reasoning.
* *Scaling Laws:* Empirical laws (Kaplan et al., Chinchilla) relate
model performance to size, data, and compute. Recent research suggests
architectural innovation and data quality improvements may reduce
reliance on sheer scale.
* *Provider Landscape:* Major providers include OpenAI, Anthropic,
Google, Cohere, Mistral AI, AWS, DeepSeek, and Together AI, each
offering models with distinct strengths in performance, efficiency,
multimodality, and cost.
* *Licensing Considerations:* Models vary from fully open-source
(permitting modification and local use) to proprietary API-only access.
Some, like Llama 2, offer permissive licenses with conditions. The Model
Openness Framework (MOF) helps evaluate transparency and usage rights.
Licensing impacts adoption, research, and commercial use.
* *Future Outlook:* The coexistence of massive general models and
smaller efficient models is expected, with ongoing advances in
architecture, training methods, and integration of external tools
enhancing AI capabilities.

Overall, the document highlights the rapid progress in generative AI,
the importance of understanding model characteristics and licensing, and
the shift toward agentic AI systems capable of autonomous, multi-step
problem-solving in practical applications.

[WARNING]
====
1. How do the KM and Chinchilla scaling laws differ in their approach to optimizing the performance of large language models, and what implications do these differences have for the future development of AI architectures beyond simply increasing model size?

2. In what ways do LangChain agents leverage external tools and memory systems to overcome the inherent limitations of large language models, and how does this transformation enable practical, production-ready AI applications?

3. Considering the licensing spectrum described, how does the Model Openness Framework (MOF) evaluate language models, and what are the key trade-offs between open-source and proprietary LLM licenses in terms of accessibility, modification rights, and commercial use?
====

=== 1.2 From models to agentic applications

The text discusses the current state and limitations of large language
models (LLMs) and the emerging paradigm of agentic AI to overcome these
challenges.

*Key points:*

* *Limitations of traditional LLMs:*
** They are fundamentally reactive, lacking true understanding and prone
to hallucinations.
** Struggle with complex reasoning, multi-step problem-solving, and
up-to-date knowledge due to static training data.
** Cannot natively interact with external tools, APIs, or take
autonomous actions.
** Exhibit biases and ethical concerns inherited from training data.
** Require significant computational resources, impacting cost and
efficiency.
* *Need for agentic AI:*
** Agentic AI systems extend LLMs by enabling planning, reasoning,
autonomous action, and interaction with external environments.
** These AI agents integrate memory, tool use, decision-making
frameworks, and multi-step workflows to act independently with minimal
human oversight.
** They transform passive language models into active problem solvers
and workflow automators.
* *Applications of LLMs and AI agents:*
** Complex integrated applications augment human workflows with decision
support, content generation, and automation under human supervision.
** Autonomous agents execute tasks independently, such as task
automation, information gathering, and multi-agent coordination.
* *Challenges for AI agents:*
** Ensuring reliability, generalization across domains, user trust, and
effective coordination in multi-agent systems.
** Practical issues like API rate limits, hallucination management,
cost, and scalability.
* *Frameworks like LangChain:*
** Provide tools and architectures to build reliable, production-ready
AI agents.
** Support memory management, tool integration, structured prompting,
and multi-step reasoning.
** Help standardize agent development and address practical deployment
challenges.
* *Future outlook:*
** AI agents represent a natural evolution from pattern-based models to
autonomous, reasoning-capable systems.
** Advances in multimodal models, reinforcement learning, and
open-weight models will drive further innovation.
** Agentic AI promises to expand AI’s impact across science,
engineering, and daily life by enabling autonomous, context-aware
decision-making and action.

In summary, while LLMs excel at language generation, their reactive
nature and limitations necessitate the development of agentic AI systems
that can autonomously plan, reason, and act. Frameworks like LangChain
facilitate this transition, enabling the creation of sophisticated AI
agents that unlock new possibilities for automation and intelligent
decision-making.

[WARNING]
====
1. How does the concept of agency differentiate AI agents from traditional LLMs in terms of autonomous decision-making and action-taking capabilities?

2. What are the specific practical challenges (e.g., rate limits, hallucination management) that production-ready AI agent systems must address, and how do frameworks like LangChain and LangSmith propose to solve them?

3. In what ways do AI agents extend LLM functionality through memory, tool use, and multi-step workflow execution to reduce human oversight and improve automation efficiency?
====


=== 1.3 Introducing LangChain

The provided content offers a comprehensive overview of LangChain, a
leading open-source framework and company focused on accelerating the
development of applications powered by large language models (LLMs). Key
points include:

[arabic]
. *LangChain Overview*
* Founded by Harrison Chase in 2022, LangChain exists as both an
open-source framework and a venture-backed company based in San
Francisco.
* It supports multiple programming languages (Python,
JavaScript/TypeScript, Go, Rust, Ruby) and has secured significant
funding, including a Series A in early 2024.
* The core framework is open source, while the company offers enterprise
features and support.
. *Challenges with Raw LLMs*
* LLMs have inherent limitations such as fixed context windows, limited
tool orchestration, and difficulty managing multi-step workflows.
* These challenges affect reliability, resource management, and
integration complexity, necessitating frameworks like LangChain for
practical production use.
. *LangChain’s Approach and Architecture*
* Emphasizes modularity and composability, treating LLMs as components
integrated with tools and services.
* Introduces the LangChain Expression Language (LCEL) for building
composable workflows.
* Provides abstract interfaces for LLMs, embeddings, vector databases,
document loaders, and search engines, enabling easy switching between
providers.
* Memory and agent management have evolved: LangGraph now handles
persistent state and agent workflows, while LangChain focuses on model
integration and workflow orchestration.
* LangSmith offers observability tools for debugging, testing, and
monitoring.
. *Ecosystem and Adoption*
* LangChain boasts over 20 million monthly downloads, 100,000+ GitHub
stars, and contributions from 4,000+ developers.
* Core libraries include LangChain (Python and JS), LangGraph (Python
and JS), and platform services like LangSmith.
* Numerous applications and extensions exist, such as ChatLangChain
(documentation assistant), Open Canvas (code/markdown UX), and various
AI agents.
* Widely adopted by enterprises like Rakuten, Elastic, Ally, and Adyen
for improving LLM application development and deployment.
. *Modular Design and Dependency Management*
* To handle rapid growth and numerous integrations, LangChain split its
monolithic codebase into specialized packages with lazy loading to
reduce dependency conflicts and simplify contributions.
* The codebase is organized into core libraries, experimental features,
community integrations, and partner packages maintained both inside and
outside the main repository.
. *Companion Projects*
* *LangGraph*: Orchestration framework for stateful, multi-actor LLM
applications with support for streaming and human-in-the-loop.
* *LangSmith*: Platform for debugging, testing, monitoring, and
evaluating LLM applications.
. *Third-Party Visual Tools*
* Tools like LangFlow and Flowise provide drag-and-drop visual
interfaces for building LangChain workflows, lowering barriers to
complex pipeline creation.
* LangChain applications can be deployed locally or on cloud platforms.

*Summary:* +
LangChain transforms raw LLMs into reliable, production-ready AI systems
by addressing fundamental limitations through a modular, composable
framework supported by a rich ecosystem of libraries, tools, and
services. Its architecture promotes flexibility, observability, and
vendor-agnostic development, enabling rapid, scalable, and maintainable
AI application development widely adopted across industries.

[WARNING]
====
1. How does LangChain’s modular package architecture specifically address dependency conflicts and contribution bottlenecks that arise from its rapid expansion and extensive third-party integrations?

2. In what ways does LangGraph improve upon LangChain’s earlier memory and agent management approaches, particularly regarding persistent state, streaming support, and human-in-the-loop capabilities?

3. What are the unique advantages of LangChain’s vendor-agnostic integration ecosystem that enable seamless switching between LLM and embedding providers without rewriting core application logic?
====

== Chapter 2: First Steps with LangChain

=== 2.2 Exploring LangChain’s building blocks

The document provides a comprehensive overview of working with large
language models (LLMs) and chat models using LangChain, focusing on
practical application development, model interfaces, prompt engineering,
and the new LangChain Expression Language (LCEL).

*Key points include:*

[arabic]
. *Model Interfaces and Usage:*
* LangChain offers a unified interface to interact with various LLM
providers (OpenAI, Google Gemini, Anthropic Claude, etc.), enabling easy
switching between models with consistent code.
* The traditional LLM interface (string input/output) is being
deprecated in favor of chat-based models, which handle multi-turn
conversations with structured messages (SystemMessage, HumanMessage,
AIMessage).
* Example code demonstrates invoking jokes from OpenAI and Gemini models
using the same `+invoke()+` method.
* Development testing can use `+FakeListLLM+` to simulate responses
without API calls.
. *Chat Models and Advanced Features:*
* Chat models expect full conversation history as structured messages
each time, aligning with provider APIs.
* Anthropic Claude 3.7 Sonnet supports "extended thinking," allowing the
model to show step-by-step reasoning before final answers, configurable
via token budgets.
* Other providers (OpenAI, DeepSeek) offer reasoning control through
parameters like `+reasoning_effort+`.
* Model behavior can be finely controlled using parameters such as
temperature, top-k, top-p, max tokens, presence/frequency penalties, and
stop sequences, with provider-specific nuances.
* Parameter tuning depends on application needs: low temperature for
factual consistency, higher for creativity.
. *Prompt Engineering and Templates:*
* LangChain’s prompt templates enable dynamic, maintainable, and
testable prompt generation with variable substitution.
* Chat prompt templates support role-based structured messages for chat
models.
* Templates improve consistency, maintainability, readability, and
scalability in production environments.
. *LangChain Expression Language (LCEL):*
* LCEL is a declarative, pipe-based syntax introduced in 2023 for
building complex LLM workflows by connecting components (prompts, LLMs,
parsers, functions) as Runnables.
* The pipe operator (`+|+`) chains components sequentially, simplifying
workflow construction and improving readability.
* LCEL supports synchronous/asynchronous execution, streaming, batching,
and easy integration with LangChain ecosystem tools (LangSmith,
LangServe).
* Examples show simple joke generation and complex multi-stage workflows
(story generation + analysis) preserving context and structured outputs.
* LCEL automatically converts functions and dictionaries into Runnable
components, enabling flexible data transformations and branching.
* LCEL replaces older Chain classes, offering faster development, better
composability, and runtime optimization.
* For advanced state management and branching, LangGraph is recommended
(covered in later chapters).
. *Deployment Flexibility:*
* LangChain supports both cloud-based and local model deployments
seamlessly, allowing developers to choose based on their needs.

*Summary:* This guide emphasizes modern best practices for building
LLM-powered applications with LangChain, advocating chat-based models,
prompt templates, and especially the new LCEL declarative syntax for
composing workflows. It covers practical coding examples, model behavior
tuning, reasoning capabilities, and scalable prompt management,
providing a solid foundation for developing robust, maintainable, and
flexible LLM applications.

[WARNING]
====
1. How does LangChain’s LangChain Expression Language (LCEL) enable seamless chaining of multiple LLM calls while preserving and transforming data throughout the workflow, and what are the key components or utilities involved in managing structured outputs and context in such complex chains?

2. What are the differences in usage and advantages between the deprecated traditional LLM interface and the recommended chat model interface in LangChain, including how message roles and content are structured and why chat models are preferred for modern multi-turn conversational workflows?

3. How can developers control and fine-tune the behavior of different LLM providers (such as OpenAI, Anthropic, and Google Gemini) using parameters like temperature, top-k, top-p, and reasoning effort within LangChain, and what provider-specific considerations should be taken into account for achieving desired output consistency or creativity?
====

=== 2.3 Running local models

The content discusses considerations and practical guidance for running
large language models (LLMs) locally versus in the cloud when building
LangChain applications.

*Local vs Cloud Models:*

* _Local models_ offer full data control, privacy, no API costs, offline
use, and parameter tuning but require hardware and setup.
* _Cloud models_ provide access to powerful, up-to-date models with
elastic scaling and no infrastructure management but depend on internet
and incur costs.
* Local models are ideal for privacy-sensitive, offline, development, or
cost-sensitive high-volume use cases.

*Running Local Models with LangChain:*

[arabic]
. *Ollama Integration:*
* Ollama enables easy local use of open-source models.
* Installation: `+pip install langchain-ollama+`
* Pull models via CLI (e.g., `+ollama pull deepseek-r1:1.5b+`) and start
server (`+ollama serve+`).
* LangChain’s LCEL chains work seamlessly with Ollama models without API
keys.
. *Hugging Face Integration:*
* Use `+HuggingFacePipeline+` for local model runs.
* Example with TinyLlama model for text generation.
* Initial downloads may be slow; usage is similar to other LangChain
LLMs.
. *Other Local Integrations:*
* llama.cpp for efficient LLaMA model inference on consumer hardware.
* GPT4All for lightweight local models.

*Tips for Local Model Usage:*

* *Resource Management:*
** Use quantized models (e.g., 4-bit) to reduce memory footprint.
** Configure GPU and CPU threads according to hardware.
* *Error Handling:*
** Implement retry logic for common errors like out-of-memory or
timeouts.
** Handle model loading failures and context length issues gracefully.
* Example code snippets demonstrate configuring models and safe
invocation with retries.

*Summary:*

LangChain supports flexible deployment of LLMs locally or in the cloud,
with developer-friendly integrations like Ollama and Hugging Face for
local use. Proper resource tuning and error handling are key to robust
local deployments. This foundation enables building text-based
applications and sets the stage for extending to multimodal AI
capabilities such as image generation and understanding.

[WARNING]
====
1. How can you configure Ollama’s local model parameters in LangChain to optimize memory usage and processing speed on a consumer-grade desktop with a single GPU and 4 CPU cores?

2. What is a recommended Python error-handling pattern for safely invoking local LLMs in LangChain that addresses common issues like CUDA out-of-memory errors and model loading failures?

3. How does the LangChain LCEL chaining pattern maintain model-agnosticism when switching from cloud-based LLMs to local models such as Ollama or Hugging Face pipelines?
====

=== 2.4 Multimodal AI applications

The content explains the distinction between two advanced AI
capabilities:

[arabic]
. *Multimodal Understanding* – AI models that simultaneously process and
reason across multiple input types (text, images, audio, video,
structured data). Examples include Gemini 2.5, GPT-4V, Sonnet 3.7, and
Llama 4. These models can analyze relationships between modalities and
perform complex reasoning, such as interpreting a chart image alongside
a text question.
. *Content Generation* – Specialized AI models focused on creating
specific media types with high quality, such as text-to-image
(Midjourney, DALL-E, Stable Diffusion), text-to-video (Sora, Pika), and
text-to-audio (Suno, ElevenLabs). These models excel at generating
content but have limited understanding capabilities.

The LangChain framework supports both workflows, enabling developers to
integrate multimodal understanding and content generation into
applications easily.

==== Text-to-Image Generation with LangChain

* LangChain provides wrappers for popular image generation models like
OpenAI’s DALL-E and Stability AI’s Stable Diffusion 3.5 Large.
* Example code demonstrates generating images from text prompts using
DALL-E and Stable Diffusion, with control over parameters like image
size, quality, prompt strength, and style.
* Images generated illustrate the models’ ability to create detailed
technical diagrams and artistic visuals.

==== Image Understanding with Multimodal Models

* Modern multimodal models (e.g., Gemini 1.5 Pro, GPT-4 Vision) can
interpret images contextually, going beyond traditional computer vision
tasks.
* LangChain uses a unified `+ChatModel+` interface to handle multimodal
inputs, allowing mixing of text and images in prompts.
* Images can be sent by value (base64-encoded) or by reference (e.g.,
Google Cloud Storage URIs).
* Examples show how to send images and videos for analysis, including
specifying video segments.
* GPT-4 Vision integration enables detailed image analysis and answering
questions about visual content, demonstrated with a futuristic cityscape
image.

==== Summary

* Multimodal understanding AI models enable reasoning across diverse
data types simultaneously.
* Content generation models specialize in producing high-quality media
from text prompts.
* LangChain facilitates both by providing standardized interfaces and
integrations for image generation and multimodal input handling.
* Practical examples illustrate generating images with DALL-E and Stable
Diffusion, and analyzing images with Gemini and GPT-4 Vision.
* These capabilities empower developers to build sophisticated
applications combining visual and textual reasoning.

[WARNING]
====
1. How does LangChain’s unified interface handle multimodal inputs differently when sending image data by value (base64 encoding) versus by reference (e.g., Google Cloud Storage URIs), and what are the practical implications for model providers like Gemini?

2. What specific parameter settings (e.g., prompt_strength, cfg, steps, aspect_ratio) are recommended for Stable Diffusion 3.5 Large within LangChain to balance image quality and prompt adherence, and how do these parameters influence the generated image characteristics?

3. In the example of GPT-4 Vision analyzing a futuristic cityscape image, how does the model’s multimodal reasoning manifest in its detailed responses about objects, mood, and presence of people, illustrating the difference between traditional computer vision and modern multimodal understanding?
====

== Chapter 3: Building Workflows with LangGraph

=== 3.1 LangGraph fundamentals


Purpose and differentiator

* LangGraph (from LangChain) is an orchestration framework for complex
AI workflows. It supports features many others don’t: cycles in graphs
(not only DAGs), streaming, pre-built loops/components for generative AI
(e.g., human moderation), and a low-level API for granular control.
* Recommendation: take the free LangGraph course at
academy.langchain.com for deeper learning.

Basic concepts

* Graphs/workflows: composed of nodes (Python functions) and edges;
START and END are reserved nodes.
* State: a workflow-wide context (think Python dict / TypedDict /
dataclass / Pydantic model). Nodes receive an immutable snapshot of
state and must return a dict of key-value updates. LangGraph merges
these updates into the master state—preventing side effects and making
changes explicit.
* Execution model: runs in discrete iterations called supersteps
(inspired by Pregel), possibly executing nodes in parallel and merging
their state updates. You can control recursion limits and max supersteps
to avoid infinite cycles.

Conditional edges and control flow

* Conditional edges allow branching based on current state. A
conditional function takes state and returns a destination (or a typed
Literal). If no type hint is present, you can pass an explicit mapping
to add_conditional_edges.

Reducers (how state keys are merged)

* Default reducer replaces values.
* Built-in/add reducers: use typing.Annotated with an operator (e.g.,
add) to concatenate lists.
* Custom reducers: you can supply your own function to control how old
and new values merge.
* add_messages reducer and MessagesState exist to handle conversational
message merging (useful for chat/LLM workflows).

Configurability

* Nodes can accept a second argument: RunnableConfig (typed dict). It
includes execution controls (e.g., recursion_limit) and a configurable
dict for user parameters (e.g., switching model_provider/model_name at
runtime).

Controlled/structured generation and output parsing

* LLM outputs are often free text; to feed downstream steps reliably,
use controlled generation and output parsers.
* LangChain provides OutputParsers (Enum, JSON, CSV, Pydantic,
DataFrame, etc.), each implementing BaseGenerationOutputParser.
* Example: EnumOutputParser converts text like "YES" / "NO" into Enum
values. Parsers can be chained with LLMs and embedded inside LangGraph
nodes.

Error handling and robustness

* Failures can occur anywhere (API/network/LLM outputs). Recommended
tactics:
** Catch exceptions, log them, and return safe/default outputs so the
workflow can continue.
** Convert exceptions into text for downstream LLMs to reason about
recovery.
** Use fake chat models provided by LangChain for testing
(GenericFakeChatModel, FakeChatModel, FakeListChatModel).
* Retries:
** Generic Runnable retry via .with_retry (configure exception types,
wait/backoff, attempts).
** Node-level RetryPolicy in LangGraph (retry_on, max_attempts).
** Semantic retry (RetryWithErrorOutputParser): when parsing fails, feed
the original prompt, completion, and parse error back to an LLM that
attempts to fix the output.
* Fallbacks:
** Define alternative runnables/chains to run if primary execution fails
(with_fallbacks).

Takeaways

* LangGraph provides a stateful, flexible orchestration model optimized
for generative AI workflows: explicit immutable state updates, reducers
to manage merged values, conditional edges and cycles, configurable
execution, and built-in mechanisms for parsing, retrying, and falling
back when integrating LLMs.
* For complex agentic or multi-step workflows, refer to later chapters
(5 and 6) and LangChain docs/examples for advanced patterns.

[WARNING]
====
1. What are the reserved built-in nodes used to mark the beginning and end of a LangGraph workflow?
2. How does LangGraph provide state to nodes and how must nodes update the workflow state?
3. What is a "superstep" in LangGraph and which system inspired this execution model?
====