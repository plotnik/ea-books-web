<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Asciidoctor 2.0.20">
<meta name="author" content="Andrei Gheorghiu">
<title>Building Data-Driven Applications with LlamaIndex</title>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700">
<style>
/*! Asciidoctor default stylesheet | MIT License | https://asciidoctor.org */
/* Uncomment the following line when using as a custom stylesheet */
/* @import "https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700"; */
html{font-family:sans-serif;-webkit-text-size-adjust:100%}
a{background:none}
a:focus{outline:thin dotted}
a:active,a:hover{outline:0}
h1{font-size:2em;margin:.67em 0}
b,strong{font-weight:bold}
abbr{font-size:.9em}
abbr[title]{cursor:help;border-bottom:1px dotted #dddddf;text-decoration:none}
dfn{font-style:italic}
hr{height:0}
mark{background:#ff0;color:#000}
code,kbd,pre,samp{font-family:monospace;font-size:1em}
pre{white-space:pre-wrap}
q{quotes:"\201C" "\201D" "\2018" "\2019"}
small{font-size:80%}
sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}
sup{top:-.5em}
sub{bottom:-.25em}
img{border:0}
svg:not(:root){overflow:hidden}
figure{margin:0}
audio,video{display:inline-block}
audio:not([controls]){display:none;height:0}
fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}
legend{border:0;padding:0}
button,input,select,textarea{font-family:inherit;font-size:100%;margin:0}
button,input{line-height:normal}
button,select{text-transform:none}
button,html input[type=button],input[type=reset],input[type=submit]{-webkit-appearance:button;cursor:pointer}
button[disabled],html input[disabled]{cursor:default}
input[type=checkbox],input[type=radio]{padding:0}
button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}
textarea{overflow:auto;vertical-align:top}
table{border-collapse:collapse;border-spacing:0}
*,::before,::after{box-sizing:border-box}
html,body{font-size:100%}
body{background:#fff;color:rgba(0,0,0,.8);padding:0;margin:0;font-family:"Noto Serif","DejaVu Serif",serif;line-height:1;position:relative;cursor:auto;-moz-tab-size:4;-o-tab-size:4;tab-size:4;word-wrap:anywhere;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased}
a:hover{cursor:pointer}
img,object,embed{max-width:100%;height:auto}
object,embed{height:100%}
img{-ms-interpolation-mode:bicubic}
.left{float:left!important}
.right{float:right!important}
.text-left{text-align:left!important}
.text-right{text-align:right!important}
.text-center{text-align:center!important}
.text-justify{text-align:justify!important}
.hide{display:none}
img,object,svg{display:inline-block;vertical-align:middle}
textarea{height:auto;min-height:50px}
select{width:100%}
.subheader,.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{line-height:1.45;color:#7a2518;font-weight:400;margin-top:0;margin-bottom:.25em}
div,dl,dt,dd,ul,ol,li,h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6,pre,form,p,blockquote,th,td{margin:0;padding:0}
a{color:#2156a5;text-decoration:underline;line-height:inherit}
a:hover,a:focus{color:#1d4b8f}
a img{border:0}
p{line-height:1.6;margin-bottom:1.25em;text-rendering:optimizeLegibility}
p aside{font-size:.875em;line-height:1.35;font-style:italic}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{font-family:"Open Sans","DejaVu Sans",sans-serif;font-weight:300;font-style:normal;color:#ba3925;text-rendering:optimizeLegibility;margin-top:1em;margin-bottom:.5em;line-height:1.0125em}
h1 small,h2 small,h3 small,#toctitle small,.sidebarblock>.content>.title small,h4 small,h5 small,h6 small{font-size:60%;color:#e99b8f;line-height:0}
h1{font-size:2.125em}
h2{font-size:1.6875em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.375em}
h4,h5{font-size:1.125em}
h6{font-size:1em}
hr{border:solid #dddddf;border-width:1px 0 0;clear:both;margin:1.25em 0 1.1875em}
em,i{font-style:italic;line-height:inherit}
strong,b{font-weight:bold;line-height:inherit}
small{font-size:60%;line-height:inherit}
code{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;font-weight:400;color:rgba(0,0,0,.9)}
ul,ol,dl{line-height:1.6;margin-bottom:1.25em;list-style-position:outside;font-family:inherit}
ul,ol{margin-left:1.5em}
ul li ul,ul li ol{margin-left:1.25em;margin-bottom:0}
ul.circle{list-style-type:circle}
ul.disc{list-style-type:disc}
ul.square{list-style-type:square}
ul.circle ul:not([class]),ul.disc ul:not([class]),ul.square ul:not([class]){list-style:inherit}
ol li ul,ol li ol{margin-left:1.25em;margin-bottom:0}
dl dt{margin-bottom:.3125em;font-weight:bold}
dl dd{margin-bottom:1.25em}
blockquote{margin:0 0 1.25em;padding:.5625em 1.25em 0 1.1875em;border-left:1px solid #ddd}
blockquote,blockquote p{line-height:1.6;color:rgba(0,0,0,.85)}
@media screen and (min-width:768px){h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2}
h1{font-size:2.75em}
h2{font-size:2.3125em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.6875em}
h4{font-size:1.4375em}}
table{background:#fff;margin-bottom:1.25em;border:1px solid #dedede;word-wrap:normal}
table thead,table tfoot{background:#f7f8f7}
table thead tr th,table thead tr td,table tfoot tr th,table tfoot tr td{padding:.5em .625em .625em;font-size:inherit;color:rgba(0,0,0,.8);text-align:left}
table tr th,table tr td{padding:.5625em .625em;font-size:inherit;color:rgba(0,0,0,.8)}
table tr.even,table tr.alt{background:#f8f8f7}
table thead tr th,table tfoot tr th,table tbody tr td,table tr td,table tfoot tr td{line-height:1.6}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2;word-spacing:-.05em}
h1 strong,h2 strong,h3 strong,#toctitle strong,.sidebarblock>.content>.title strong,h4 strong,h5 strong,h6 strong{font-weight:400}
.center{margin-left:auto;margin-right:auto}
.stretch{width:100%}
.clearfix::before,.clearfix::after,.float-group::before,.float-group::after{content:" ";display:table}
.clearfix::after,.float-group::after{clear:both}
:not(pre).nobreak{word-wrap:normal}
:not(pre).nowrap{white-space:nowrap}
:not(pre).pre-wrap{white-space:pre-wrap}
:not(pre):not([class^=L])>code{font-size:.9375em;font-style:normal!important;letter-spacing:0;padding:.1em .5ex;word-spacing:-.15em;background:#f7f7f8;border-radius:4px;line-height:1.45;text-rendering:optimizeSpeed}
pre{color:rgba(0,0,0,.9);font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;line-height:1.45;text-rendering:optimizeSpeed}
pre code,pre pre{color:inherit;font-size:inherit;line-height:inherit}
pre>code{display:block}
pre.nowrap,pre.nowrap pre{white-space:pre;word-wrap:normal}
em em{font-style:normal}
strong strong{font-weight:400}
.keyseq{color:rgba(51,51,51,.8)}
kbd{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;display:inline-block;color:rgba(0,0,0,.8);font-size:.65em;line-height:1.45;background:#f7f7f7;border:1px solid #ccc;border-radius:3px;box-shadow:0 1px 0 rgba(0,0,0,.2),inset 0 0 0 .1em #fff;margin:0 .15em;padding:.2em .5em;vertical-align:middle;position:relative;top:-.1em;white-space:nowrap}
.keyseq kbd:first-child{margin-left:0}
.keyseq kbd:last-child{margin-right:0}
.menuseq,.menuref{color:#000}
.menuseq b:not(.caret),.menuref{font-weight:inherit}
.menuseq{word-spacing:-.02em}
.menuseq b.caret{font-size:1.25em;line-height:.8}
.menuseq i.caret{font-weight:bold;text-align:center;width:.45em}
b.button::before,b.button::after{position:relative;top:-1px;font-weight:400}
b.button::before{content:"[";padding:0 3px 0 2px}
b.button::after{content:"]";padding:0 2px 0 3px}
p a>code:hover{color:rgba(0,0,0,.9)}
#header,#content,#footnotes,#footer{width:100%;margin:0 auto;max-width:62.5em;*zoom:1;position:relative;padding-left:.9375em;padding-right:.9375em}
#header::before,#header::after,#content::before,#content::after,#footnotes::before,#footnotes::after,#footer::before,#footer::after{content:" ";display:table}
#header::after,#content::after,#footnotes::after,#footer::after{clear:both}
#content{margin-top:1.25em}
#content::before{content:none}
#header>h1:first-child{color:rgba(0,0,0,.85);margin-top:2.25rem;margin-bottom:0}
#header>h1:first-child+#toc{margin-top:8px;border-top:1px solid #dddddf}
#header>h1:only-child,body.toc2 #header>h1:nth-last-child(2){border-bottom:1px solid #dddddf;padding-bottom:8px}
#header .details{border-bottom:1px solid #dddddf;line-height:1.45;padding-top:.25em;padding-bottom:.25em;padding-left:.25em;color:rgba(0,0,0,.6);display:flex;flex-flow:row wrap}
#header .details span:first-child{margin-left:-.125em}
#header .details span.email a{color:rgba(0,0,0,.85)}
#header .details br{display:none}
#header .details br+span::before{content:"\00a0\2013\00a0"}
#header .details br+span.author::before{content:"\00a0\22c5\00a0";color:rgba(0,0,0,.85)}
#header .details br+span#revremark::before{content:"\00a0|\00a0"}
#header #revnumber{text-transform:capitalize}
#header #revnumber::after{content:"\00a0"}
#content>h1:first-child:not([class]){color:rgba(0,0,0,.85);border-bottom:1px solid #dddddf;padding-bottom:8px;margin-top:0;padding-top:1rem;margin-bottom:1.25rem}
#toc{border-bottom:1px solid #e7e7e9;padding-bottom:.5em}
#toc>ul{margin-left:.125em}
#toc ul.sectlevel0>li>a{font-style:italic}
#toc ul.sectlevel0 ul.sectlevel1{margin:.5em 0}
#toc ul{font-family:"Open Sans","DejaVu Sans",sans-serif;list-style-type:none}
#toc li{line-height:1.3334;margin-top:.3334em}
#toc a{text-decoration:none}
#toc a:active{text-decoration:underline}
#toctitle{color:#7a2518;font-size:1.2em}
@media screen and (min-width:768px){#toctitle{font-size:1.375em}
body.toc2{padding-left:15em;padding-right:0}
#toc.toc2{margin-top:0!important;background:#f8f8f7;position:fixed;width:15em;left:0;top:0;border-right:1px solid #e7e7e9;border-top-width:0!important;border-bottom-width:0!important;z-index:1000;padding:1.25em 1em;height:100%;overflow:auto}
#toc.toc2 #toctitle{margin-top:0;margin-bottom:.8rem;font-size:1.2em}
#toc.toc2>ul{font-size:.9em;margin-bottom:0}
#toc.toc2 ul ul{margin-left:0;padding-left:1em}
#toc.toc2 ul.sectlevel0 ul.sectlevel1{padding-left:0;margin-top:.5em;margin-bottom:.5em}
body.toc2.toc-right{padding-left:0;padding-right:15em}
body.toc2.toc-right #toc.toc2{border-right-width:0;border-left:1px solid #e7e7e9;left:auto;right:0}}
@media screen and (min-width:1280px){body.toc2{padding-left:20em;padding-right:0}
#toc.toc2{width:20em}
#toc.toc2 #toctitle{font-size:1.375em}
#toc.toc2>ul{font-size:.95em}
#toc.toc2 ul ul{padding-left:1.25em}
body.toc2.toc-right{padding-left:0;padding-right:20em}}
#content #toc{border:1px solid #e0e0dc;margin-bottom:1.25em;padding:1.25em;background:#f8f8f7;border-radius:4px}
#content #toc>:first-child{margin-top:0}
#content #toc>:last-child{margin-bottom:0}
#footer{max-width:none;background:rgba(0,0,0,.8);padding:1.25em}
#footer-text{color:hsla(0,0%,100%,.8);line-height:1.44}
#content{margin-bottom:.625em}
.sect1{padding-bottom:.625em}
@media screen and (min-width:768px){#content{margin-bottom:1.25em}
.sect1{padding-bottom:1.25em}}
.sect1:last-child{padding-bottom:0}
.sect1+.sect1{border-top:1px solid #e7e7e9}
#content h1>a.anchor,h2>a.anchor,h3>a.anchor,#toctitle>a.anchor,.sidebarblock>.content>.title>a.anchor,h4>a.anchor,h5>a.anchor,h6>a.anchor{position:absolute;z-index:1001;width:1.5ex;margin-left:-1.5ex;display:block;text-decoration:none!important;visibility:hidden;text-align:center;font-weight:400}
#content h1>a.anchor::before,h2>a.anchor::before,h3>a.anchor::before,#toctitle>a.anchor::before,.sidebarblock>.content>.title>a.anchor::before,h4>a.anchor::before,h5>a.anchor::before,h6>a.anchor::before{content:"\00A7";font-size:.85em;display:block;padding-top:.1em}
#content h1:hover>a.anchor,#content h1>a.anchor:hover,h2:hover>a.anchor,h2>a.anchor:hover,h3:hover>a.anchor,#toctitle:hover>a.anchor,.sidebarblock>.content>.title:hover>a.anchor,h3>a.anchor:hover,#toctitle>a.anchor:hover,.sidebarblock>.content>.title>a.anchor:hover,h4:hover>a.anchor,h4>a.anchor:hover,h5:hover>a.anchor,h5>a.anchor:hover,h6:hover>a.anchor,h6>a.anchor:hover{visibility:visible}
#content h1>a.link,h2>a.link,h3>a.link,#toctitle>a.link,.sidebarblock>.content>.title>a.link,h4>a.link,h5>a.link,h6>a.link{color:#ba3925;text-decoration:none}
#content h1>a.link:hover,h2>a.link:hover,h3>a.link:hover,#toctitle>a.link:hover,.sidebarblock>.content>.title>a.link:hover,h4>a.link:hover,h5>a.link:hover,h6>a.link:hover{color:#a53221}
details,.audioblock,.imageblock,.literalblock,.listingblock,.stemblock,.videoblock{margin-bottom:1.25em}
details{margin-left:1.25rem}
details>summary{cursor:pointer;display:block;position:relative;line-height:1.6;margin-bottom:.625rem;outline:none;-webkit-tap-highlight-color:transparent}
details>summary::-webkit-details-marker{display:none}
details>summary::before{content:"";border:solid transparent;border-left:solid;border-width:.3em 0 .3em .5em;position:absolute;top:.5em;left:-1.25rem;transform:translateX(15%)}
details[open]>summary::before{border:solid transparent;border-top:solid;border-width:.5em .3em 0;transform:translateY(15%)}
details>summary::after{content:"";width:1.25rem;height:1em;position:absolute;top:.3em;left:-1.25rem}
.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{text-rendering:optimizeLegibility;text-align:left;font-family:"Noto Serif","DejaVu Serif",serif;font-size:1rem;font-style:italic}
table.tableblock.fit-content>caption.title{white-space:nowrap;width:0}
.paragraph.lead>p,#preamble>.sectionbody>[class=paragraph]:first-of-type p{font-size:1.21875em;line-height:1.6;color:rgba(0,0,0,.85)}
.admonitionblock>table{border-collapse:separate;border:0;background:none;width:100%}
.admonitionblock>table td.icon{text-align:center;width:80px}
.admonitionblock>table td.icon img{max-width:none}
.admonitionblock>table td.icon .title{font-weight:bold;font-family:"Open Sans","DejaVu Sans",sans-serif;text-transform:uppercase}
.admonitionblock>table td.content{padding-left:1.125em;padding-right:1.25em;border-left:1px solid #dddddf;color:rgba(0,0,0,.6);word-wrap:anywhere}
.admonitionblock>table td.content>:last-child>:last-child{margin-bottom:0}
.exampleblock>.content{border:1px solid #e6e6e6;margin-bottom:1.25em;padding:1.25em;background:#fff;border-radius:4px}
.sidebarblock{border:1px solid #dbdbd6;margin-bottom:1.25em;padding:1.25em;background:#f3f3f2;border-radius:4px}
.sidebarblock>.content>.title{color:#7a2518;margin-top:0;text-align:center}
.exampleblock>.content>:first-child,.sidebarblock>.content>:first-child{margin-top:0}
.exampleblock>.content>:last-child,.exampleblock>.content>:last-child>:last-child,.exampleblock>.content .olist>ol>li:last-child>:last-child,.exampleblock>.content .ulist>ul>li:last-child>:last-child,.exampleblock>.content .qlist>ol>li:last-child>:last-child,.sidebarblock>.content>:last-child,.sidebarblock>.content>:last-child>:last-child,.sidebarblock>.content .olist>ol>li:last-child>:last-child,.sidebarblock>.content .ulist>ul>li:last-child>:last-child,.sidebarblock>.content .qlist>ol>li:last-child>:last-child{margin-bottom:0}
.literalblock pre,.listingblock>.content>pre{border-radius:4px;overflow-x:auto;padding:1em;font-size:.8125em}
@media screen and (min-width:768px){.literalblock pre,.listingblock>.content>pre{font-size:.90625em}}
@media screen and (min-width:1280px){.literalblock pre,.listingblock>.content>pre{font-size:1em}}
.literalblock pre,.listingblock>.content>pre:not(.highlight),.listingblock>.content>pre[class=highlight],.listingblock>.content>pre[class^="highlight "]{background:#f7f7f8}
.literalblock.output pre{color:#f7f7f8;background:rgba(0,0,0,.9)}
.listingblock>.content{position:relative}
.listingblock code[data-lang]::before{display:none;content:attr(data-lang);position:absolute;font-size:.75em;top:.425rem;right:.5rem;line-height:1;text-transform:uppercase;color:inherit;opacity:.5}
.listingblock:hover code[data-lang]::before{display:block}
.listingblock.terminal pre .command::before{content:attr(data-prompt);padding-right:.5em;color:inherit;opacity:.5}
.listingblock.terminal pre .command:not([data-prompt])::before{content:"$"}
.listingblock pre.highlightjs{padding:0}
.listingblock pre.highlightjs>code{padding:1em;border-radius:4px}
.listingblock pre.prettyprint{border-width:0}
.prettyprint{background:#f7f7f8}
pre.prettyprint .linenums{line-height:1.45;margin-left:2em}
pre.prettyprint li{background:none;list-style-type:inherit;padding-left:0}
pre.prettyprint li code[data-lang]::before{opacity:1}
pre.prettyprint li:not(:first-child) code[data-lang]::before{display:none}
table.linenotable{border-collapse:separate;border:0;margin-bottom:0;background:none}
table.linenotable td[class]{color:inherit;vertical-align:top;padding:0;line-height:inherit;white-space:normal}
table.linenotable td.code{padding-left:.75em}
table.linenotable td.linenos,pre.pygments .linenos{border-right:1px solid;opacity:.35;padding-right:.5em;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}
pre.pygments span.linenos{display:inline-block;margin-right:.75em}
.quoteblock{margin:0 1em 1.25em 1.5em;display:table}
.quoteblock:not(.excerpt)>.title{margin-left:-1.5em;margin-bottom:.75em}
.quoteblock blockquote,.quoteblock p{color:rgba(0,0,0,.85);font-size:1.15rem;line-height:1.75;word-spacing:.1em;letter-spacing:0;font-style:italic;text-align:justify}
.quoteblock blockquote{margin:0;padding:0;border:0}
.quoteblock blockquote::before{content:"\201c";float:left;font-size:2.75em;font-weight:bold;line-height:.6em;margin-left:-.6em;color:#7a2518;text-shadow:0 1px 2px rgba(0,0,0,.1)}
.quoteblock blockquote>.paragraph:last-child p{margin-bottom:0}
.quoteblock .attribution{margin-top:.75em;margin-right:.5ex;text-align:right}
.verseblock{margin:0 1em 1.25em}
.verseblock pre{font-family:"Open Sans","DejaVu Sans",sans-serif;font-size:1.15rem;color:rgba(0,0,0,.85);font-weight:300;text-rendering:optimizeLegibility}
.verseblock pre strong{font-weight:400}
.verseblock .attribution{margin-top:1.25rem;margin-left:.5ex}
.quoteblock .attribution,.verseblock .attribution{font-size:.9375em;line-height:1.45;font-style:italic}
.quoteblock .attribution br,.verseblock .attribution br{display:none}
.quoteblock .attribution cite,.verseblock .attribution cite{display:block;letter-spacing:-.025em;color:rgba(0,0,0,.6)}
.quoteblock.abstract blockquote::before,.quoteblock.excerpt blockquote::before,.quoteblock .quoteblock blockquote::before{display:none}
.quoteblock.abstract blockquote,.quoteblock.abstract p,.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{line-height:1.6;word-spacing:0}
.quoteblock.abstract{margin:0 1em 1.25em;display:block}
.quoteblock.abstract>.title{margin:0 0 .375em;font-size:1.15em;text-align:center}
.quoteblock.excerpt>blockquote,.quoteblock .quoteblock{padding:0 0 .25em 1em;border-left:.25em solid #dddddf}
.quoteblock.excerpt,.quoteblock .quoteblock{margin-left:0}
.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{color:inherit;font-size:1.0625rem}
.quoteblock.excerpt .attribution,.quoteblock .quoteblock .attribution{color:inherit;font-size:.85rem;text-align:left;margin-right:0}
p.tableblock:last-child{margin-bottom:0}
td.tableblock>.content{margin-bottom:1.25em;word-wrap:anywhere}
td.tableblock>.content>:last-child{margin-bottom:-1.25em}
table.tableblock,th.tableblock,td.tableblock{border:0 solid #dedede}
table.grid-all>*>tr>*{border-width:1px}
table.grid-cols>*>tr>*{border-width:0 1px}
table.grid-rows>*>tr>*{border-width:1px 0}
table.frame-all{border-width:1px}
table.frame-ends{border-width:1px 0}
table.frame-sides{border-width:0 1px}
table.frame-none>colgroup+*>:first-child>*,table.frame-sides>colgroup+*>:first-child>*{border-top-width:0}
table.frame-none>:last-child>:last-child>*,table.frame-sides>:last-child>:last-child>*{border-bottom-width:0}
table.frame-none>*>tr>:first-child,table.frame-ends>*>tr>:first-child{border-left-width:0}
table.frame-none>*>tr>:last-child,table.frame-ends>*>tr>:last-child{border-right-width:0}
table.stripes-all>*>tr,table.stripes-odd>*>tr:nth-of-type(odd),table.stripes-even>*>tr:nth-of-type(even),table.stripes-hover>*>tr:hover{background:#f8f8f7}
th.halign-left,td.halign-left{text-align:left}
th.halign-right,td.halign-right{text-align:right}
th.halign-center,td.halign-center{text-align:center}
th.valign-top,td.valign-top{vertical-align:top}
th.valign-bottom,td.valign-bottom{vertical-align:bottom}
th.valign-middle,td.valign-middle{vertical-align:middle}
table thead th,table tfoot th{font-weight:bold}
tbody tr th{background:#f7f8f7}
tbody tr th,tbody tr th p,tfoot tr th,tfoot tr th p{color:rgba(0,0,0,.8);font-weight:bold}
p.tableblock>code:only-child{background:none;padding:0}
p.tableblock{font-size:1em}
ol{margin-left:1.75em}
ul li ol{margin-left:1.5em}
dl dd{margin-left:1.125em}
dl dd:last-child,dl dd:last-child>:last-child{margin-bottom:0}
li p,ul dd,ol dd,.olist .olist,.ulist .ulist,.ulist .olist,.olist .ulist{margin-bottom:.625em}
ul.checklist,ul.none,ol.none,ul.no-bullet,ol.no-bullet,ol.unnumbered,ul.unstyled,ol.unstyled{list-style-type:none}
ul.no-bullet,ol.no-bullet,ol.unnumbered{margin-left:.625em}
ul.unstyled,ol.unstyled{margin-left:0}
li>p:empty:only-child::before{content:"";display:inline-block}
ul.checklist>li>p:first-child{margin-left:-1em}
ul.checklist>li>p:first-child>.fa-square-o:first-child,ul.checklist>li>p:first-child>.fa-check-square-o:first-child{width:1.25em;font-size:.8em;position:relative;bottom:.125em}
ul.checklist>li>p:first-child>input[type=checkbox]:first-child{margin-right:.25em}
ul.inline{display:flex;flex-flow:row wrap;list-style:none;margin:0 0 .625em -1.25em}
ul.inline>li{margin-left:1.25em}
.unstyled dl dt{font-weight:400;font-style:normal}
ol.arabic{list-style-type:decimal}
ol.decimal{list-style-type:decimal-leading-zero}
ol.loweralpha{list-style-type:lower-alpha}
ol.upperalpha{list-style-type:upper-alpha}
ol.lowerroman{list-style-type:lower-roman}
ol.upperroman{list-style-type:upper-roman}
ol.lowergreek{list-style-type:lower-greek}
.hdlist>table,.colist>table{border:0;background:none}
.hdlist>table>tbody>tr,.colist>table>tbody>tr{background:none}
td.hdlist1,td.hdlist2{vertical-align:top;padding:0 .625em}
td.hdlist1{font-weight:bold;padding-bottom:1.25em}
td.hdlist2{word-wrap:anywhere}
.literalblock+.colist,.listingblock+.colist{margin-top:-.5em}
.colist td:not([class]):first-child{padding:.4em .75em 0;line-height:1;vertical-align:top}
.colist td:not([class]):first-child img{max-width:none}
.colist td:not([class]):last-child{padding:.25em 0}
.thumb,.th{line-height:0;display:inline-block;border:4px solid #fff;box-shadow:0 0 0 1px #ddd}
.imageblock.left{margin:.25em .625em 1.25em 0}
.imageblock.right{margin:.25em 0 1.25em .625em}
.imageblock>.title{margin-bottom:0}
.imageblock.thumb,.imageblock.th{border-width:6px}
.imageblock.thumb>.title,.imageblock.th>.title{padding:0 .125em}
.image.left,.image.right{margin-top:.25em;margin-bottom:.25em;display:inline-block;line-height:0}
.image.left{margin-right:.625em}
.image.right{margin-left:.625em}
a.image{text-decoration:none;display:inline-block}
a.image object{pointer-events:none}
sup.footnote,sup.footnoteref{font-size:.875em;position:static;vertical-align:super}
sup.footnote a,sup.footnoteref a{text-decoration:none}
sup.footnote a:active,sup.footnoteref a:active{text-decoration:underline}
#footnotes{padding-top:.75em;padding-bottom:.75em;margin-bottom:.625em}
#footnotes hr{width:20%;min-width:6.25em;margin:-.25em 0 .75em;border-width:1px 0 0}
#footnotes .footnote{padding:0 .375em 0 .225em;line-height:1.3334;font-size:.875em;margin-left:1.2em;margin-bottom:.2em}
#footnotes .footnote a:first-of-type{font-weight:bold;text-decoration:none;margin-left:-1.05em}
#footnotes .footnote:last-of-type{margin-bottom:0}
#content #footnotes{margin-top:-.625em;margin-bottom:0;padding:.75em 0}
div.unbreakable{page-break-inside:avoid}
.big{font-size:larger}
.small{font-size:smaller}
.underline{text-decoration:underline}
.overline{text-decoration:overline}
.line-through{text-decoration:line-through}
.aqua{color:#00bfbf}
.aqua-background{background:#00fafa}
.black{color:#000}
.black-background{background:#000}
.blue{color:#0000bf}
.blue-background{background:#0000fa}
.fuchsia{color:#bf00bf}
.fuchsia-background{background:#fa00fa}
.gray{color:#606060}
.gray-background{background:#7d7d7d}
.green{color:#006000}
.green-background{background:#007d00}
.lime{color:#00bf00}
.lime-background{background:#00fa00}
.maroon{color:#600000}
.maroon-background{background:#7d0000}
.navy{color:#000060}
.navy-background{background:#00007d}
.olive{color:#606000}
.olive-background{background:#7d7d00}
.purple{color:#600060}
.purple-background{background:#7d007d}
.red{color:#bf0000}
.red-background{background:#fa0000}
.silver{color:#909090}
.silver-background{background:#bcbcbc}
.teal{color:#006060}
.teal-background{background:#007d7d}
.white{color:#bfbfbf}
.white-background{background:#fafafa}
.yellow{color:#bfbf00}
.yellow-background{background:#fafa00}
span.icon>.fa{cursor:default}
a span.icon>.fa{cursor:inherit}
.admonitionblock td.icon [class^="fa icon-"]{font-size:2.5em;text-shadow:1px 1px 2px rgba(0,0,0,.5);cursor:default}
.admonitionblock td.icon .icon-note::before{content:"\f05a";color:#19407c}
.admonitionblock td.icon .icon-tip::before{content:"\f0eb";text-shadow:1px 1px 2px rgba(155,155,0,.8);color:#111}
.admonitionblock td.icon .icon-warning::before{content:"\f071";color:#bf6900}
.admonitionblock td.icon .icon-caution::before{content:"\f06d";color:#bf3400}
.admonitionblock td.icon .icon-important::before{content:"\f06a";color:#bf0000}
.conum[data-value]{display:inline-block;color:#fff!important;background:rgba(0,0,0,.8);border-radius:50%;text-align:center;font-size:.75em;width:1.67em;height:1.67em;line-height:1.67em;font-family:"Open Sans","DejaVu Sans",sans-serif;font-style:normal;font-weight:bold}
.conum[data-value] *{color:#fff!important}
.conum[data-value]+b{display:none}
.conum[data-value]::after{content:attr(data-value)}
pre .conum[data-value]{position:relative;top:-.125em}
b.conum *{color:inherit!important}
.conum:not([data-value]):empty{display:none}
dt,th.tableblock,td.content,div.footnote{text-rendering:optimizeLegibility}
h1,h2,p,td.content,span.alt,summary{letter-spacing:-.01em}
p strong,td.content strong,div.footnote strong{letter-spacing:-.005em}
p,blockquote,dt,td.content,td.hdlist1,span.alt,summary{font-size:1.0625rem}
p{margin-bottom:1.25rem}
.sidebarblock p,.sidebarblock dt,.sidebarblock td.content,p.tableblock{font-size:1em}
.exampleblock>.content{background:#fffef7;border-color:#e0e0dc;box-shadow:0 1px 4px #e0e0dc}
.print-only{display:none!important}
@page{margin:1.25cm .75cm}
@media print{*{box-shadow:none!important;text-shadow:none!important}
html{font-size:80%}
a{color:inherit!important;text-decoration:underline!important}
a.bare,a[href^="#"],a[href^="mailto:"]{text-decoration:none!important}
a[href^="http:"]:not(.bare)::after,a[href^="https:"]:not(.bare)::after{content:"(" attr(href) ")";display:inline-block;font-size:.875em;padding-left:.25em}
abbr[title]{border-bottom:1px dotted}
abbr[title]::after{content:" (" attr(title) ")"}
pre,blockquote,tr,img,object,svg{page-break-inside:avoid}
thead{display:table-header-group}
svg{max-width:100%}
p,blockquote,dt,td.content{font-size:1em;orphans:3;widows:3}
h2,h3,#toctitle,.sidebarblock>.content>.title{page-break-after:avoid}
#header,#content,#footnotes,#footer{max-width:none}
#toc,.sidebarblock,.exampleblock>.content{background:none!important}
#toc{border-bottom:1px solid #dddddf!important;padding-bottom:0!important}
body.book #header{text-align:center}
body.book #header>h1:first-child{border:0!important;margin:2.5em 0 1em}
body.book #header .details{border:0!important;display:block;padding:0!important}
body.book #header .details span:first-child{margin-left:0!important}
body.book #header .details br{display:block}
body.book #header .details br+span::before{content:none!important}
body.book #toc{border:0!important;text-align:left!important;padding:0!important;margin:0!important}
body.book #toc,body.book #preamble,body.book h1.sect0,body.book .sect1>h2{page-break-before:always}
.listingblock code[data-lang]::before{display:block}
#footer{padding:0 .9375em}
.hide-on-print{display:none!important}
.print-only{display:block!important}
.hide-for-print{display:none!important}
.show-for-print{display:inherit!important}}
@media amzn-kf8,print{#header>h1:first-child{margin-top:1.25rem}
.sect1{padding:0!important}
.sect1+.sect1{border:0}
#footer{background:none}
#footer-text{color:rgba(0,0,0,.6);font-size:.9em}}
@media amzn-kf8{#header,#content,#footnotes,#footer{padding:0}}
</style>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
</head>
<body class="article toc2 toc-left">
<div id="header">
<h1>Building Data-Driven Applications with LlamaIndex</h1>
<div class="details">
<span id="author" class="author">Andrei Gheorghiu</span><br>
</div>
<div id="toc" class="toc2">
<div id="toctitle">Table of Contents</div>
<ul class="sectlevel1">
<li><a href="#_part_1_introduction_to_generative_ai_and_llamaindex">Part 1: Introduction to Generative AI and LlamaIndex</a></li>
<li><a href="#_chapter_2_llamaindex_the_hidden_jewel_an_introduction_to_the_llamaindex_ecosystem">Chapter 2: LlamaIndex: The Hidden Jewel - An Introduction to the LlamaIndex Ecosystem</a>
<ul class="sectlevel2">
<li><a href="#_introducing_pits_our_llamaindex_hands_on_project">Introducing PITS – our LlamaIndex hands-on project</a></li>
<li><a href="#_familiarizing_ourselves_with_the_structure_of_the_llamaindex_code_repository">Familiarizing ourselves with the structure of the LlamaIndex code repository</a></li>
</ul>
</li>
<li><a href="#_part_2_starting_your_first_llamaindex_project">Part 2: Starting Your First LlamaIndex Project</a></li>
<li><a href="#_chapter_3_kickstarting_your_journey_with_llamaindex">Chapter 3: Kickstarting your Journey with LlamaIndex</a>
<ul class="sectlevel2">
<li><a href="#_uncovering_the_essential_building_blocks_of_llamaindex_documents_nodes_and_indexes">Uncovering the essential building blocks of LlamaIndex – documents, nodes, and indexes</a>
<ul class="sectlevel3">
<li><a href="#_documents">Documents</a></li>
<li><a href="#_nodes">Nodes</a></li>
<li><a href="#_manually_creating_the_node_objects">Manually creating the Node objects</a></li>
<li><a href="#_automatically_extracting_nodes_from_documents_using_splitters">Automatically extracting Nodes from Documents using splitters</a></li>
<li><a href="#_nodes_dont_like_to_be_alone_they_crave_relationships">Nodes don’t like to be alone – they crave relationships</a></li>
<li><a href="#_why_are_relationships_important">Why are relationships important?</a></li>
<li><a href="#_indexes">Indexes</a></li>
<li><a href="#_are_we_there_yet">Are we there yet?</a></li>
<li><a href="#_how_does_this_actually_work_under_the_hood">How does this actually work under the hood?</a></li>
</ul>
</li>
<li><a href="#_starting_our_pits_project_hands_on_exercise">Starting our PITS project – hands-on exercise</a></li>
</ul>
</li>
<li><a href="#_chapter_4_ingesting_data_into_our_rag_workflow">Chapter 4: Ingesting Data into Our RAG Workflow</a>
<ul class="sectlevel2">
<li><a href="#_ingesting_data_via_llamahub">Ingesting data via LlamaHub</a></li>
<li><a href="#_an_overview_of_llamahub">An overview of LlamaHub</a></li>
<li><a href="#_using_the_llamahub_data_loaders_to_ingest_content">Using the LlamaHub data loaders to ingest content</a>
<ul class="sectlevel3">
<li><a href="#_ingesting_data_from_a_web_page">Ingesting data from a web page</a></li>
<li><a href="#_ingesting_data_from_a_database">Ingesting data from a database</a></li>
<li><a href="#_bulk_ingesting_data_from_sources_with_multiple_file_formats">Bulk-ingesting data from sources with multiple file formats</a></li>
</ul>
</li>
<li><a href="#_parsing_the_documents_into_nodes">Parsing the documents into nodes</a>
<ul class="sectlevel3">
<li><a href="#_understanding_the_simple_text_splitters">Understanding the simple text splitters</a></li>
<li><a href="#_using_more_advanced_node_parsers">Using more advanced node parsers</a></li>
<li><a href="#_practical_ways_of_using_these_node_creation_models">Practical ways of using these node creation models</a></li>
</ul>
</li>
<li><a href="#_working_with_metadata_to_improve_the_context">Working with metadata to improve the context</a>
<ul class="sectlevel3">
<li><a href="#_summaryextractor">SummaryExtractor</a></li>
<li><a href="#_questionsansweredextractor">QuestionsAnsweredExtractor</a></li>
</ul>
</li>
<li><a href="#_estimating_the_potential_cost_of_using_metadata_extractors">Estimating the potential cost of using metadata extractors</a>
<ul class="sectlevel3">
<li><a href="#_estimate_your_maximal_costs_before_running_the_actual_extractors">Estimate your maximal costs before running the actual extractors</a></li>
</ul>
</li>
<li><a href="#_hands_on_ingesting_study_materials_into_our_pits">Hands-on – ingesting study materials into our PITS</a></li>
</ul>
</li>
<li><a href="#_chapter_5_indexing_with_llamaindex">Chapter 5: Indexing with LlamaIndex</a>
<ul class="sectlevel2">
<li><a href="#_indexing_data_a_birds_eye_view">Indexing data – a bird’s-eye view</a>
<ul class="sectlevel3">
<li><a href="#_common_features_of_all_index_types">Common features of all Index types</a></li>
</ul>
</li>
<li><a href="#_understanding_the_vectorstoreindex">Understanding the VectorStoreIndex</a>
<ul class="sectlevel3">
<li><a href="#_a_simple_usage_example_for_the_vectorstoreindex">A simple usage example for the VectorStoreIndex</a></li>
<li><a href="#_understanding_embeddings">Understanding embeddings</a></li>
<li><a href="#_understanding_similarity_search">Understanding similarity search</a></li>
<li><a href="#_ok_but_how_does_llamaindex_generate_these_embeddings">OK, but how does LlamaIndex generate these embeddings?</a></li>
<li><a href="#_how_do_i_decide_which_embedding_model_i_should_use">How do I decide which embedding model I should use?</a></li>
</ul>
</li>
<li><a href="#_persisting_and_reusing_indexes">Persisting and reusing Indexes</a>
<ul class="sectlevel3">
<li><a href="#_understanding_the_storagecontext">Understanding the StorageContext</a></li>
</ul>
</li>
<li><a href="#_exploring_other_index_types_in_llamaindex">Exploring other index types in LlamaIndex</a>
<ul class="sectlevel3">
<li><a href="#_the_summaryindex">The SummaryIndex</a></li>
<li><a href="#_the_documentsummaryindex">The DocumentSummaryIndex</a></li>
<li><a href="#_the_keywordtableindex">The KeywordTableIndex</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div id="content">
<div id="preamble">
<div class="sectionbody">
<div class="exampleblock">
<div class="content">
<div class="dlist">
<dl>
<dt class="hdlist1">GitHub</dt>
<dd>
<p><a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex" class="bare">https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex</a></p>
</dd>
</dl>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_part_1_introduction_to_generative_ai_and_llamaindex">Part 1: Introduction to Generative AI and LlamaIndex</h2>
<div class="sectionbody">

</div>
</div>
<div class="sect1">
<h2 id="_chapter_2_llamaindex_the_hidden_jewel_an_introduction_to_the_llamaindex_ecosystem">Chapter 2: LlamaIndex: The Hidden Jewel - An Introduction to the LlamaIndex Ecosystem</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_introducing_pits_our_llamaindex_hands_on_project">Introducing PITS – our LlamaIndex hands-on project</h3>
<div class="exampleblock">
<div class="content">
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/tree/main/PITS_APP" target="_blank">
PITS_APP</a>
</div>
</div>
<div class="paragraph">
<p>The author introduces PITS, an AI tutor built with LlamaIndex, designed to provide a personalized and interactive learning experience. Users can upload study materials, after which PITS will assess the user&#8217;s knowledge with a quiz and then create customized learning material, including slides and narration, divided into chapters. PITS will adapt to the user&#8217;s knowledge level, answer questions, and remember the conversation context across multiple sessions. LlamaIndex will be used to understand and index the study materials, while GPT-4 will power the teaching interactions.</p>
</div>
</div>
<div class="sect2">
<h3 id="_familiarizing_ourselves_with_the_structure_of_the_llamaindex_code_repository">Familiarizing ourselves with the structure of the LlamaIndex code repository</h3>
<div class="exampleblock">
<div class="content">
<a href="https://github.com/run-llama/llama_index" target="_blank">
https://github.com/run-llama/llama_index</a>
</div>
</div>
<div class="paragraph">
<p>The LlamaIndex framework&#8217;s code, reorganized for modularity and efficiency, is structured as follows:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>llama-index-core:</strong> The foundational package, providing essential framework components.</p>
</li>
<li>
<p><strong>llama-index-integrations:</strong> Add-on packages for customizing the framework with specific LLMs, data loaders, embedding models, and vector store providers.</p>
</li>
<li>
<p><strong>llama-index-packs:</strong> Ready-made templates developed by the community to kickstart applications.</p>
</li>
<li>
<p><strong>llama-index-cli:</strong> Supports the LlamaIndex command-line interface.</p>
</li>
<li>
<p><strong>OTHERS:</strong> Contains fine-tuning abstractions and experimental features.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Each subfolder within <code>llama-index-integrations</code> and <code>llama-index-packs</code> represents an individual package that can be installed via pip. For example, to use <code>llama_index.llms.mistralai</code>, you must first install the <code>llama-index-llms-mistralai</code> package. The book will list necessary packages at the beginning of each chapter.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_part_2_starting_your_first_llamaindex_project">Part 2: Starting Your First LlamaIndex Project</h2>
<div class="sectionbody">

</div>
</div>
<div class="sect1">
<h2 id="_chapter_3_kickstarting_your_journey_with_llamaindex">Chapter 3: Kickstarting your Journey with LlamaIndex</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_uncovering_the_essential_building_blocks_of_llamaindex_documents_nodes_and_indexes">Uncovering the essential building blocks of LlamaIndex – documents, nodes, and indexes</h3>
<div class="sect3">
<h4 id="_documents">Documents</h4>
<div class="paragraph">
<p>LlamaIndex uses <code>Document</code> objects to contain and structure raw data from various sources like PDFs, databases, or APIs. A <code>Document</code> holds the text content, a unique ID, and metadata (additional information) for more specific queries. Documents can be created manually or, more commonly, generated in bulk using data loaders from LlamaHub, which supports various data formats and sources. An example is provided using the <code>WikipediaReader</code> to load data from Wikipedia articles into <code>Document</code> objects. The next step is converting these raw <code>Document</code> objects into a format that LLMs can process, which is where Nodes come in.</p>
</div>
</div>
<div class="sect3">
<h4 id="_nodes">Nodes</h4>
<div class="paragraph">
<p>Nodes are smaller, manageable chunks of content extracted from Documents, addressing prompt size limits by allowing selection of relevant information. They create semantic units of data centered around specific information and allow the creation of relationships between Nodes. In LlamaIndex, the <code>TextNode</code> class is a main focus, with attributes like <code>text</code>, <code>start_char_idx</code>, <code>end_char_idx</code>, <code>text_template</code>, <code>metadata_template</code>, <code>metadata_seperator</code>, and <code>metadata</code>. Nodes inherit Document-level metadata but can also be individually customized.</p>
</div>
</div>
<div class="sect3">
<h4 id="_manually_creating_the_node_objects">Manually creating the Node objects</h4>
<div class="paragraph">
<p>The provided code demonstrates how to manually create <code>TextNode</code> objects from a <code>Document</code> object in LlamaIndex. It involves slicing the document&#8217;s text and assigning it to individual nodes. Each node is automatically assigned a unique ID, but this can be customized. This manual approach offers full control over the node&#8217;s text and metadata.</p>
</div>
</div>
<div class="sect3">
<h4 id="_automatically_extracting_nodes_from_documents_using_splitters">Automatically extracting Nodes from Documents using splitters</h4>
<div class="paragraph">
<p>The <code>TokenTextSplitter</code> in LlamaIndex is a tool for chunking documents into nodes, which is important for RAG workflows. It splits text into chunks of whole sentences with a default overlap to maintain context. The splitter can be customized with parameters like <code>chunk_size</code> and <code>chunk_overlap</code>. The example shows how to use <code>TokenTextSplitter</code> on a <code>Document</code> object, splitting the text into nodes and inheriting metadata from the original document. A warning is triggered if the metadata is too large, leaving less room for the actual content text. The next chapter will cover more text-splitting and node-parsing techniques available in LlamaIndex.</p>
</div>
</div>
<div class="sect3">
<h4 id="_nodes_dont_like_to_be_alone_they_crave_relationships">Nodes don’t like to be alone – they crave relationships</h4>
<div class="paragraph">
<p>This content explains how to manually create relationships between nodes in LlamaIndex, focusing on the "previous" and "next" relationships to maintain order within a document. It highlights that LlamaIndex can automatically create these relationships during node parsing. Additionally, it introduces other relationship types like "SOURCE," "PARENT," and "CHILD," which are useful for tracking the origin of nodes and representing hierarchical structures within the data. The content concludes by posing the question of why these relationships are important, setting the stage for further discussion on their utility.</p>
</div>
</div>
<div class="sect3">
<h4 id="_why_are_relationships_important">Why are relationships important?</h4>
<div class="paragraph">
<p>Creating relationships between Nodes in LlamaIndex enhances querying by providing more context, tracking provenance, enabling navigation, supporting knowledge graph construction, and improving index structure. These relationships augment Nodes with contextual connections, leading to more expressive querying and complex index topologies. After structuring raw data into queryable Nodes, the next step is to organize them into efficient indexes.</p>
</div>
</div>
<div class="sect3">
<h4 id="_indexes">Indexes</h4>
<div class="paragraph">
<p>The passage explains the concept of indexing in LlamaIndex, which is crucial for organizing data for retrieval-augmented generation (RAG). Indexing transforms messy data into structured knowledge that AI can use effectively. LlamaIndex supports various index types, including <code>SummaryIndex</code>, <code>DocumentSummaryIndex</code>, <code>VectorStoreIndex</code>, <code>TreeIndex</code>, <code>KeywordTableIndex</code>, <code>KnowledgeGraphIndex</code>, and <code>ComposableGraph</code>, each with its own strengths and trade-offs. All index types share common features like building the index, inserting new nodes, and querying the index. A <code>SummaryIndex</code> example is provided, illustrating its creation and function as a simple list-based data structure that organizes nodes in order.</p>
</div>
</div>
<div class="sect3">
<h4 id="_are_we_there_yet">Are we there yet?</h4>
<div class="paragraph">
<p>The text discusses how to retrieve answers from an index using retrievers and response synthesizers. It uses a Lionel Messi index as an example, querying "What is Messi&#8217;s hometown?" The summary index retrieves all nodes to synthesize a response with full context.</p>
</div>
</div>
<div class="sect3">
<h4 id="_how_does_this_actually_work_under_the_hood">How does this actually work under the hood?</h4>
<div class="paragraph">
<p>The <code>QueryEngine</code> in LlamaIndex retrieves relevant Nodes from an index using a retriever, which fetches and ranks them. A node postprocessor then transforms, re-ranks, or filters these Nodes. Finally, a response synthesizer formulates an LLM prompt with the query and Node context, generates a response, and post-processes it into a natural language answer. The <code>index.as_query_engine()</code> creates a complete query engine with default components. The overall process involves loading data, parsing it into Nodes, building an index, querying the index, and synthesizing a response. Different index types like <code>SummaryIndex</code>, <code>TreeIndex</code>, and <code>KeywordIndex</code> impact performance and use cases, and the index structure defines the data management logic.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_starting_our_pits_project_hands_on_exercise">Starting our PITS project – hands-on exercise</h3>
<div class="exampleblock">
<div class="content">
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/PITS_APP/global_settings.py" target="_blank">
PITS_APP/global_settings.py</a>
<hr>
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/PITS_APP/session_functions.py" target="_blank">
PITS_APP/session_functions.py</a>
<hr>
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/PITS_APP/logging_functions.py" target="_blank">
PITS_APP/logging_functions.py</a>
</div>
</div>
<div class="paragraph">
<p>The chapter introduces the hands-on development of the PITS project, emphasizing a modular code structure for clarity and ease of understanding. The project is built using Python and integrates with LlamaIndex, with a focus on creating a learning application. The author provides a disclaimer that the current implementation lacks certain features, such as authentication and error handling, which can be improved upon later.</p>
</div>
<div class="paragraph">
<p>A detailed overview of the Python source code files is provided, including their functions:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>app.py</strong>: Main entry point for the Streamlit app.</p>
</li>
<li>
<p><strong>document_uploader.py</strong>: Manages document ingestion and indexing.</p>
</li>
<li>
<p><strong>training_material_builder.py</strong>: Creates learning materials based on user knowledge.</p>
</li>
<li>
<p><strong>training_interface.py</strong>: Displays teaching content and facilitates user interaction.</p>
</li>
<li>
<p><strong>quiz_builder.py</strong>: Generates quizzes based on user knowledge.</p>
</li>
<li>
<p><strong>quiz_interface.py</strong>: Administers quizzes and evaluates user performance.</p>
</li>
<li>
<p><strong>conversation_engine.py</strong>: Manages user interactions and maintains conversational context.</p>
</li>
<li>
<p><strong>storage_manager.py</strong>: Handles file operations for session states and user uploads.</p>
</li>
<li>
<p><strong>session_functions.py</strong>: Manages session state saving, loading, and deletion.</p>
</li>
<li>
<p><strong>logging_functions.py</strong>: Records user interactions and application events.</p>
</li>
<li>
<p><strong>global_settings.py</strong>: Contains application configurations and settings.</p>
</li>
<li>
<p><strong>user_onboarding.py</strong>: Manages user onboarding processes.</p>
</li>
<li>
<p><strong>index_builder.py</strong>: Builds indexes for the application.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The chapter also highlights the importance of the YAML package for session management and provides installation instructions. It delves into the <code>global_settings.py</code>, <code>session_functions.py</code>, and <code>logging_functions.py</code> modules, explaining their roles in managing configurations, session states, and logging user actions, respectively. The author emphasizes the necessity of logging for debugging and monitoring the application. The chapter concludes with a promise of further coding in subsequent chapters.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_chapter_4_ingesting_data_into_our_rag_workflow">Chapter 4: Ingesting Data into Our RAG Workflow</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_ingesting_data_via_llamahub">Ingesting data via LlamaHub</h3>
<div class="paragraph">
<p>This section emphasizes the importance of data ingestion and processing in a RAG workflow, highlighting common challenges and potential solutions.</p>
</div>
<div class="paragraph">
<p><strong>Key Challenges:</strong></p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Data Quality:</strong> The quality of the RAG output depends on the quality of the input data. Cleaning, deduplicating, and removing redundant, ambiguous, biased, incomplete, or outdated information is crucial.</p>
</li>
<li>
<p><strong>Data Dynamics:</strong> Knowledge repositories evolve, requiring a system for regularly updating content to incorporate new information and remove outdated data.</p>
</li>
<li>
<p><strong>Data Variety:</strong> Data comes in various formats, and a RAG system should handle them all. While LlamaIndex offers many data loaders, automated ingestion can be challenging. LlamaParse is introduced as a solution for automated data ingestion and processing.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>The section then transitions to discussing data ingestion using LlamaHub data loaders.</p>
</div>
</div>
<div class="sect2">
<h3 id="_an_overview_of_llamahub">An overview of LlamaHub</h3>
<div class="paragraph">
<p>LlamaHub is a library of integrations, including over 180 data connectors (also known as data readers or data loaders), that allow seamless integration of external data with LlamaIndex. These connectors extract data from various sources like databases, APIs, files, and websites, converting it into LlamaIndex <code>Document</code> objects, saving you from writing custom parsers. LlamaIndex&#8217;s modular architecture means these integrations aren&#8217;t included in the core installation, requiring separate installation of the corresponding package. These readers may also utilize specialized libraries and tools tailored to each data type. The LlamaHub website lists all available readers with documentation and samples. The source code for the readers can be found in the <code>llama-index-integrations/readers</code> subfolder of the Llama-index GitHub repository. Before using a data reader, make sure to install any additional dependencies required by the specific connector.</p>
</div>
</div>
<div class="sect2">
<h3 id="_using_the_llamahub_data_loaders_to_ingest_content">Using the LlamaHub data loaders to ingest content</h3>
<div class="sect3">
<h4 id="_ingesting_data_from_a_web_page">Ingesting data from a web page</h4>
<div class="exampleblock">
<div class="content">
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/ch4/sample_reader_SimpleWebPageReader.py" target="_blank">
ch4/sample_reader_SimpleWebPageReader.py</a>
</div>
</div>
<div class="paragraph">
<p>The <code>SimpleWebPageReader</code> in LlamaIndex extracts text content from web pages. It requires the <code>llama-index-readers-web</code> package to be installed. The reader fetches content from URLs, converts HTML to plain text (if specified and if the <code>html2text</code> package is installed), and attaches metadata using a custom function if provided. The content, URL, and metadata are then encapsulated in a <code>Document</code> object. While effective for simple web pages, it may not be suitable for complex, interactive websites. It simplifies the process of ingesting and structuring basic web content, allowing developers to focus on building RAG applications.</p>
</div>
</div>
<div class="sect3">
<h4 id="_ingesting_data_from_a_database">Ingesting data from a database</h4>
<div class="exampleblock">
<div class="content">
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/ch4/sample_reader_DatabaseReader.py" target="_blank">
ch4/sample_reader_DatabaseReader.py</a>
</div>
</div>
<div class="paragraph">
<p>This text discusses using databases for efficient data management and introduces the <code>DatabaseReader</code> connector in LlamaIndex for querying various database systems. It explains how to install the connector, connect to a database (using a URI, SQLAlchemy Engine, or credentials), execute a SQL query, and convert the results into LlamaIndex Document objects. The text provides an example using an SQLite database and points to the official documentation for a more general example. It also highlights the ease of use of LlamaHub readers, mentioning the wide variety of supported data formats and hinting at more efficient methods for ingesting multiple documents in the next section.</p>
</div>
</div>
<div class="sect3">
<h4 id="_bulk_ingesting_data_from_sources_with_multiple_file_formats">Bulk-ingesting data from sources with multiple file formats</h4>
<div class="exampleblock">
<div class="content">
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/ch4/sample_reader_SimpleDirectoryReader.py" target="_blank">
ch4/sample_reader_SimpleDirectoryReader.py</a>
</div>
</div>
<div class="paragraph">
<p>This document discusses two methods for loading data into LlamaIndex for use in Retrieval-Augmented Generation (RAG) systems.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>SimpleDirectoryReader</strong>: This is a simple and easy-to-use reader that can ingest multiple data formats (PDFs, Word docs, text files, CSVs) from a directory or a list of files. It automatically detects the file type and uses the appropriate reader to extract the content.</p>
</li>
<li>
<p><strong>LlamaParse</strong>: This is a more advanced parsing service that is part of the LlamaCloud enterprise platform. It is designed for complex file formats and uses multi-modal capabilities and LLM intelligence to provide high-quality document parsing. It allows users to provide natural language instructions to guide the parsing process and offers a JSON output mode for structured data. It can be used in combination with <code>SimpleDirectoryReader</code> for bulk ingestion. It supports a wide range of file types and offers a free tier. It is a paid service, so users should review the privacy policy before submitting proprietary data.</p>
</li>
</ol>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_parsing_the_documents_into_nodes">Parsing the documents into nodes</h3>
<div class="sect3">
<h4 id="_understanding_the_simple_text_splitters">Understanding the simple text splitters</h4>
<div class="exampleblock">
<div class="content">
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/ch4/sample_splitter_TokenTextSplitter.py" target="_blank">
ch4/sample_splitter_TokenTextSplitter.py</a>
<hr>
<a href="https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/node_parser/text/token.py" target="_blank">
llama-index-core/llama_index/core/node_parser/text/token.py</a>
<hr>
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/ch4/sample_splitter_CodeSplitter.py" target="_blank">
ch4/sample_splitter_CodeSplitter.py</a>
<hr>
<a href="https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/node_parser/text/code.py" target="_blank">
llama-index-core/llama_index/core/node_parser/text/code.py</a>
</div>
</div>
<div class="paragraph">
<p>This text discusses text splitters in LlamaIndex, which break down documents into smaller pieces at the raw text level. It provides code examples and explanations for three specific text splitters:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>SentenceSplitter:</strong> Splits text while maintaining sentence boundaries, creating nodes containing groups of sentences.</p>
</li>
<li>
<p><strong>TokenTextSplitter:</strong> Splits text at the token level, respecting sentence boundaries. Key parameters include <code>chunk_size</code> (max tokens per chunk), <code>chunk_overlap</code> (token overlap between chunks), <code>separator</code> (primary token boundary), and <code>backup_separators</code> (additional splitting points).</p>
</li>
<li>
<p><strong>CodeSplitter:</strong> Designed for source code, splitting based on programming language using an abstract syntax tree (AST) to keep related statements together. Requires installing <code>tree_sitter</code> and <code>tree_sitter_languages</code>. Key parameters include <code>language</code> (programming language), <code>chunk_lines</code> (lines per chunk), <code>chunk_lines_overlap</code> (line overlap), and <code>max_chars</code> (max characters per chunk).</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_using_more_advanced_node_parsers">Using more advanced node parsers</h4>
<div class="exampleblock">
<div class="content">
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/ch4/sample_parser_SentenceWindowNodeParser.py" target="_blank">
ch4/sample_parser_SentenceWindowNodeParser.py</a>
<hr>
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/ch4/sample_parser_LangchainNodeParser.py" target="_blank">
ch4/sample_parser_LangchainNodeParser.py</a>
<hr>
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/ch4/sample_parser_SimpleFileNodeParser.py" target="_blank">
ch4/sample_parser_SimpleFileNodeParser.py</a>
<hr>
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/ch4/sample_parser_HTMLNodeParser.py" target="_blank">
ch4/sample_parser_HTMLNodeParser.py</a>
<hr>
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/ch4/sample_parser_MarkdownNodeParser.py" target="_blank">
ch4/sample_parser_MarkdownNodeParser.py</a>
<hr>
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/ch4/sample_parser_JSONNodeParser.py" target="_blank">
ch4/sample_parser_JSONNodeParser.py</a>
</div>
</div>
<div class="paragraph">
<p>This text discusses advanced tools in LlamaIndex for chunking text into nodes, focusing on <code>NodeParser</code> and its derived classes. Key aspects include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>NodeParser Basics:</strong> All node parsers inherit from the <code>NodeParser</code> class, which allows customization of <code>include_metadata</code>, <code>Include_prev_next_rel</code>, and <code>Callback_manager</code>.</p>
</li>
<li>
<p><strong>SentenceWindowNodeParser:</strong> Splits text into sentences and includes a window of surrounding sentences in the metadata.</p>
</li>
<li>
<p><strong>LangchainNodeParser:</strong> Integrates Langchain text splitters into LlamaIndex.</p>
</li>
<li>
<p><strong>SimpleFileNodeParser:</strong> Automatically selects a node parser based on the file type.</p>
</li>
<li>
<p><strong>HTMLNodeParser:</strong> Parses HTML files using Beautiful Soup, converting them into nodes based on HTML tags.</p>
</li>
<li>
<p><strong>MarkdownNodeParser:</strong> Processes markdown text, creating nodes for each header and incorporating the header hierarchy into the metadata.</p>
</li>
<li>
<p><strong>JSONNodeParser:</strong> Processes structured data in JSON format.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_practical_ways_of_using_these_node_creation_models">Practical ways of using these node creation models</h4>
<div class="paragraph">
<p>The provided text outlines three main ways to implement node parsers or text splitters in LlamaIndex:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Standalone Usage:</strong> Directly calling <code>get_nodes_from_documents()</code> on a parser instance. This allows for explicit control and inspection of the generated nodes and their metadata.</p>
</li>
<li>
<p><strong>Configuring in <code>Settings</code>:</strong> Setting a custom <code>text_splitter</code> in <code>Settings</code> makes it the default for all subsequent operations that rely on text splitting.</p>
</li>
<li>
<p><strong>Ingestion Pipeline:</strong> Defining the parser as a transformation step within an ingestion pipeline, which is a structured process for data ingestion. This will be explained later in the chapter.</p>
</li>
</ol>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_working_with_metadata_to_improve_the_context">Working with metadata to improve the context</h3>
<div class="exampleblock">
<div class="content">
<a href="https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/extractors/metadata_extractors.py" target="_blank">
llama-index-core/llama_index/core/extractors/metadata_extractors.py</a>
</div>
</div>
<div class="sect3">
<h4 id="_summaryextractor">SummaryExtractor</h4>
<div class="exampleblock">
<div class="content">
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/ch4/sample_extractor_SummaryExtractor.py" target="_blank">
ch4/sample_extractor_SummaryExtractor.py</a>
</div>
</div>
<div class="paragraph">
<p>The <code>SummaryExtractor</code> in LlamaIndex generates concise summaries of nodes and their adjacent nodes ("prev", "self", "next"). This is useful in RAG architectures to improve retrieval by allowing search to consider summaries instead of full document content.  It can be customized by specifying which summaries to generate and defining a custom prompt template. A practical use case is summarizing customer support issues and resolutions to quickly retrieve relevant past cases for new support requests.</p>
</div>
</div>
<div class="sect3">
<h4 id="_questionsansweredextractor">QuestionsAnsweredExtractor</h4>
<div class="paragraph">
<p>The <code>QuestionsAnsweredExtractor</code> in LlamaIndex generates a specified number of questions that a given text node can answer. This helps focus retrieval on nodes directly addressing specific inquiries, making it useful for applications like FAQ systems.</p>
</div>
<div class="paragraph">
<p>Key features include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Customizable Question Count:</strong> You can control how many questions are generated.</p>
</li>
<li>
<p><strong>Prompt Customization:</strong> The prompt used to generate questions can be modified via the <code>prompt_template</code> parameter.</p>
</li>
<li>
<p><strong>Embedding Option:</strong>  The <code>embedding_only</code> parameter allows controlling whether the generated metadata is used solely for embeddings.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_estimating_the_potential_cost_of_using_metadata_extractors">Estimating the potential cost of using metadata extractors</h3>
<div class="sect3">
<h4 id="_estimate_your_maximal_costs_before_running_the_actual_extractors">Estimate your maximal costs before running the actual extractors</h4>
<div class="paragraph">
<p>This section explains how to estimate LLM costs before running extractors on a real LLM using LlamaIndex tools.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>MockLLM:</strong> A stand-in LLM that simulates LLM behavior locally without API calls. It uses a <code>max_tokens</code> parameter to mimic token generation limits for cost prediction. The actual cost will likely be lower than the <code>max_tokens</code> value.</p>
</li>
<li>
<p><strong>CallbackManager and TokenCountingHandler:</strong> <code>CallbackManager</code> is a debugging tool, used here with <code>TokenCountingHandler</code> to count tokens used in LLM operations.</p>
</li>
<li>
<p><strong>Tokenizer:</strong> Converts text into tokens for LLMs. It&#8217;s crucial to use a tokenizer compatible with the specific LLM for accurate cost predictions. LlamaIndex defaults to <code>CL100K</code> (GPT-4 tokenizer) but can be customized.</p>
</li>
<li>
<p><strong>Workflow:</strong> The extractor uses <code>MockLLM</code> locally. <code>TokenCountingHandler</code> intercepts the prompt and response to count tokens.</p>
</li>
<li>
<p><strong>Multiple Extractors:</strong> Use <code>token_counter.reset_counts()</code> to estimate costs for multiple extractors individually in the same run.</p>
</li>
<li>
<p><strong>Key Takeaway:</strong> Metadata extraction costs should be estimated and optimized to avoid high operating costs.</p>
</li>
</ol>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_hands_on_ingesting_study_materials_into_our_pits">Hands-on – ingesting study materials into our PITS</h3>
<div class="exampleblock">
<div class="content">
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/PITS_APP/document_uploader.py" target="_blank">
PITS_APP/document_uploader.py</a>
</div>
</div>
<div class="paragraph">
<p>This text details the creation of a <code>document_uploader.py</code> module designed to ingest and prepare study materials for a tutoring project. Here&#8217;s a summary:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Purpose:</strong> The module handles uploading books, documentation, and articles to provide context for the tutor.</p>
</li>
<li>
<p><strong>Key Function: <code>ingest_documents()</code></strong> This function is the core of the module. It:</p>
</li>
<li>
<p><strong>Loads Documents:</strong> Reads files from a designated <code>STORAGE_PATH</code> (defined in <code>global_settings.py</code>).</p>
</li>
<li>
<p><strong>Logs Uploads:</strong> Records each uploaded file using a logging function.</p>
</li>
<li>
<p><strong>Utilizes Caching:</strong> Checks for a pre-existing cache file (<code>CACHE_FILE</code>) to speed up processing. If found, it uses the cached data; otherwise, it processes the documents from scratch.</p>
</li>
<li>
<p><strong>Ingestion Pipeline:</strong> Employs an <code>IngestionPipeline</code> with three transformations:</p>
</li>
<li>
<p><strong>TokenTextSplitter:</strong>  Splits documents into chunks.</p>
</li>
<li>
<p><strong>SummaryExtractor:</strong> Summarizes each chunk.</p>
</li>
<li>
<p><strong>OpenAIEmbedding:</strong> Generates embeddings (explained in a later chapter).</p>
</li>
<li>
<p><strong>Saves Cache:</strong>  Persists the processed data to the cache file for future use.</p>
</li>
<li>
<p><strong>Returns Nodes:</strong> Returns the processed data as "nodes."</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The module aims to streamline document processing and improve efficiency through caching, preparing the study materials for indexing in the next step of the project.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_chapter_5_indexing_with_llamaindex">Chapter 5: Indexing with LlamaIndex</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_indexing_data_a_birds_eye_view">Indexing data – a bird’s-eye view</h3>
<div class="sect3">
<h4 id="_common_features_of_all_index_types">Common features of all Index types</h4>
<div class="paragraph">
<p>LlamaIndex&#8217;s index types share common features inherited from the <code>BaseIndex</code> class, allowing for customization across all index types. These shared features include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Nodes:</strong> Indexes are built upon nodes, which can be customized and dynamically updated through insertion and deletion. Indexes can be built from pre-existing nodes or from documents, with settings available to customize underlying mechanics.</p>
</li>
<li>
<p><strong>Storage Context:</strong> This defines how and where data is stored, crucial for efficient data management.</p>
</li>
<li>
<p><strong>Progress Display:</strong> The <code>show_progress</code> option uses <code>tqdm</code> to display progress bars for long operations.</p>
</li>
<li>
<p><strong>Retrieval Modes:</strong> Indexes offer pre-defined retrieval modes and customizable Retriever classes for query processing.</p>
</li>
<li>
<p><strong>Asynchronous Operations:</strong> The <code>use_async</code> parameter enables asynchronous processing for performance optimization.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Indexing may involve LLM calls, potentially raising cost and privacy concerns.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_understanding_the_vectorstoreindex">Understanding the VectorStoreIndex</h3>
<div class="sect3">
<h4 id="_a_simple_usage_example_for_the_vectorstoreindex">A simple usage example for the VectorStoreIndex</h4>
<div class="paragraph">
<p>The <code>VectorStoreIndex</code> in LlamaIndex provides a simple way to ingest documents and make them searchable. It automatically handles node parsing (breaking down documents into chunks) using default or customizable parameters like chunk size and overlap.</p>
</div>
<div class="paragraph">
<p>Here&#8217;s a breakdown of the process:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Ingestion:</strong> Documents are loaded using <code>SimpleDirectoryReader</code>.</p>
</li>
<li>
<p><strong>Node Creation:</strong> Documents are split into nodes (chunks of text).</p>
</li>
<li>
<p><strong>Embedding:</strong> These nodes are converted into high-dimensional vectors using a language model.</p>
</li>
<li>
<p><strong>Storage:</strong> The vectors are stored in a vector store.</p>
</li>
<li>
<p><strong>Querying:</strong>  Incoming queries are also embedded, and their similarity to the stored vectors is calculated using cosine similarity.</p>
</li>
<li>
<p><strong>Retrieval:</strong> The most similar vectors (and their corresponding document chunks) are returned as the query result.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p><strong>Key Parameters:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>use_async</code>: Enables asynchronous calls (default: <code>False</code>).</p>
</li>
<li>
<p><code>show_progress</code>: Displays progress bars during index construction (default: <code>False</code>).</p>
</li>
<li>
<p><code>store_nodes_override</code>: Forces storage of Node objects (default: <code>False</code>).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The index utilizes <strong>fixed-size chunking</strong> by default, but performance can be optimized by testing different chunk sizes. The core strength of this index lies in its ability to perform <strong>semantic search</strong> by leveraging vector similarity.</p>
</div>
</div>
<div class="sect3">
<h4 id="_understanding_embeddings">Understanding embeddings</h4>
<div class="paragraph">
<p>Vector embeddings are a way to translate data (text, images, sounds, etc.) into a numerical format that Large Language Models (LLMs) can understand. Think of them as converting information into a "standard language" for the LLM.</p>
</div>
<div class="paragraph">
<p>Here&#8217;s a breakdown of the key ideas:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Numerical Representation:</strong> Embeddings represent data as lists of numbers (vectors). These numbers capture the <strong>meaning</strong> of the data.</p>
</li>
<li>
<p><strong>Semantic Understanding:</strong>  LLMs use these numbers to understand relationships between concepts – like synonyms or different meanings of the same word (e.g., "bank" as a riverbank vs. a financial institution).</p>
</li>
<li>
<p><strong>Similarity Search:</strong> Embeddings allow LLMs to find data that is <strong>similar</strong> in meaning. This is done by calculating the "distance" between vectors.  A process called "top-k similarity search" finds the <strong>k</strong> most similar pieces of data.</p>
</li>
<li>
<p><strong>Context is Key:</strong> The size of the text chunks used to create embeddings matters. Too small, and context is lost; too large, and meaning can be diluted.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Essentially, vector embeddings allow LLMs to "see" and "think" about data in a structured way, enabling them to process information and generate relevant responses. They are fundamental to how LLMs work with and understand the world around them.</p>
</div>
</div>
<div class="sect3">
<h4 id="_understanding_similarity_search">Understanding similarity search</h4>
<div class="paragraph">
<p>This text discusses the importance of <strong>similarity search</strong> in machine learning, particularly with the rise of <strong>embeddings</strong> which capture semantic meaning in vector form. Identifying similar vectors allows machines to understand relationships in data and is crucial for applications like recommendation systems and information retrieval.</p>
</div>
<div class="paragraph">
<p>The document focuses on three methods LlamaIndex uses to measure vector similarity:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Cosine Similarity:</strong> Measures the angle between two vectors – a smaller angle indicates higher similarity. It&#8217;s less sensitive to vector length and is the default method in LlamaIndex.</p>
</li>
<li>
<p><strong>Dot Product:</strong> Calculates similarity based on the alignment and length of vectors. Higher values indicate greater similarity, but it <strong>is</strong> sensitive to vector length, potentially biasing results towards longer documents.</p>
</li>
<li>
<p><strong>Euclidean Distance:</strong> Measures the actual distance between vector values, useful when vector dimensions represent real-world measurements.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The key difference lies in how each method approaches similarity: cosine similarity and dot product focus on <strong>direction</strong>, while Euclidean distance focuses on <strong>magnitude/distance</strong>. Understanding these differences is important for choosing the right method for a specific Retrieval-Augmented Generation (RAG) scenario.</p>
</div>
</div>
<div class="sect3">
<h4 id="_ok_but_how_does_llamaindex_generate_these_embeddings">OK, but how does LlamaIndex generate these embeddings?</h4>
<div class="paragraph">
<p>LlamaIndex defaults to using OpenAI’s <code>text-embedding-ada-002</code> model for creating text embeddings, which are crucial for tasks like semantic search. However, it offers flexibility to use alternative models due to cost, privacy, or specialization needs.</p>
</div>
<div class="paragraph">
<p><strong>Key takeaways:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Alternatives to OpenAI:</strong> LlamaIndex supports various embedding models beyond OpenAI, including local models and those from other providers.</p>
</li>
<li>
<p><strong>Hugging Face Integration:</strong>  A popular option is using models from <strong>Hugging Face</strong>, a community-driven platform for AI models (particularly in NLP).  The <code>llama-index-embeddings-huggingface</code> package enables this, with <code>BAAI/bge-small-en-v1.5</code> as a well-balanced default local model.</p>
</li>
<li>
<p><strong>Custom Models:</strong> Advanced users can create and integrate their own custom embedding models by extending LlamaIndex’s <code>BaseEmbedding</code> class.</p>
</li>
<li>
<p><strong>Further Integrations:</strong> LlamaIndex also integrates with Langchain, Azure, CohereAI, and other providers, expanding the range of available embedding models.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In essence, LlamaIndex provides a versatile system for handling text embeddings, allowing users to choose the model that best fits their requirements and constraints.</p>
</div>
</div>
<div class="sect3">
<h4 id="_how_do_i_decide_which_embedding_model_i_should_use">How do I decide which embedding model I should use?</h4>
<div class="paragraph">
<p>Choosing the right embedding model is crucial for a successful Retrieval-Augmented Generation (RAG) application, impacting performance, quality, and cost. Key considerations include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Performance:</strong> Both qualitative (semantic understanding, domain specificity) and quantitative (semantic similarity, benchmarks like <strong>MTEB Leaderboard</strong> - <a href="https://huggingface.co/spaces/mteb/leaderboard" class="bare">https://huggingface.co/spaces/mteb/leaderboard</a> are important.</p>
</li>
<li>
<p><strong>Speed &amp; Efficiency:</strong> Latency and throughput matter for real-time applications, as queries need to be embedded quickly. Consider input chunk size limitations.</p>
</li>
<li>
<p><strong>Language Support:</strong> Choose a model that supports the languages your application requires.</p>
</li>
<li>
<p><strong>Resources &amp; Cost:</strong> Balance embedding accuracy with computational costs, storage, and API usage fees.</p>
</li>
<li>
<p><strong>Accessibility:</strong> Consider availability (API vs. local install) and ease of integration.</p>
</li>
<li>
<p><strong>Privacy &amp; Connectivity:</strong> Local models offer privacy and offline functionality.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>LlamaIndex</strong> offers flexibility and supports many embedding models (see <a href="https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html#list-of-supported-embeddings" class="bare">https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html#list-of-supported-embeddings</a>.</p>
</div>
<div class="paragraph">
<p>While <strong>OpenAI’s <code>text-embedding-ada-002</code></strong> is a good default choice, benchmarking different models is recommended to optimize for specific application needs. Resources like <a href="https://blog.getzep.com/text-embedding-latency-a-semi-scientific-look/" class="bare">https://blog.getzep.com/text-embedding-latency-a-semi-scientific-look/</a> can help evaluate model performance.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_persisting_and_reusing_indexes">Persisting and reusing Indexes</h3>
<div class="exampleblock">
<div class="content">
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/ch5/sample_persist.py" target="_blank">
ch5/sample_persist.py</a>
<hr>
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/ch5/sample_persist_reload.py" target="_blank">
ch5/sample_persist_reload.py</a>
</div>
</div>
<div class="paragraph">
<p>This text discusses the importance of storing vector embeddings generated by LlamaIndex to avoid redundant computation and ensure consistent query results. Here&#8217;s a summary:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Why persist embeddings?</strong> Re-embedding documents is computationally expensive and slow. Storing embeddings allows for faster processing, lower costs, and consistent query accuracy.</p>
</li>
<li>
<p><strong>Vector Stores in LlamaIndex:</strong> LlamaIndex uses vector stores for efficient storage and retrieval of these embeddings. It defaults to in-memory storage, but offers persistence via the <code>.persist()</code> method.</p>
</li>
<li>
<p><strong>How to persist and load:</strong></p>
</li>
<li>
<p>Use <code>index.storage_context.persist(persist_dir="index_cache")</code> to save the index data to disk.</p>
</li>
<li>
<p>Use <code>StorageContext.from_defaults()</code> and <code>load_index_from_storage()</code> to reload the index from the saved directory in future sessions, avoiding re-indexing.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In essence, the text explains how to save and reload LlamaIndex indexes to disk for efficiency and consistency.</p>
</div>
<div class="sect3">
<h4 id="_understanding_the_storagecontext">Understanding the StorageContext</h4>
<div class="paragraph">
<p>The <code>StorageContext</code> in LlamaIndex is a central component for managing data storage during indexing and querying. It encompasses four key stores:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Document Store:</strong> Stores documents locally in <code>docstore.json</code>.</p>
</li>
<li>
<p><strong>Index Store:</strong> Stores index structures locally in <code>index_store.json</code>.</p>
</li>
<li>
<p><strong>Vector Stores:</strong> Manages multiple vector stores (locally in <code>vector_store.json</code> by default).</p>
</li>
<li>
<p><strong>Graph Store:</strong> Stores graph data structures in <code>graph_store.json</code>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>LlamaIndex automatically creates these local storage files when using the <code>persist()</code> method, but allows for custom persistence locations.</p>
</div>
<div class="paragraph">
<p>While basic local stores are provided, the <code>StorageContext</code> is designed to be flexible, supporting integrations with more robust solutions like AWS S3, Pinecone, and MongoDB.</p>
</div>
<div class="paragraph">
<p>The example demonstrates customizing vector storage using <strong>ChromaDB</strong>:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Install <code>chromadb</code> via pip.</p>
</li>
<li>
<p>Initialize a Chroma client and create a collection (<code>my_chroma_store</code>).</p>
</li>
<li>
<p>Create a <code>ChromaVectorStore</code> instance linked to the Chroma collection.</p>
</li>
<li>
<p>Integrate the <code>ChromaVectorStore</code> into the <code>StorageContext</code>.</p>
</li>
<li>
<p>Build an index using the customized <code>StorageContext</code>.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>This approach simplifies working with vector databases, abstracting away complexity and allowing developers to focus on application logic.  LlamaIndex offers a scalable solution, ranging from simple in-memory storage to cloud-hosted databases, with easy component swapping.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_exploring_other_index_types_in_llamaindex">Exploring other index types in LlamaIndex</h3>
<div class="sect3">
<h4 id="_the_summaryindex">The SummaryIndex</h4>
<div class="paragraph">
<p>The <code>SummaryIndex</code> is a simple and efficient indexing method in LlamaIndex, differing from the <code>VectorStoreIndex</code> by storing data in a sequential list of nodes <strong>without</strong> using embeddings or a vector store. This makes it faster and less resource-intensive.</p>
</div>
<div class="paragraph">
<p><strong>Key features and use cases:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Simple Structure:</strong> Data is stored as a list of chunks from ingested documents.</p>
</li>
<li>
<p><strong>No LLM or Embeddings:</strong> Operates locally without requiring large language models or embedding models during indexing.</p>
</li>
<li>
<p><strong>Linear Scan:</strong>  Retrieval involves scanning the list sequentially for relevant information.</p>
</li>
<li>
<p><strong>Suitable for:</strong> Documentation search, scenarios with resource constraints, or when complex semantic search isn&#8217;t necessary.</p>
</li>
<li>
<p><strong>Usage:</strong> Easily created using <code>SummaryIndex.from_documents()</code>.</p>
</li>
<li>
<p><strong>Refinement Process:</strong> Uses a "create and refine" approach during queries, building an initial response and then refining it with additional context.</p>
</li>
<li>
<p><strong>Retrievers:</strong> Compatible with different retrievers (<code>SummaryIndexRetriever</code>, <code>SummaryIndexEmbeddingRetriever</code>, <code>SummaryIndexLLMRetriever</code>) for varied search mechanisms.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In essence, the <code>SummaryIndex</code> provides a straightforward way to index and search data when speed and simplicity are prioritized over complex semantic understanding.</p>
</div>
</div>
<div class="sect3">
<h4 id="_the_documentsummaryindex">The DocumentSummaryIndex</h4>
<div class="paragraph">
<p>The <code>DocumentSummaryIndex</code> is a specialized indexing tool within LlamaIndex designed for efficient document retrieval, particularly useful for large datasets where quick access to specific documents is needed.</p>
</div>
<div class="paragraph">
<p><strong>Key Features &amp; Functionality:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Summarization:</strong> It works by summarizing each document and linking these summaries to the document&#8217;s underlying nodes.</p>
</li>
<li>
<p><strong>Efficient Retrieval:</strong>  These summaries act as a quick filter, identifying relevant documents before deeper analysis.</p>
</li>
<li>
<p><strong>Use Case:</strong> Ideal for knowledge management systems within organizations dealing with extensive documentation (reports, policies, manuals, etc.). It avoids issues with embedding-based retrieval on entire datasets with similar text chunks.</p>
</li>
<li>
<p><strong>Customization:</strong> Offers parameters to control:</p>
</li>
<li>
<p><code>response_synthesizer</code>:  How summaries are generated.</p>
</li>
<li>
<p><code>summary_query</code>: The prompt used for summarization.</p>
</li>
<li>
<p><code>show_progress</code>: Display progress bars during indexing.</p>
</li>
<li>
<p><code>embed_summaries</code>:  Embed summaries for similarity-based searches (default is <code>True</code>).</p>
</li>
<li>
<p><strong>Retrieval Methods:</strong> Supports both embedding-based and LLM-based retrievers.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Basic Usage:</strong></p>
</div>
<div class="paragraph">
<p>Creating a <code>DocumentSummaryIndex</code> involves loading documents, summarizing them, and associating the summaries with the document nodes.  The <code>get_document_summary()</code> method allows access to the generated summaries for individual documents.</p>
</div>
<div class="paragraph">
<p>In essence, the <code>DocumentSummaryIndex</code> prioritizes speed and relevance by leveraging document summaries to narrow the search space, making it a valuable tool for specific retrieval scenarios.</p>
</div>
</div>
<div class="sect3">
<h4 id="_the_keywordtableindex">The KeywordTableIndex</h4>
<div class="paragraph">
<p>The <code>KeywordTableIndex</code> in LlamaIndex is an efficient index structure designed for rapid, targeted factual lookup based on keyword matching. It functions similarly to a glossary, creating a keyword-to-node mapping for quick retrieval of relevant information.</p>
</div>
<div class="paragraph">
<p><strong>Key Features:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Keyword-Based:</strong>  Instead of relying on complex embedding spaces, it uses a straightforward keyword table.</p>
</li>
<li>
<p><strong>Efficient Search:</strong> Enables fast retrieval by directly matching keywords in queries to those in the index.</p>
</li>
<li>
<p><strong>Customizable:</strong> Offers parameters like <code>keyword_extract_template</code> (for prompt customization), <code>max_keywords_per_chunk</code> (to manage table size), and <code>use_async</code> (for performance).</p>
</li>
<li>
<p><strong>Keyword Extraction:</strong>  Extracts keywords from documents using an LLM and a defined prompt, linking them to the source text chunks.</p>
</li>
<li>
<p><strong>Retrieval Modes:</strong> Supports simple keyword matching, RAKE, and LLM-based keyword extraction/matching.</p>
</li>
<li>
<p><strong>Alternatives:</strong> Offers <code>SimpleKeywordTableIndex</code> (regex-based) and <code>RAKEKeywordTableIndex</code> (using <code>rake_nltk</code>) as LLM-free options.</p>
</li>
<li>
<p><strong>Create and Refine:</strong> Like <code>SummaryIndex</code>, it uses a create and refine approach for final response synthesis.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The index is particularly useful when precise keyword matching is crucial, and provides a versatile tool for applications requiring keyword precision.  A simple example demonstrates its ease of use, automatically extracting keywords and setting up the retrieval system.</p>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="footer">
<div id="footer-text">
Last updated 2025-03-23 12:00:07 +0200
</div>
</div>
</body>
</html>