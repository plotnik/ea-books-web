<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Asciidoctor 2.0.23">
<meta name="author" content="Andrei Gheorghiu">
<title>Building Data-Driven Applications with LlamaIndex</title>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700">
<style>
/*! Asciidoctor default stylesheet | MIT License | https://asciidoctor.org */
/* Uncomment the following line when using as a custom stylesheet */
/* @import "https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700"; */
html{font-family:sans-serif;-webkit-text-size-adjust:100%}
a{background:none}
a:focus{outline:thin dotted}
a:active,a:hover{outline:0}
h1{font-size:2em;margin:.67em 0}
b,strong{font-weight:bold}
abbr{font-size:.9em}
abbr[title]{cursor:help;border-bottom:1px dotted #dddddf;text-decoration:none}
dfn{font-style:italic}
hr{height:0}
mark{background:#ff0;color:#000}
code,kbd,pre,samp{font-family:monospace;font-size:1em}
pre{white-space:pre-wrap}
q{quotes:"\201C" "\201D" "\2018" "\2019"}
small{font-size:80%}
sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}
sup{top:-.5em}
sub{bottom:-.25em}
img{border:0}
svg:not(:root){overflow:hidden}
figure{margin:0}
audio,video{display:inline-block}
audio:not([controls]){display:none;height:0}
fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}
legend{border:0;padding:0}
button,input,select,textarea{font-family:inherit;font-size:100%;margin:0}
button,input{line-height:normal}
button,select{text-transform:none}
button,html input[type=button],input[type=reset],input[type=submit]{-webkit-appearance:button;cursor:pointer}
button[disabled],html input[disabled]{cursor:default}
input[type=checkbox],input[type=radio]{padding:0}
button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}
textarea{overflow:auto;vertical-align:top}
table{border-collapse:collapse;border-spacing:0}
*,::before,::after{box-sizing:border-box}
html,body{font-size:100%}
body{background:#fff;color:rgba(0,0,0,.8);padding:0;margin:0;font-family:"Noto Serif","DejaVu Serif",serif;line-height:1;position:relative;cursor:auto;-moz-tab-size:4;-o-tab-size:4;tab-size:4;word-wrap:anywhere;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased}
a:hover{cursor:pointer}
img,object,embed{max-width:100%;height:auto}
object,embed{height:100%}
img{-ms-interpolation-mode:bicubic}
.left{float:left!important}
.right{float:right!important}
.text-left{text-align:left!important}
.text-right{text-align:right!important}
.text-center{text-align:center!important}
.text-justify{text-align:justify!important}
.hide{display:none}
img,object,svg{display:inline-block;vertical-align:middle}
textarea{height:auto;min-height:50px}
select{width:100%}
.subheader,.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{line-height:1.45;color:#7a2518;font-weight:400;margin-top:0;margin-bottom:.25em}
div,dl,dt,dd,ul,ol,li,h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6,pre,form,p,blockquote,th,td{margin:0;padding:0}
a{color:#2156a5;text-decoration:underline;line-height:inherit}
a:hover,a:focus{color:#1d4b8f}
a img{border:0}
p{line-height:1.6;margin-bottom:1.25em;text-rendering:optimizeLegibility}
p aside{font-size:.875em;line-height:1.35;font-style:italic}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{font-family:"Open Sans","DejaVu Sans",sans-serif;font-weight:300;font-style:normal;color:#ba3925;text-rendering:optimizeLegibility;margin-top:1em;margin-bottom:.5em;line-height:1.0125em}
h1 small,h2 small,h3 small,#toctitle small,.sidebarblock>.content>.title small,h4 small,h5 small,h6 small{font-size:60%;color:#e99b8f;line-height:0}
h1{font-size:2.125em}
h2{font-size:1.6875em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.375em}
h4,h5{font-size:1.125em}
h6{font-size:1em}
hr{border:solid #dddddf;border-width:1px 0 0;clear:both;margin:1.25em 0 1.1875em}
em,i{font-style:italic;line-height:inherit}
strong,b{font-weight:bold;line-height:inherit}
small{font-size:60%;line-height:inherit}
code{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;font-weight:400;color:rgba(0,0,0,.9)}
ul,ol,dl{line-height:1.6;margin-bottom:1.25em;list-style-position:outside;font-family:inherit}
ul,ol{margin-left:1.5em}
ul li ul,ul li ol{margin-left:1.25em;margin-bottom:0}
ul.circle{list-style-type:circle}
ul.disc{list-style-type:disc}
ul.square{list-style-type:square}
ul.circle ul:not([class]),ul.disc ul:not([class]),ul.square ul:not([class]){list-style:inherit}
ol li ul,ol li ol{margin-left:1.25em;margin-bottom:0}
dl dt{margin-bottom:.3125em;font-weight:bold}
dl dd{margin-bottom:1.25em}
blockquote{margin:0 0 1.25em;padding:.5625em 1.25em 0 1.1875em;border-left:1px solid #ddd}
blockquote,blockquote p{line-height:1.6;color:rgba(0,0,0,.85)}
@media screen and (min-width:768px){h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2}
h1{font-size:2.75em}
h2{font-size:2.3125em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.6875em}
h4{font-size:1.4375em}}
table{background:#fff;margin-bottom:1.25em;border:1px solid #dedede;word-wrap:normal}
table thead,table tfoot{background:#f7f8f7}
table thead tr th,table thead tr td,table tfoot tr th,table tfoot tr td{padding:.5em .625em .625em;font-size:inherit;color:rgba(0,0,0,.8);text-align:left}
table tr th,table tr td{padding:.5625em .625em;font-size:inherit;color:rgba(0,0,0,.8)}
table tr.even,table tr.alt{background:#f8f8f7}
table thead tr th,table tfoot tr th,table tbody tr td,table tr td,table tfoot tr td{line-height:1.6}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2;word-spacing:-.05em}
h1 strong,h2 strong,h3 strong,#toctitle strong,.sidebarblock>.content>.title strong,h4 strong,h5 strong,h6 strong{font-weight:400}
.center{margin-left:auto;margin-right:auto}
.stretch{width:100%}
.clearfix::before,.clearfix::after,.float-group::before,.float-group::after{content:" ";display:table}
.clearfix::after,.float-group::after{clear:both}
:not(pre).nobreak{word-wrap:normal}
:not(pre).nowrap{white-space:nowrap}
:not(pre).pre-wrap{white-space:pre-wrap}
:not(pre):not([class^=L])>code{font-size:.9375em;font-style:normal!important;letter-spacing:0;padding:.1em .5ex;word-spacing:-.15em;background:#f7f7f8;border-radius:4px;line-height:1.45;text-rendering:optimizeSpeed}
pre{color:rgba(0,0,0,.9);font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;line-height:1.45;text-rendering:optimizeSpeed}
pre code,pre pre{color:inherit;font-size:inherit;line-height:inherit}
pre>code{display:block}
pre.nowrap,pre.nowrap pre{white-space:pre;word-wrap:normal}
em em{font-style:normal}
strong strong{font-weight:400}
.keyseq{color:rgba(51,51,51,.8)}
kbd{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;display:inline-block;color:rgba(0,0,0,.8);font-size:.65em;line-height:1.45;background:#f7f7f7;border:1px solid #ccc;border-radius:3px;box-shadow:0 1px 0 rgba(0,0,0,.2),inset 0 0 0 .1em #fff;margin:0 .15em;padding:.2em .5em;vertical-align:middle;position:relative;top:-.1em;white-space:nowrap}
.keyseq kbd:first-child{margin-left:0}
.keyseq kbd:last-child{margin-right:0}
.menuseq,.menuref{color:#000}
.menuseq b:not(.caret),.menuref{font-weight:inherit}
.menuseq{word-spacing:-.02em}
.menuseq b.caret{font-size:1.25em;line-height:.8}
.menuseq i.caret{font-weight:bold;text-align:center;width:.45em}
b.button::before,b.button::after{position:relative;top:-1px;font-weight:400}
b.button::before{content:"[";padding:0 3px 0 2px}
b.button::after{content:"]";padding:0 2px 0 3px}
p a>code:hover{color:rgba(0,0,0,.9)}
#header,#content,#footnotes,#footer{width:100%;margin:0 auto;max-width:62.5em;*zoom:1;position:relative;padding-left:.9375em;padding-right:.9375em}
#header::before,#header::after,#content::before,#content::after,#footnotes::before,#footnotes::after,#footer::before,#footer::after{content:" ";display:table}
#header::after,#content::after,#footnotes::after,#footer::after{clear:both}
#content{margin-top:1.25em}
#content::before{content:none}
#header>h1:first-child{color:rgba(0,0,0,.85);margin-top:2.25rem;margin-bottom:0}
#header>h1:first-child+#toc{margin-top:8px;border-top:1px solid #dddddf}
#header>h1:only-child{border-bottom:1px solid #dddddf;padding-bottom:8px}
#header .details{border-bottom:1px solid #dddddf;line-height:1.45;padding-top:.25em;padding-bottom:.25em;padding-left:.25em;color:rgba(0,0,0,.6);display:flex;flex-flow:row wrap}
#header .details span:first-child{margin-left:-.125em}
#header .details span.email a{color:rgba(0,0,0,.85)}
#header .details br{display:none}
#header .details br+span::before{content:"\00a0\2013\00a0"}
#header .details br+span.author::before{content:"\00a0\22c5\00a0";color:rgba(0,0,0,.85)}
#header .details br+span#revremark::before{content:"\00a0|\00a0"}
#header #revnumber{text-transform:capitalize}
#header #revnumber::after{content:"\00a0"}
#content>h1:first-child:not([class]){color:rgba(0,0,0,.85);border-bottom:1px solid #dddddf;padding-bottom:8px;margin-top:0;padding-top:1rem;margin-bottom:1.25rem}
#toc{border-bottom:1px solid #e7e7e9;padding-bottom:.5em}
#toc>ul{margin-left:.125em}
#toc ul.sectlevel0>li>a{font-style:italic}
#toc ul.sectlevel0 ul.sectlevel1{margin:.5em 0}
#toc ul{font-family:"Open Sans","DejaVu Sans",sans-serif;list-style-type:none}
#toc li{line-height:1.3334;margin-top:.3334em}
#toc a{text-decoration:none}
#toc a:active{text-decoration:underline}
#toctitle{color:#7a2518;font-size:1.2em}
@media screen and (min-width:768px){#toctitle{font-size:1.375em}
body.toc2{padding-left:15em;padding-right:0}
body.toc2 #header>h1:nth-last-child(2){border-bottom:1px solid #dddddf;padding-bottom:8px}
#toc.toc2{margin-top:0!important;background:#f8f8f7;position:fixed;width:15em;left:0;top:0;border-right:1px solid #e7e7e9;border-top-width:0!important;border-bottom-width:0!important;z-index:1000;padding:1.25em 1em;height:100%;overflow:auto}
#toc.toc2 #toctitle{margin-top:0;margin-bottom:.8rem;font-size:1.2em}
#toc.toc2>ul{font-size:.9em;margin-bottom:0}
#toc.toc2 ul ul{margin-left:0;padding-left:1em}
#toc.toc2 ul.sectlevel0 ul.sectlevel1{padding-left:0;margin-top:.5em;margin-bottom:.5em}
body.toc2.toc-right{padding-left:0;padding-right:15em}
body.toc2.toc-right #toc.toc2{border-right-width:0;border-left:1px solid #e7e7e9;left:auto;right:0}}
@media screen and (min-width:1280px){body.toc2{padding-left:20em;padding-right:0}
#toc.toc2{width:20em}
#toc.toc2 #toctitle{font-size:1.375em}
#toc.toc2>ul{font-size:.95em}
#toc.toc2 ul ul{padding-left:1.25em}
body.toc2.toc-right{padding-left:0;padding-right:20em}}
#content #toc{border:1px solid #e0e0dc;margin-bottom:1.25em;padding:1.25em;background:#f8f8f7;border-radius:4px}
#content #toc>:first-child{margin-top:0}
#content #toc>:last-child{margin-bottom:0}
#footer{max-width:none;background:rgba(0,0,0,.8);padding:1.25em}
#footer-text{color:hsla(0,0%,100%,.8);line-height:1.44}
#content{margin-bottom:.625em}
.sect1{padding-bottom:.625em}
@media screen and (min-width:768px){#content{margin-bottom:1.25em}
.sect1{padding-bottom:1.25em}}
.sect1:last-child{padding-bottom:0}
.sect1+.sect1{border-top:1px solid #e7e7e9}
#content h1>a.anchor,h2>a.anchor,h3>a.anchor,#toctitle>a.anchor,.sidebarblock>.content>.title>a.anchor,h4>a.anchor,h5>a.anchor,h6>a.anchor{position:absolute;z-index:1001;width:1.5ex;margin-left:-1.5ex;display:block;text-decoration:none!important;visibility:hidden;text-align:center;font-weight:400}
#content h1>a.anchor::before,h2>a.anchor::before,h3>a.anchor::before,#toctitle>a.anchor::before,.sidebarblock>.content>.title>a.anchor::before,h4>a.anchor::before,h5>a.anchor::before,h6>a.anchor::before{content:"\00A7";font-size:.85em;display:block;padding-top:.1em}
#content h1:hover>a.anchor,#content h1>a.anchor:hover,h2:hover>a.anchor,h2>a.anchor:hover,h3:hover>a.anchor,#toctitle:hover>a.anchor,.sidebarblock>.content>.title:hover>a.anchor,h3>a.anchor:hover,#toctitle>a.anchor:hover,.sidebarblock>.content>.title>a.anchor:hover,h4:hover>a.anchor,h4>a.anchor:hover,h5:hover>a.anchor,h5>a.anchor:hover,h6:hover>a.anchor,h6>a.anchor:hover{visibility:visible}
#content h1>a.link,h2>a.link,h3>a.link,#toctitle>a.link,.sidebarblock>.content>.title>a.link,h4>a.link,h5>a.link,h6>a.link{color:#ba3925;text-decoration:none}
#content h1>a.link:hover,h2>a.link:hover,h3>a.link:hover,#toctitle>a.link:hover,.sidebarblock>.content>.title>a.link:hover,h4>a.link:hover,h5>a.link:hover,h6>a.link:hover{color:#a53221}
details,.audioblock,.imageblock,.literalblock,.listingblock,.stemblock,.videoblock{margin-bottom:1.25em}
details{margin-left:1.25rem}
details>summary{cursor:pointer;display:block;position:relative;line-height:1.6;margin-bottom:.625rem;outline:none;-webkit-tap-highlight-color:transparent}
details>summary::-webkit-details-marker{display:none}
details>summary::before{content:"";border:solid transparent;border-left:solid;border-width:.3em 0 .3em .5em;position:absolute;top:.5em;left:-1.25rem;transform:translateX(15%)}
details[open]>summary::before{border:solid transparent;border-top:solid;border-width:.5em .3em 0;transform:translateY(15%)}
details>summary::after{content:"";width:1.25rem;height:1em;position:absolute;top:.3em;left:-1.25rem}
.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{text-rendering:optimizeLegibility;text-align:left;font-family:"Noto Serif","DejaVu Serif",serif;font-size:1rem;font-style:italic}
table.tableblock.fit-content>caption.title{white-space:nowrap;width:0}
.paragraph.lead>p,#preamble>.sectionbody>[class=paragraph]:first-of-type p{font-size:1.21875em;line-height:1.6;color:rgba(0,0,0,.85)}
.admonitionblock>table{border-collapse:separate;border:0;background:none;width:100%}
.admonitionblock>table td.icon{text-align:center;width:80px}
.admonitionblock>table td.icon img{max-width:none}
.admonitionblock>table td.icon .title{font-weight:bold;font-family:"Open Sans","DejaVu Sans",sans-serif;text-transform:uppercase}
.admonitionblock>table td.content{padding-left:1.125em;padding-right:1.25em;border-left:1px solid #dddddf;color:rgba(0,0,0,.6);word-wrap:anywhere}
.admonitionblock>table td.content>:last-child>:last-child{margin-bottom:0}
.exampleblock>.content{border:1px solid #e6e6e6;margin-bottom:1.25em;padding:1.25em;background:#fff;border-radius:4px}
.sidebarblock{border:1px solid #dbdbd6;margin-bottom:1.25em;padding:1.25em;background:#f3f3f2;border-radius:4px}
.sidebarblock>.content>.title{color:#7a2518;margin-top:0;text-align:center}
.exampleblock>.content>:first-child,.sidebarblock>.content>:first-child{margin-top:0}
.exampleblock>.content>:last-child,.exampleblock>.content>:last-child>:last-child,.exampleblock>.content .olist>ol>li:last-child>:last-child,.exampleblock>.content .ulist>ul>li:last-child>:last-child,.exampleblock>.content .qlist>ol>li:last-child>:last-child,.sidebarblock>.content>:last-child,.sidebarblock>.content>:last-child>:last-child,.sidebarblock>.content .olist>ol>li:last-child>:last-child,.sidebarblock>.content .ulist>ul>li:last-child>:last-child,.sidebarblock>.content .qlist>ol>li:last-child>:last-child{margin-bottom:0}
.literalblock pre,.listingblock>.content>pre{border-radius:4px;overflow-x:auto;padding:1em;font-size:.8125em}
@media screen and (min-width:768px){.literalblock pre,.listingblock>.content>pre{font-size:.90625em}}
@media screen and (min-width:1280px){.literalblock pre,.listingblock>.content>pre{font-size:1em}}
.literalblock pre,.listingblock>.content>pre:not(.highlight),.listingblock>.content>pre[class=highlight],.listingblock>.content>pre[class^="highlight "]{background:#f7f7f8}
.literalblock.output pre{color:#f7f7f8;background:rgba(0,0,0,.9)}
.listingblock>.content{position:relative}
.listingblock code[data-lang]::before{display:none;content:attr(data-lang);position:absolute;font-size:.75em;top:.425rem;right:.5rem;line-height:1;text-transform:uppercase;color:inherit;opacity:.5}
.listingblock:hover code[data-lang]::before{display:block}
.listingblock.terminal pre .command::before{content:attr(data-prompt);padding-right:.5em;color:inherit;opacity:.5}
.listingblock.terminal pre .command:not([data-prompt])::before{content:"$"}
.listingblock pre.highlightjs{padding:0}
.listingblock pre.highlightjs>code{padding:1em;border-radius:4px}
.listingblock pre.prettyprint{border-width:0}
.prettyprint{background:#f7f7f8}
pre.prettyprint .linenums{line-height:1.45;margin-left:2em}
pre.prettyprint li{background:none;list-style-type:inherit;padding-left:0}
pre.prettyprint li code[data-lang]::before{opacity:1}
pre.prettyprint li:not(:first-child) code[data-lang]::before{display:none}
table.linenotable{border-collapse:separate;border:0;margin-bottom:0;background:none}
table.linenotable td[class]{color:inherit;vertical-align:top;padding:0;line-height:inherit;white-space:normal}
table.linenotable td.code{padding-left:.75em}
table.linenotable td.linenos,pre.pygments .linenos{border-right:1px solid;opacity:.35;padding-right:.5em;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}
pre.pygments span.linenos{display:inline-block;margin-right:.75em}
.quoteblock{margin:0 1em 1.25em 1.5em;display:table}
.quoteblock:not(.excerpt)>.title{margin-left:-1.5em;margin-bottom:.75em}
.quoteblock blockquote,.quoteblock p{color:rgba(0,0,0,.85);font-size:1.15rem;line-height:1.75;word-spacing:.1em;letter-spacing:0;font-style:italic;text-align:justify}
.quoteblock blockquote{margin:0;padding:0;border:0}
.quoteblock blockquote::before{content:"\201c";float:left;font-size:2.75em;font-weight:bold;line-height:.6em;margin-left:-.6em;color:#7a2518;text-shadow:0 1px 2px rgba(0,0,0,.1)}
.quoteblock blockquote>.paragraph:last-child p{margin-bottom:0}
.quoteblock .attribution{margin-top:.75em;margin-right:.5ex;text-align:right}
.verseblock{margin:0 1em 1.25em}
.verseblock pre{font-family:"Open Sans","DejaVu Sans",sans-serif;font-size:1.15rem;color:rgba(0,0,0,.85);font-weight:300;text-rendering:optimizeLegibility}
.verseblock pre strong{font-weight:400}
.verseblock .attribution{margin-top:1.25rem;margin-left:.5ex}
.quoteblock .attribution,.verseblock .attribution{font-size:.9375em;line-height:1.45;font-style:italic}
.quoteblock .attribution br,.verseblock .attribution br{display:none}
.quoteblock .attribution cite,.verseblock .attribution cite{display:block;letter-spacing:-.025em;color:rgba(0,0,0,.6)}
.quoteblock.abstract blockquote::before,.quoteblock.excerpt blockquote::before,.quoteblock .quoteblock blockquote::before{display:none}
.quoteblock.abstract blockquote,.quoteblock.abstract p,.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{line-height:1.6;word-spacing:0}
.quoteblock.abstract{margin:0 1em 1.25em;display:block}
.quoteblock.abstract>.title{margin:0 0 .375em;font-size:1.15em;text-align:center}
.quoteblock.excerpt>blockquote,.quoteblock .quoteblock{padding:0 0 .25em 1em;border-left:.25em solid #dddddf}
.quoteblock.excerpt,.quoteblock .quoteblock{margin-left:0}
.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{color:inherit;font-size:1.0625rem}
.quoteblock.excerpt .attribution,.quoteblock .quoteblock .attribution{color:inherit;font-size:.85rem;text-align:left;margin-right:0}
p.tableblock:last-child{margin-bottom:0}
td.tableblock>.content{margin-bottom:1.25em;word-wrap:anywhere}
td.tableblock>.content>:last-child{margin-bottom:-1.25em}
table.tableblock,th.tableblock,td.tableblock{border:0 solid #dedede}
table.grid-all>*>tr>*{border-width:1px}
table.grid-cols>*>tr>*{border-width:0 1px}
table.grid-rows>*>tr>*{border-width:1px 0}
table.frame-all{border-width:1px}
table.frame-ends{border-width:1px 0}
table.frame-sides{border-width:0 1px}
table.frame-none>colgroup+*>:first-child>*,table.frame-sides>colgroup+*>:first-child>*{border-top-width:0}
table.frame-none>:last-child>:last-child>*,table.frame-sides>:last-child>:last-child>*{border-bottom-width:0}
table.frame-none>*>tr>:first-child,table.frame-ends>*>tr>:first-child{border-left-width:0}
table.frame-none>*>tr>:last-child,table.frame-ends>*>tr>:last-child{border-right-width:0}
table.stripes-all>*>tr,table.stripes-odd>*>tr:nth-of-type(odd),table.stripes-even>*>tr:nth-of-type(even),table.stripes-hover>*>tr:hover{background:#f8f8f7}
th.halign-left,td.halign-left{text-align:left}
th.halign-right,td.halign-right{text-align:right}
th.halign-center,td.halign-center{text-align:center}
th.valign-top,td.valign-top{vertical-align:top}
th.valign-bottom,td.valign-bottom{vertical-align:bottom}
th.valign-middle,td.valign-middle{vertical-align:middle}
table thead th,table tfoot th{font-weight:bold}
tbody tr th{background:#f7f8f7}
tbody tr th,tbody tr th p,tfoot tr th,tfoot tr th p{color:rgba(0,0,0,.8);font-weight:bold}
p.tableblock>code:only-child{background:none;padding:0}
p.tableblock{font-size:1em}
ol{margin-left:1.75em}
ul li ol{margin-left:1.5em}
dl dd{margin-left:1.125em}
dl dd:last-child,dl dd:last-child>:last-child{margin-bottom:0}
li p,ul dd,ol dd,.olist .olist,.ulist .ulist,.ulist .olist,.olist .ulist{margin-bottom:.625em}
ul.checklist,ul.none,ol.none,ul.no-bullet,ol.no-bullet,ol.unnumbered,ul.unstyled,ol.unstyled{list-style-type:none}
ul.no-bullet,ol.no-bullet,ol.unnumbered{margin-left:.625em}
ul.unstyled,ol.unstyled{margin-left:0}
li>p:empty:only-child::before{content:"";display:inline-block}
ul.checklist>li>p:first-child{margin-left:-1em}
ul.checklist>li>p:first-child>.fa-square-o:first-child,ul.checklist>li>p:first-child>.fa-check-square-o:first-child{width:1.25em;font-size:.8em;position:relative;bottom:.125em}
ul.checklist>li>p:first-child>input[type=checkbox]:first-child{margin-right:.25em}
ul.inline{display:flex;flex-flow:row wrap;list-style:none;margin:0 0 .625em -1.25em}
ul.inline>li{margin-left:1.25em}
.unstyled dl dt{font-weight:400;font-style:normal}
ol.arabic{list-style-type:decimal}
ol.decimal{list-style-type:decimal-leading-zero}
ol.loweralpha{list-style-type:lower-alpha}
ol.upperalpha{list-style-type:upper-alpha}
ol.lowerroman{list-style-type:lower-roman}
ol.upperroman{list-style-type:upper-roman}
ol.lowergreek{list-style-type:lower-greek}
.hdlist>table,.colist>table{border:0;background:none}
.hdlist>table>tbody>tr,.colist>table>tbody>tr{background:none}
td.hdlist1,td.hdlist2{vertical-align:top;padding:0 .625em}
td.hdlist1{font-weight:bold;padding-bottom:1.25em}
td.hdlist2{word-wrap:anywhere}
.literalblock+.colist,.listingblock+.colist{margin-top:-.5em}
.colist td:not([class]):first-child{padding:.4em .75em 0;line-height:1;vertical-align:top}
.colist td:not([class]):first-child img{max-width:none}
.colist td:not([class]):last-child{padding:.25em 0}
.thumb,.th{line-height:0;display:inline-block;border:4px solid #fff;box-shadow:0 0 0 1px #ddd}
.imageblock.left{margin:.25em .625em 1.25em 0}
.imageblock.right{margin:.25em 0 1.25em .625em}
.imageblock>.title{margin-bottom:0}
.imageblock.thumb,.imageblock.th{border-width:6px}
.imageblock.thumb>.title,.imageblock.th>.title{padding:0 .125em}
.image.left,.image.right{margin-top:.25em;margin-bottom:.25em;display:inline-block;line-height:0}
.image.left{margin-right:.625em}
.image.right{margin-left:.625em}
a.image{text-decoration:none;display:inline-block}
a.image object{pointer-events:none}
sup.footnote,sup.footnoteref{font-size:.875em;position:static;vertical-align:super}
sup.footnote a,sup.footnoteref a{text-decoration:none}
sup.footnote a:active,sup.footnoteref a:active,#footnotes .footnote a:first-of-type:active{text-decoration:underline}
#footnotes{padding-top:.75em;padding-bottom:.75em;margin-bottom:.625em}
#footnotes hr{width:20%;min-width:6.25em;margin:-.25em 0 .75em;border-width:1px 0 0}
#footnotes .footnote{padding:0 .375em 0 .225em;line-height:1.3334;font-size:.875em;margin-left:1.2em;margin-bottom:.2em}
#footnotes .footnote a:first-of-type{font-weight:bold;text-decoration:none;margin-left:-1.05em}
#footnotes .footnote:last-of-type{margin-bottom:0}
#content #footnotes{margin-top:-.625em;margin-bottom:0;padding:.75em 0}
div.unbreakable{page-break-inside:avoid}
.big{font-size:larger}
.small{font-size:smaller}
.underline{text-decoration:underline}
.overline{text-decoration:overline}
.line-through{text-decoration:line-through}
.aqua{color:#00bfbf}
.aqua-background{background:#00fafa}
.black{color:#000}
.black-background{background:#000}
.blue{color:#0000bf}
.blue-background{background:#0000fa}
.fuchsia{color:#bf00bf}
.fuchsia-background{background:#fa00fa}
.gray{color:#606060}
.gray-background{background:#7d7d7d}
.green{color:#006000}
.green-background{background:#007d00}
.lime{color:#00bf00}
.lime-background{background:#00fa00}
.maroon{color:#600000}
.maroon-background{background:#7d0000}
.navy{color:#000060}
.navy-background{background:#00007d}
.olive{color:#606000}
.olive-background{background:#7d7d00}
.purple{color:#600060}
.purple-background{background:#7d007d}
.red{color:#bf0000}
.red-background{background:#fa0000}
.silver{color:#909090}
.silver-background{background:#bcbcbc}
.teal{color:#006060}
.teal-background{background:#007d7d}
.white{color:#bfbfbf}
.white-background{background:#fafafa}
.yellow{color:#bfbf00}
.yellow-background{background:#fafa00}
span.icon>.fa{cursor:default}
a span.icon>.fa{cursor:inherit}
.admonitionblock td.icon [class^="fa icon-"]{font-size:2.5em;text-shadow:1px 1px 2px rgba(0,0,0,.5);cursor:default}
.admonitionblock td.icon .icon-note::before{content:"\f05a";color:#19407c}
.admonitionblock td.icon .icon-tip::before{content:"\f0eb";text-shadow:1px 1px 2px rgba(155,155,0,.8);color:#111}
.admonitionblock td.icon .icon-warning::before{content:"\f071";color:#bf6900}
.admonitionblock td.icon .icon-caution::before{content:"\f06d";color:#bf3400}
.admonitionblock td.icon .icon-important::before{content:"\f06a";color:#bf0000}
.conum[data-value]{display:inline-block;color:#fff!important;background:rgba(0,0,0,.8);border-radius:50%;text-align:center;font-size:.75em;width:1.67em;height:1.67em;line-height:1.67em;font-family:"Open Sans","DejaVu Sans",sans-serif;font-style:normal;font-weight:bold}
.conum[data-value] *{color:#fff!important}
.conum[data-value]+b{display:none}
.conum[data-value]::after{content:attr(data-value)}
pre .conum[data-value]{position:relative;top:-.125em}
b.conum *{color:inherit!important}
.conum:not([data-value]):empty{display:none}
dt,th.tableblock,td.content,div.footnote{text-rendering:optimizeLegibility}
h1,h2,p,td.content,span.alt,summary{letter-spacing:-.01em}
p strong,td.content strong,div.footnote strong{letter-spacing:-.005em}
p,blockquote,dt,td.content,td.hdlist1,span.alt,summary{font-size:1.0625rem}
p{margin-bottom:1.25rem}
.sidebarblock p,.sidebarblock dt,.sidebarblock td.content,p.tableblock{font-size:1em}
.exampleblock>.content{background:#fffef7;border-color:#e0e0dc;box-shadow:0 1px 4px #e0e0dc}
.print-only{display:none!important}
@page{margin:1.25cm .75cm}
@media print{*{box-shadow:none!important;text-shadow:none!important}
html{font-size:80%}
a{color:inherit!important;text-decoration:underline!important}
a.bare,a[href^="#"],a[href^="mailto:"]{text-decoration:none!important}
a[href^="http:"]:not(.bare)::after,a[href^="https:"]:not(.bare)::after{content:"(" attr(href) ")";display:inline-block;font-size:.875em;padding-left:.25em}
abbr[title]{border-bottom:1px dotted}
abbr[title]::after{content:" (" attr(title) ")"}
pre,blockquote,tr,img,object,svg{page-break-inside:avoid}
thead{display:table-header-group}
svg{max-width:100%}
p,blockquote,dt,td.content{font-size:1em;orphans:3;widows:3}
h2,h3,#toctitle,.sidebarblock>.content>.title{page-break-after:avoid}
#header,#content,#footnotes,#footer{max-width:none}
#toc,.sidebarblock,.exampleblock>.content{background:none!important}
#toc{border-bottom:1px solid #dddddf!important;padding-bottom:0!important}
body.book #header{text-align:center}
body.book #header>h1:first-child{border:0!important;margin:2.5em 0 1em}
body.book #header .details{border:0!important;display:block;padding:0!important}
body.book #header .details span:first-child{margin-left:0!important}
body.book #header .details br{display:block}
body.book #header .details br+span::before{content:none!important}
body.book #toc{border:0!important;text-align:left!important;padding:0!important;margin:0!important}
body.book #toc,body.book #preamble,body.book h1.sect0,body.book .sect1>h2{page-break-before:always}
.listingblock code[data-lang]::before{display:block}
#footer{padding:0 .9375em}
.hide-on-print{display:none!important}
.print-only{display:block!important}
.hide-for-print{display:none!important}
.show-for-print{display:inherit!important}}
@media amzn-kf8,print{#header>h1:first-child{margin-top:1.25rem}
.sect1{padding:0!important}
.sect1+.sect1{border:0}
#footer{background:none}
#footer-text{color:rgba(0,0,0,.6);font-size:.9em}}
@media amzn-kf8{#header,#content,#footnotes,#footer{padding:0}}
</style>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
</head>
<body class="article toc2 toc-left">
<div id="header">
<h1>Building Data-Driven Applications with LlamaIndex</h1>
<div class="details">
<span id="author" class="author">Andrei Gheorghiu</span><br>
</div>
<div id="toc" class="toc2">
<div id="toctitle">Table of Contents</div>
<ul class="sectlevel1">
<li><a href="#_part_1_introduction_to_generative_ai_and_llamaindex">Part 1: Introduction to Generative AI and LlamaIndex</a></li>
<li><a href="#_chapter_2_llamaindex_the_hidden_jewel_an_introduction_to_the_llamaindex_ecosystem">Chapter 2: LlamaIndex: The Hidden Jewel - An Introduction to the LlamaIndex Ecosystem</a>
<ul class="sectlevel2">
<li><a href="#_introducing_pits_our_llamaindex_hands_on_project">Introducing PITS – our LlamaIndex hands-on project</a></li>
<li><a href="#_familiarizing_ourselves_with_the_structure_of_the_llamaindex_code_repository">Familiarizing ourselves with the structure of the LlamaIndex code repository</a></li>
</ul>
</li>
<li><a href="#_part_2_starting_your_first_llamaindex_project">Part 2: Starting Your First LlamaIndex Project</a></li>
<li><a href="#_chapter_3_kickstarting_your_journey_with_llamaindex">Chapter 3: Kickstarting your Journey with LlamaIndex</a>
<ul class="sectlevel2">
<li><a href="#_uncovering_the_essential_building_blocks_of_llamaindex_documents_nodes_and_indexes">Uncovering the essential building blocks of LlamaIndex – documents, nodes, and indexes</a>
<ul class="sectlevel3">
<li><a href="#_documents">Documents</a></li>
<li><a href="#_nodes">Nodes</a></li>
<li><a href="#_manually_creating_the_node_objects">Manually creating the Node objects</a></li>
<li><a href="#_automatically_extracting_nodes_from_documents_using_splitters">Automatically extracting Nodes from Documents using splitters</a></li>
<li><a href="#_nodes_dont_like_to_be_alone_they_crave_relationships">Nodes don’t like to be alone – they crave relationships</a></li>
<li><a href="#_why_are_relationships_important">Why are relationships important?</a></li>
<li><a href="#_indexes">Indexes</a></li>
<li><a href="#_are_we_there_yet">Are we there yet?</a></li>
<li><a href="#_how_does_this_actually_work_under_the_hood">How does this actually work under the hood?</a></li>
</ul>
</li>
<li><a href="#_starting_our_pits_project_hands_on_exercise">Starting our PITS project – hands-on exercise</a></li>
</ul>
</li>
<li><a href="#_chapter_4_ingesting_data_into_our_rag_workflow">Chapter 4: Ingesting Data into Our RAG Workflow</a>
<ul class="sectlevel2">
<li><a href="#_ingesting_data_via_llamahub">Ingesting data via LlamaHub</a></li>
<li><a href="#_an_overview_of_llamahub">An overview of LlamaHub</a></li>
<li><a href="#_using_the_llamahub_data_loaders_to_ingest_content">Using the LlamaHub data loaders to ingest content</a>
<ul class="sectlevel3">
<li><a href="#_ingesting_data_from_a_web_page">Ingesting data from a web page</a></li>
<li><a href="#_ingesting_data_from_a_database">Ingesting data from a database</a></li>
<li><a href="#_bulk_ingesting_data_from_sources_with_multiple_file_formats">Bulk-ingesting data from sources with multiple file formats</a></li>
</ul>
</li>
<li><a href="#_parsing_the_documents_into_nodes">Parsing the documents into nodes</a>
<ul class="sectlevel3">
<li><a href="#_understanding_the_simple_text_splitters">Understanding the simple text splitters</a></li>
<li><a href="#_using_more_advanced_node_parsers">Using more advanced node parsers</a></li>
<li><a href="#_practical_ways_of_using_these_node_creation_models">Practical ways of using these node creation models</a></li>
</ul>
</li>
<li><a href="#_working_with_metadata_to_improve_the_context">Working with metadata to improve the context</a>
<ul class="sectlevel3">
<li><a href="#_summaryextractor">SummaryExtractor</a></li>
<li><a href="#_questionsansweredextractor">QuestionsAnsweredExtractor</a></li>
</ul>
</li>
<li><a href="#_estimating_the_potential_cost_of_using_metadata_extractors">Estimating the potential cost of using metadata extractors</a>
<ul class="sectlevel3">
<li><a href="#_estimate_your_maximal_costs_before_running_the_actual_extractors">Estimate your maximal costs before running the actual extractors</a></li>
</ul>
</li>
<li><a href="#_hands_on_ingesting_study_materials_into_our_pits">Hands-on – ingesting study materials into our PITS</a></li>
</ul>
</li>
<li><a href="#_chapter_5_indexing_with_llamaindex">Chapter 5: Indexing with LlamaIndex</a>
<ul class="sectlevel2">
<li><a href="#_indexing_data_a_birds_eye_view">Indexing data – a bird’s-eye view</a>
<ul class="sectlevel3">
<li><a href="#_common_features_of_all_index_types">Common features of all Index types</a></li>
</ul>
</li>
<li><a href="#_understanding_the_vectorstoreindex">Understanding the VectorStoreIndex</a>
<ul class="sectlevel3">
<li><a href="#_a_simple_usage_example_for_the_vectorstoreindex">A simple usage example for the VectorStoreIndex</a></li>
<li><a href="#_understanding_embeddings">Understanding embeddings</a></li>
<li><a href="#_understanding_similarity_search">Understanding similarity search</a></li>
<li><a href="#_ok_but_how_does_llamaindex_generate_these_embeddings">OK, but how does LlamaIndex generate these embeddings?</a></li>
<li><a href="#_how_do_i_decide_which_embedding_model_i_should_use">How do I decide which embedding model I should use?</a></li>
</ul>
</li>
<li><a href="#_persisting_and_reusing_indexes">Persisting and reusing Indexes</a>
<ul class="sectlevel3">
<li><a href="#_understanding_the_storagecontext">Understanding the StorageContext</a></li>
</ul>
</li>
<li><a href="#_exploring_other_index_types_in_llamaindex">Exploring other index types in LlamaIndex</a>
<ul class="sectlevel3">
<li><a href="#_the_summaryindex">The SummaryIndex</a></li>
<li><a href="#_the_documentsummaryindex">The DocumentSummaryIndex</a></li>
<li><a href="#_the_keywordtableindex">The KeywordTableIndex</a></li>
<li><a href="#_the_treeindex">The TreeIndex</a></li>
<li><a href="#_the_knowledgegraphindex">The KnowledgeGraphIndex</a></li>
</ul>
</li>
<li><a href="#_building_indexes_on_top_of_other_indexes_with_composablegraph">Building Indexes on top of other Indexes with ComposableGraph</a></li>
<li><a href="#_estimating_the_potential_cost_of_building_and_querying_indexes">Estimating the potential cost of building and querying Indexes</a></li>
<li><a href="#_indexing_our_pits_study_materials_hands_on">Indexing our PITS study materials – hands-on</a></li>
</ul>
</li>
<li><a href="#_part_3_retrieving_and_working_with_indexed_data">Part 3: Retrieving and Working with Indexed Data</a></li>
<li><a href="#_chapter_6_querying_our_data_part_1_context_retrieval">Chapter 6: Querying Our Data, Part 1 – Context Retrieval</a>
<ul class="sectlevel2">
<li><a href="#_understanding_the_basic_retrievers">Understanding the basic retrievers</a>
<ul class="sectlevel3">
<li><a href="#_the_vectorstoreindex_retrievers">The VectorStoreIndex retrievers</a></li>
<li><a href="#_the_documentsummaryindex_retrievers">The DocumentSummaryIndex retrievers</a></li>
<li><a href="#_the_treeindex_retrievers">The TreeIndex retrievers</a></li>
<li><a href="#_the_keywordtableindex_retrievers">The KeywordTableIndex retrievers</a></li>
<li><a href="#_the_knowledgegraphindex_retrievers">The KnowledgeGraphIndex retrievers</a></li>
<li><a href="#_efficient_use_of_retrieval_mechanisms_asynchronous_operation">Efficient use of retrieval mechanisms – asynchronous operation</a></li>
</ul>
</li>
<li><a href="#_building_more_advanced_retrieval_mechanisms">Building more advanced retrieval mechanisms</a>
<ul class="sectlevel3">
<li><a href="#_implementing_metadata_filters">Implementing metadata filters</a></li>
<li><a href="#_using_selectors_for_more_advanced_decision_logic">Using selectors for more advanced decision logic</a></li>
<li><a href="#_understanding_tools">Understanding tools</a></li>
<li><a href="#_transforming_and_rewriting_queries">Transforming and rewriting queries</a></li>
<li><a href="#_creating_more_specific_sub_queries">Creating more specific sub-queries</a></li>
</ul>
</li>
<li><a href="#_understanding_the_concepts_of_dense_and_sparse_retrieval">Understanding the concepts of dense and sparse retrieval</a>
<ul class="sectlevel3">
<li><a href="#_implementing_sparse_retrieval_in_llamaindex">Implementing sparse retrieval in LlamaIndex</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#_chapter_7_querying_our_data_part_2_postprocessing_and_response_synthesis">Chapter 7: Querying Our Data, Part 2 – Postprocessing and Response Synthesis</a>
<ul class="sectlevel2">
<li><a href="#_re_ranking_transforming_and_filtering_nodes_using_postprocessors">Re-ranking, transforming, and filtering nodes using postprocessors</a>
<ul class="sectlevel3">
<li><a href="#_similaritypostprocessor">SimilarityPostprocessor</a></li>
<li><a href="#_keywordnodepostprocessor">KeywordNodePostprocessor</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div id="content">
<div id="preamble">
<div class="sectionbody">
<div class="exampleblock">
<div class="content">
<div class="dlist">
<dl>
<dt class="hdlist1">GitHub</dt>
<dd>
<p><a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex" class="bare">https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex</a></p>
</dd>
</dl>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_part_1_introduction_to_generative_ai_and_llamaindex">Part 1: Introduction to Generative AI and LlamaIndex</h2>
<div class="sectionbody">

</div>
</div>
<div class="sect1">
<h2 id="_chapter_2_llamaindex_the_hidden_jewel_an_introduction_to_the_llamaindex_ecosystem">Chapter 2: LlamaIndex: The Hidden Jewel - An Introduction to the LlamaIndex Ecosystem</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_introducing_pits_our_llamaindex_hands_on_project">Introducing PITS – our LlamaIndex hands-on project</h3>
<div class="exampleblock">
<div class="content">
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/tree/main/PITS_APP" target="_blank">
PITS_APP</a>
</div>
</div>
<div class="paragraph">
<p>The author introduces PITS, an AI tutor built with LlamaIndex, designed to provide a personalized and interactive learning experience. Users can upload study materials, after which PITS will assess the user&#8217;s knowledge with a quiz and then create customized learning material, including slides and narration, divided into chapters. PITS will adapt to the user&#8217;s knowledge level, answer questions, and remember the conversation context across multiple sessions. LlamaIndex will be used to understand and index the study materials, while GPT-4 will power the teaching interactions.</p>
</div>
</div>
<div class="sect2">
<h3 id="_familiarizing_ourselves_with_the_structure_of_the_llamaindex_code_repository">Familiarizing ourselves with the structure of the LlamaIndex code repository</h3>
<div class="exampleblock">
<div class="content">
<a href="https://github.com/run-llama/llama_index" target="_blank">
https://github.com/run-llama/llama_index</a>
</div>
</div>
<div class="paragraph">
<p>The LlamaIndex framework&#8217;s code, reorganized for modularity and efficiency, is structured as follows:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>llama-index-core:</strong> The foundational package, providing essential framework components.</p>
</li>
<li>
<p><strong>llama-index-integrations:</strong> Add-on packages for customizing the framework with specific LLMs, data loaders, embedding models, and vector store providers.</p>
</li>
<li>
<p><strong>llama-index-packs:</strong> Ready-made templates developed by the community to kickstart applications.</p>
</li>
<li>
<p><strong>llama-index-cli:</strong> Supports the LlamaIndex command-line interface.</p>
</li>
<li>
<p><strong>OTHERS:</strong> Contains fine-tuning abstractions and experimental features.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Each subfolder within <code>llama-index-integrations</code> and <code>llama-index-packs</code> represents an individual package that can be installed via pip. For example, to use <code>llama_index.llms.mistralai</code>, you must first install the <code>llama-index-llms-mistralai</code> package. The book will list necessary packages at the beginning of each chapter.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_part_2_starting_your_first_llamaindex_project">Part 2: Starting Your First LlamaIndex Project</h2>
<div class="sectionbody">

</div>
</div>
<div class="sect1">
<h2 id="_chapter_3_kickstarting_your_journey_with_llamaindex">Chapter 3: Kickstarting your Journey with LlamaIndex</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_uncovering_the_essential_building_blocks_of_llamaindex_documents_nodes_and_indexes">Uncovering the essential building blocks of LlamaIndex – documents, nodes, and indexes</h3>
<div class="sect3">
<h4 id="_documents">Documents</h4>
<div class="paragraph">
<p>LlamaIndex uses <code>Document</code> objects to contain and structure raw data from various sources like PDFs, databases, or APIs. A <code>Document</code> holds the text content, a unique ID, and metadata (additional information) for more specific queries. Documents can be created manually or, more commonly, generated in bulk using data loaders from LlamaHub, which supports various data formats and sources. An example is provided using the <code>WikipediaReader</code> to load data from Wikipedia articles into <code>Document</code> objects. The next step is converting these raw <code>Document</code> objects into a format that LLMs can process, which is where Nodes come in.</p>
</div>
</div>
<div class="sect3">
<h4 id="_nodes">Nodes</h4>
<div class="paragraph">
<p>Nodes are smaller, manageable chunks of content extracted from Documents, addressing prompt size limits by allowing selection of relevant information. They create semantic units of data centered around specific information and allow the creation of relationships between Nodes. In LlamaIndex, the <code>TextNode</code> class is a main focus, with attributes like <code>text</code>, <code>start_char_idx</code>, <code>end_char_idx</code>, <code>text_template</code>, <code>metadata_template</code>, <code>metadata_seperator</code>, and <code>metadata</code>. Nodes inherit Document-level metadata but can also be individually customized.</p>
</div>
</div>
<div class="sect3">
<h4 id="_manually_creating_the_node_objects">Manually creating the Node objects</h4>
<div class="paragraph">
<p>The provided code demonstrates how to manually create <code>TextNode</code> objects from a <code>Document</code> object in LlamaIndex. It involves slicing the document&#8217;s text and assigning it to individual nodes. Each node is automatically assigned a unique ID, but this can be customized. This manual approach offers full control over the node&#8217;s text and metadata.</p>
</div>
</div>
<div class="sect3">
<h4 id="_automatically_extracting_nodes_from_documents_using_splitters">Automatically extracting Nodes from Documents using splitters</h4>
<div class="paragraph">
<p>The <code>TokenTextSplitter</code> in LlamaIndex is a tool for chunking documents into nodes, which is important for RAG workflows. It splits text into chunks of whole sentences with a default overlap to maintain context. The splitter can be customized with parameters like <code>chunk_size</code> and <code>chunk_overlap</code>. The example shows how to use <code>TokenTextSplitter</code> on a <code>Document</code> object, splitting the text into nodes and inheriting metadata from the original document. A warning is triggered if the metadata is too large, leaving less room for the actual content text. The next chapter will cover more text-splitting and node-parsing techniques available in LlamaIndex.</p>
</div>
</div>
<div class="sect3">
<h4 id="_nodes_dont_like_to_be_alone_they_crave_relationships">Nodes don’t like to be alone – they crave relationships</h4>
<div class="paragraph">
<p>This content explains how to manually create relationships between nodes in LlamaIndex, focusing on the "previous" and "next" relationships to maintain order within a document. It highlights that LlamaIndex can automatically create these relationships during node parsing. Additionally, it introduces other relationship types like "SOURCE," "PARENT," and "CHILD," which are useful for tracking the origin of nodes and representing hierarchical structures within the data. The content concludes by posing the question of why these relationships are important, setting the stage for further discussion on their utility.</p>
</div>
</div>
<div class="sect3">
<h4 id="_why_are_relationships_important">Why are relationships important?</h4>
<div class="paragraph">
<p>Creating relationships between Nodes in LlamaIndex enhances querying by providing more context, tracking provenance, enabling navigation, supporting knowledge graph construction, and improving index structure. These relationships augment Nodes with contextual connections, leading to more expressive querying and complex index topologies. After structuring raw data into queryable Nodes, the next step is to organize them into efficient indexes.</p>
</div>
</div>
<div class="sect3">
<h4 id="_indexes">Indexes</h4>
<div class="paragraph">
<p>The passage explains the concept of indexing in LlamaIndex, which is crucial for organizing data for retrieval-augmented generation (RAG). Indexing transforms messy data into structured knowledge that AI can use effectively. LlamaIndex supports various index types, including <code>SummaryIndex</code>, <code>DocumentSummaryIndex</code>, <code>VectorStoreIndex</code>, <code>TreeIndex</code>, <code>KeywordTableIndex</code>, <code>KnowledgeGraphIndex</code>, and <code>ComposableGraph</code>, each with its own strengths and trade-offs. All index types share common features like building the index, inserting new nodes, and querying the index. A <code>SummaryIndex</code> example is provided, illustrating its creation and function as a simple list-based data structure that organizes nodes in order.</p>
</div>
</div>
<div class="sect3">
<h4 id="_are_we_there_yet">Are we there yet?</h4>
<div class="paragraph">
<p>The text discusses how to retrieve answers from an index using retrievers and response synthesizers. It uses a Lionel Messi index as an example, querying "What is Messi&#8217;s hometown?" The summary index retrieves all nodes to synthesize a response with full context.</p>
</div>
</div>
<div class="sect3">
<h4 id="_how_does_this_actually_work_under_the_hood">How does this actually work under the hood?</h4>
<div class="paragraph">
<p>The <code>QueryEngine</code> in LlamaIndex retrieves relevant Nodes from an index using a retriever, which fetches and ranks them. A node postprocessor then transforms, re-ranks, or filters these Nodes. Finally, a response synthesizer formulates an LLM prompt with the query and Node context, generates a response, and post-processes it into a natural language answer. The <code>index.as_query_engine()</code> creates a complete query engine with default components. The overall process involves loading data, parsing it into Nodes, building an index, querying the index, and synthesizing a response. Different index types like <code>SummaryIndex</code>, <code>TreeIndex</code>, and <code>KeywordIndex</code> impact performance and use cases, and the index structure defines the data management logic.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_starting_our_pits_project_hands_on_exercise">Starting our PITS project – hands-on exercise</h3>
<div class="exampleblock">
<div class="content">
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/PITS_APP/global_settings.py" target="_blank">
PITS_APP/global_settings.py</a>
<hr>
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/PITS_APP/session_functions.py" target="_blank">
PITS_APP/session_functions.py</a>
<hr>
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/PITS_APP/logging_functions.py" target="_blank">
PITS_APP/logging_functions.py</a>
</div>
</div>
<div class="paragraph">
<p>The chapter introduces the hands-on development of the PITS project, emphasizing a modular code structure for clarity and ease of understanding. The project is built using Python and integrates with LlamaIndex, with a focus on creating a learning application. The author provides a disclaimer that the current implementation lacks certain features, such as authentication and error handling, which can be improved upon later.</p>
</div>
<div class="paragraph">
<p>A detailed overview of the Python source code files is provided, including their functions:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>app.py</strong>: Main entry point for the Streamlit app.</p>
</li>
<li>
<p><strong>document_uploader.py</strong>: Manages document ingestion and indexing.</p>
</li>
<li>
<p><strong>training_material_builder.py</strong>: Creates learning materials based on user knowledge.</p>
</li>
<li>
<p><strong>training_interface.py</strong>: Displays teaching content and facilitates user interaction.</p>
</li>
<li>
<p><strong>quiz_builder.py</strong>: Generates quizzes based on user knowledge.</p>
</li>
<li>
<p><strong>quiz_interface.py</strong>: Administers quizzes and evaluates user performance.</p>
</li>
<li>
<p><strong>conversation_engine.py</strong>: Manages user interactions and maintains conversational context.</p>
</li>
<li>
<p><strong>storage_manager.py</strong>: Handles file operations for session states and user uploads.</p>
</li>
<li>
<p><strong>session_functions.py</strong>: Manages session state saving, loading, and deletion.</p>
</li>
<li>
<p><strong>logging_functions.py</strong>: Records user interactions and application events.</p>
</li>
<li>
<p><strong>global_settings.py</strong>: Contains application configurations and settings.</p>
</li>
<li>
<p><strong>user_onboarding.py</strong>: Manages user onboarding processes.</p>
</li>
<li>
<p><strong>index_builder.py</strong>: Builds indexes for the application.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The chapter also highlights the importance of the YAML package for session management and provides installation instructions. It delves into the <code>global_settings.py</code>, <code>session_functions.py</code>, and <code>logging_functions.py</code> modules, explaining their roles in managing configurations, session states, and logging user actions, respectively. The author emphasizes the necessity of logging for debugging and monitoring the application. The chapter concludes with a promise of further coding in subsequent chapters.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_chapter_4_ingesting_data_into_our_rag_workflow">Chapter 4: Ingesting Data into Our RAG Workflow</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_ingesting_data_via_llamahub">Ingesting data via LlamaHub</h3>
<div class="paragraph">
<p>This section emphasizes the importance of data ingestion and processing in a RAG workflow, highlighting common challenges and potential solutions.</p>
</div>
<div class="paragraph">
<p><strong>Key Challenges:</strong></p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Data Quality:</strong> The quality of the RAG output depends on the quality of the input data. Cleaning, deduplicating, and removing redundant, ambiguous, biased, incomplete, or outdated information is crucial.</p>
</li>
<li>
<p><strong>Data Dynamics:</strong> Knowledge repositories evolve, requiring a system for regularly updating content to incorporate new information and remove outdated data.</p>
</li>
<li>
<p><strong>Data Variety:</strong> Data comes in various formats, and a RAG system should handle them all. While LlamaIndex offers many data loaders, automated ingestion can be challenging. LlamaParse is introduced as a solution for automated data ingestion and processing.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>The section then transitions to discussing data ingestion using LlamaHub data loaders.</p>
</div>
</div>
<div class="sect2">
<h3 id="_an_overview_of_llamahub">An overview of LlamaHub</h3>
<div class="paragraph">
<p>LlamaHub is a library of integrations, including over 180 data connectors (also known as data readers or data loaders), that allow seamless integration of external data with LlamaIndex. These connectors extract data from various sources like databases, APIs, files, and websites, converting it into LlamaIndex <code>Document</code> objects, saving you from writing custom parsers. LlamaIndex&#8217;s modular architecture means these integrations aren&#8217;t included in the core installation, requiring separate installation of the corresponding package. These readers may also utilize specialized libraries and tools tailored to each data type. The LlamaHub website lists all available readers with documentation and samples. The source code for the readers can be found in the <code>llama-index-integrations/readers</code> subfolder of the Llama-index GitHub repository. Before using a data reader, make sure to install any additional dependencies required by the specific connector.</p>
</div>
</div>
<div class="sect2">
<h3 id="_using_the_llamahub_data_loaders_to_ingest_content">Using the LlamaHub data loaders to ingest content</h3>
<div class="sect3">
<h4 id="_ingesting_data_from_a_web_page">Ingesting data from a web page</h4>
<div class="exampleblock">
<div class="content">
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/ch4/sample_reader_SimpleWebPageReader.py" target="_blank">
ch4/sample_reader_SimpleWebPageReader.py</a>
</div>
</div>
<div class="paragraph">
<p>The <code>SimpleWebPageReader</code> in LlamaIndex extracts text content from web pages. It requires the <code>llama-index-readers-web</code> package to be installed. The reader fetches content from URLs, converts HTML to plain text (if specified and if the <code>html2text</code> package is installed), and attaches metadata using a custom function if provided. The content, URL, and metadata are then encapsulated in a <code>Document</code> object. While effective for simple web pages, it may not be suitable for complex, interactive websites. It simplifies the process of ingesting and structuring basic web content, allowing developers to focus on building RAG applications.</p>
</div>
</div>
<div class="sect3">
<h4 id="_ingesting_data_from_a_database">Ingesting data from a database</h4>
<div class="exampleblock">
<div class="content">
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/ch4/sample_reader_DatabaseReader.py" target="_blank">
ch4/sample_reader_DatabaseReader.py</a>
</div>
</div>
<div class="paragraph">
<p>This text discusses using databases for efficient data management and introduces the <code>DatabaseReader</code> connector in LlamaIndex for querying various database systems. It explains how to install the connector, connect to a database (using a URI, SQLAlchemy Engine, or credentials), execute a SQL query, and convert the results into LlamaIndex Document objects. The text provides an example using an SQLite database and points to the official documentation for a more general example. It also highlights the ease of use of LlamaHub readers, mentioning the wide variety of supported data formats and hinting at more efficient methods for ingesting multiple documents in the next section.</p>
</div>
</div>
<div class="sect3">
<h4 id="_bulk_ingesting_data_from_sources_with_multiple_file_formats">Bulk-ingesting data from sources with multiple file formats</h4>
<div class="exampleblock">
<div class="content">
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/ch4/sample_reader_SimpleDirectoryReader.py" target="_blank">
ch4/sample_reader_SimpleDirectoryReader.py</a>
</div>
</div>
<div class="paragraph">
<p>This document discusses two methods for loading data into LlamaIndex for use in Retrieval-Augmented Generation (RAG) systems.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>SimpleDirectoryReader</strong>: This is a simple and easy-to-use reader that can ingest multiple data formats (PDFs, Word docs, text files, CSVs) from a directory or a list of files. It automatically detects the file type and uses the appropriate reader to extract the content.</p>
</li>
<li>
<p><strong>LlamaParse</strong>: This is a more advanced parsing service that is part of the LlamaCloud enterprise platform. It is designed for complex file formats and uses multi-modal capabilities and LLM intelligence to provide high-quality document parsing. It allows users to provide natural language instructions to guide the parsing process and offers a JSON output mode for structured data. It can be used in combination with <code>SimpleDirectoryReader</code> for bulk ingestion. It supports a wide range of file types and offers a free tier. It is a paid service, so users should review the privacy policy before submitting proprietary data.</p>
</li>
</ol>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_parsing_the_documents_into_nodes">Parsing the documents into nodes</h3>
<div class="sect3">
<h4 id="_understanding_the_simple_text_splitters">Understanding the simple text splitters</h4>
<div class="exampleblock">
<div class="content">
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/ch4/sample_splitter_TokenTextSplitter.py" target="_blank">
ch4/sample_splitter_TokenTextSplitter.py</a>
<hr>
<a href="https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/node_parser/text/token.py" target="_blank">
llama-index-core/llama_index/core/node_parser/text/token.py</a>
<hr>
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/ch4/sample_splitter_CodeSplitter.py" target="_blank">
ch4/sample_splitter_CodeSplitter.py</a>
<hr>
<a href="https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/node_parser/text/code.py" target="_blank">
llama-index-core/llama_index/core/node_parser/text/code.py</a>
</div>
</div>
<div class="paragraph">
<p>This text discusses text splitters in LlamaIndex, which break down documents into smaller pieces at the raw text level. It provides code examples and explanations for three specific text splitters:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>SentenceSplitter:</strong> Splits text while maintaining sentence boundaries, creating nodes containing groups of sentences.</p>
</li>
<li>
<p><strong>TokenTextSplitter:</strong> Splits text at the token level, respecting sentence boundaries. Key parameters include <code>chunk_size</code> (max tokens per chunk), <code>chunk_overlap</code> (token overlap between chunks), <code>separator</code> (primary token boundary), and <code>backup_separators</code> (additional splitting points).</p>
</li>
<li>
<p><strong>CodeSplitter:</strong> Designed for source code, splitting based on programming language using an abstract syntax tree (AST) to keep related statements together. Requires installing <code>tree_sitter</code> and <code>tree_sitter_languages</code>. Key parameters include <code>language</code> (programming language), <code>chunk_lines</code> (lines per chunk), <code>chunk_lines_overlap</code> (line overlap), and <code>max_chars</code> (max characters per chunk).</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_using_more_advanced_node_parsers">Using more advanced node parsers</h4>
<div class="exampleblock">
<div class="content">
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/ch4/sample_parser_SentenceWindowNodeParser.py" target="_blank">
ch4/sample_parser_SentenceWindowNodeParser.py</a>
<hr>
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/ch4/sample_parser_LangchainNodeParser.py" target="_blank">
ch4/sample_parser_LangchainNodeParser.py</a>
<hr>
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/ch4/sample_parser_SimpleFileNodeParser.py" target="_blank">
ch4/sample_parser_SimpleFileNodeParser.py</a>
<hr>
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/ch4/sample_parser_HTMLNodeParser.py" target="_blank">
ch4/sample_parser_HTMLNodeParser.py</a>
<hr>
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/ch4/sample_parser_MarkdownNodeParser.py" target="_blank">
ch4/sample_parser_MarkdownNodeParser.py</a>
<hr>
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/ch4/sample_parser_JSONNodeParser.py" target="_blank">
ch4/sample_parser_JSONNodeParser.py</a>
</div>
</div>
<div class="paragraph">
<p>This text discusses advanced tools in LlamaIndex for chunking text into nodes, focusing on <code>NodeParser</code> and its derived classes. Key aspects include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>NodeParser Basics:</strong> All node parsers inherit from the <code>NodeParser</code> class, which allows customization of <code>include_metadata</code>, <code>Include_prev_next_rel</code>, and <code>Callback_manager</code>.</p>
</li>
<li>
<p><strong>SentenceWindowNodeParser:</strong> Splits text into sentences and includes a window of surrounding sentences in the metadata.</p>
</li>
<li>
<p><strong>LangchainNodeParser:</strong> Integrates Langchain text splitters into LlamaIndex.</p>
</li>
<li>
<p><strong>SimpleFileNodeParser:</strong> Automatically selects a node parser based on the file type.</p>
</li>
<li>
<p><strong>HTMLNodeParser:</strong> Parses HTML files using Beautiful Soup, converting them into nodes based on HTML tags.</p>
</li>
<li>
<p><strong>MarkdownNodeParser:</strong> Processes markdown text, creating nodes for each header and incorporating the header hierarchy into the metadata.</p>
</li>
<li>
<p><strong>JSONNodeParser:</strong> Processes structured data in JSON format.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_practical_ways_of_using_these_node_creation_models">Practical ways of using these node creation models</h4>
<div class="paragraph">
<p>The provided text outlines three main ways to implement node parsers or text splitters in LlamaIndex:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Standalone Usage:</strong> Directly calling <code>get_nodes_from_documents()</code> on a parser instance. This allows for explicit control and inspection of the generated nodes and their metadata.</p>
</li>
<li>
<p><strong>Configuring in <code>Settings</code>:</strong> Setting a custom <code>text_splitter</code> in <code>Settings</code> makes it the default for all subsequent operations that rely on text splitting.</p>
</li>
<li>
<p><strong>Ingestion Pipeline:</strong> Defining the parser as a transformation step within an ingestion pipeline, which is a structured process for data ingestion. This will be explained later in the chapter.</p>
</li>
</ol>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_working_with_metadata_to_improve_the_context">Working with metadata to improve the context</h3>
<div class="exampleblock">
<div class="content">
<a href="https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/extractors/metadata_extractors.py" target="_blank">
llama-index-core/llama_index/core/extractors/metadata_extractors.py</a>
</div>
</div>
<div class="sect3">
<h4 id="_summaryextractor">SummaryExtractor</h4>
<div class="exampleblock">
<div class="content">
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/ch4/sample_extractor_SummaryExtractor.py" target="_blank">
ch4/sample_extractor_SummaryExtractor.py</a>
</div>
</div>
<div class="paragraph">
<p>The <code>SummaryExtractor</code> in LlamaIndex generates concise summaries of nodes and their adjacent nodes ("prev", "self", "next"). This is useful in RAG architectures to improve retrieval by allowing search to consider summaries instead of full document content.  It can be customized by specifying which summaries to generate and defining a custom prompt template. A practical use case is summarizing customer support issues and resolutions to quickly retrieve relevant past cases for new support requests.</p>
</div>
</div>
<div class="sect3">
<h4 id="_questionsansweredextractor">QuestionsAnsweredExtractor</h4>
<div class="paragraph">
<p>The <code>QuestionsAnsweredExtractor</code> in LlamaIndex generates a specified number of questions that a given text node can answer. This helps focus retrieval on nodes directly addressing specific inquiries, making it useful for applications like FAQ systems.</p>
</div>
<div class="paragraph">
<p>Key features include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Customizable Question Count:</strong> You can control how many questions are generated.</p>
</li>
<li>
<p><strong>Prompt Customization:</strong> The prompt used to generate questions can be modified via the <code>prompt_template</code> parameter.</p>
</li>
<li>
<p><strong>Embedding Option:</strong>  The <code>embedding_only</code> parameter allows controlling whether the generated metadata is used solely for embeddings.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_estimating_the_potential_cost_of_using_metadata_extractors">Estimating the potential cost of using metadata extractors</h3>
<div class="sect3">
<h4 id="_estimate_your_maximal_costs_before_running_the_actual_extractors">Estimate your maximal costs before running the actual extractors</h4>
<div class="paragraph">
<p>This section explains how to estimate LLM costs before running extractors on a real LLM using LlamaIndex tools.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>MockLLM:</strong> A stand-in LLM that simulates LLM behavior locally without API calls. It uses a <code>max_tokens</code> parameter to mimic token generation limits for cost prediction. The actual cost will likely be lower than the <code>max_tokens</code> value.</p>
</li>
<li>
<p><strong>CallbackManager and TokenCountingHandler:</strong> <code>CallbackManager</code> is a debugging tool, used here with <code>TokenCountingHandler</code> to count tokens used in LLM operations.</p>
</li>
<li>
<p><strong>Tokenizer:</strong> Converts text into tokens for LLMs. It&#8217;s crucial to use a tokenizer compatible with the specific LLM for accurate cost predictions. LlamaIndex defaults to <code>CL100K</code> (GPT-4 tokenizer) but can be customized.</p>
</li>
<li>
<p><strong>Workflow:</strong> The extractor uses <code>MockLLM</code> locally. <code>TokenCountingHandler</code> intercepts the prompt and response to count tokens.</p>
</li>
<li>
<p><strong>Multiple Extractors:</strong> Use <code>token_counter.reset_counts()</code> to estimate costs for multiple extractors individually in the same run.</p>
</li>
<li>
<p><strong>Key Takeaway:</strong> Metadata extraction costs should be estimated and optimized to avoid high operating costs.</p>
</li>
</ol>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_hands_on_ingesting_study_materials_into_our_pits">Hands-on – ingesting study materials into our PITS</h3>
<div class="exampleblock">
<div class="content">
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/PITS_APP/document_uploader.py" target="_blank">
PITS_APP/document_uploader.py</a>
</div>
</div>
<div class="paragraph">
<p>This text details the creation of a <code>document_uploader.py</code> module designed to ingest and prepare study materials for a tutoring project. Here&#8217;s a summary:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Purpose:</strong> The module handles uploading books, documentation, and articles to provide context for the tutor.</p>
</li>
<li>
<p><strong>Key Function: <code>ingest_documents()</code></strong> This function is the core of the module. It:</p>
</li>
<li>
<p><strong>Loads Documents:</strong> Reads files from a designated <code>STORAGE_PATH</code> (defined in <code>global_settings.py</code>).</p>
</li>
<li>
<p><strong>Logs Uploads:</strong> Records each uploaded file using a logging function.</p>
</li>
<li>
<p><strong>Utilizes Caching:</strong> Checks for a pre-existing cache file (<code>CACHE_FILE</code>) to speed up processing. If found, it uses the cached data; otherwise, it processes the documents from scratch.</p>
</li>
<li>
<p><strong>Ingestion Pipeline:</strong> Employs an <code>IngestionPipeline</code> with three transformations:</p>
</li>
<li>
<p><strong>TokenTextSplitter:</strong>  Splits documents into chunks.</p>
</li>
<li>
<p><strong>SummaryExtractor:</strong> Summarizes each chunk.</p>
</li>
<li>
<p><strong>OpenAIEmbedding:</strong> Generates embeddings (explained in a later chapter).</p>
</li>
<li>
<p><strong>Saves Cache:</strong>  Persists the processed data to the cache file for future use.</p>
</li>
<li>
<p><strong>Returns Nodes:</strong> Returns the processed data as "nodes."</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The module aims to streamline document processing and improve efficiency through caching, preparing the study materials for indexing in the next step of the project.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_chapter_5_indexing_with_llamaindex">Chapter 5: Indexing with LlamaIndex</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_indexing_data_a_birds_eye_view">Indexing data – a bird’s-eye view</h3>
<div class="sect3">
<h4 id="_common_features_of_all_index_types">Common features of all Index types</h4>
<div class="paragraph">
<p>LlamaIndex&#8217;s index types share common features inherited from the <code>BaseIndex</code> class, allowing for customization across all index types. These shared features include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Nodes:</strong> Indexes are built upon nodes, which can be customized and dynamically updated through insertion and deletion. Indexes can be built from pre-existing nodes or from documents, with settings available to customize underlying mechanics.</p>
</li>
<li>
<p><strong>Storage Context:</strong> This defines how and where data is stored, crucial for efficient data management.</p>
</li>
<li>
<p><strong>Progress Display:</strong> The <code>show_progress</code> option uses <code>tqdm</code> to display progress bars for long operations.</p>
</li>
<li>
<p><strong>Retrieval Modes:</strong> Indexes offer pre-defined retrieval modes and customizable Retriever classes for query processing.</p>
</li>
<li>
<p><strong>Asynchronous Operations:</strong> The <code>use_async</code> parameter enables asynchronous processing for performance optimization.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Indexing may involve LLM calls, potentially raising cost and privacy concerns.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_understanding_the_vectorstoreindex">Understanding the VectorStoreIndex</h3>
<div class="sect3">
<h4 id="_a_simple_usage_example_for_the_vectorstoreindex">A simple usage example for the VectorStoreIndex</h4>
<div class="paragraph">
<p>The <code>VectorStoreIndex</code> in LlamaIndex provides a simple way to ingest documents and make them searchable. It automatically handles node parsing (breaking down documents into chunks) using default or customizable parameters like chunk size and overlap.</p>
</div>
<div class="paragraph">
<p>Here&#8217;s a breakdown of the process:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Ingestion:</strong> Documents are loaded using <code>SimpleDirectoryReader</code>.</p>
</li>
<li>
<p><strong>Node Creation:</strong> Documents are split into nodes (chunks of text).</p>
</li>
<li>
<p><strong>Embedding:</strong> These nodes are converted into high-dimensional vectors using a language model.</p>
</li>
<li>
<p><strong>Storage:</strong> The vectors are stored in a vector store.</p>
</li>
<li>
<p><strong>Querying:</strong>  Incoming queries are also embedded, and their similarity to the stored vectors is calculated using cosine similarity.</p>
</li>
<li>
<p><strong>Retrieval:</strong> The most similar vectors (and their corresponding document chunks) are returned as the query result.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p><strong>Key Parameters:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>use_async</code>: Enables asynchronous calls (default: <code>False</code>).</p>
</li>
<li>
<p><code>show_progress</code>: Displays progress bars during index construction (default: <code>False</code>).</p>
</li>
<li>
<p><code>store_nodes_override</code>: Forces storage of Node objects (default: <code>False</code>).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The index utilizes <strong>fixed-size chunking</strong> by default, but performance can be optimized by testing different chunk sizes. The core strength of this index lies in its ability to perform <strong>semantic search</strong> by leveraging vector similarity.</p>
</div>
</div>
<div class="sect3">
<h4 id="_understanding_embeddings">Understanding embeddings</h4>
<div class="paragraph">
<p>Vector embeddings are a way to translate data (text, images, sounds, etc.) into a numerical format that Large Language Models (LLMs) can understand. Think of them as converting information into a "standard language" for the LLM.</p>
</div>
<div class="paragraph">
<p>Here&#8217;s a breakdown of the key ideas:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Numerical Representation:</strong> Embeddings represent data as lists of numbers (vectors). These numbers capture the <strong>meaning</strong> of the data.</p>
</li>
<li>
<p><strong>Semantic Understanding:</strong>  LLMs use these numbers to understand relationships between concepts – like synonyms or different meanings of the same word (e.g., "bank" as a riverbank vs. a financial institution).</p>
</li>
<li>
<p><strong>Similarity Search:</strong> Embeddings allow LLMs to find data that is <strong>similar</strong> in meaning. This is done by calculating the "distance" between vectors.  A process called "top-k similarity search" finds the <strong>k</strong> most similar pieces of data.</p>
</li>
<li>
<p><strong>Context is Key:</strong> The size of the text chunks used to create embeddings matters. Too small, and context is lost; too large, and meaning can be diluted.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Essentially, vector embeddings allow LLMs to "see" and "think" about data in a structured way, enabling them to process information and generate relevant responses. They are fundamental to how LLMs work with and understand the world around them.</p>
</div>
</div>
<div class="sect3">
<h4 id="_understanding_similarity_search">Understanding similarity search</h4>
<div class="paragraph">
<p>This text discusses the importance of <strong>similarity search</strong> in machine learning, particularly with the rise of <strong>embeddings</strong> which capture semantic meaning in vector form. Identifying similar vectors allows machines to understand relationships in data and is crucial for applications like recommendation systems and information retrieval.</p>
</div>
<div class="paragraph">
<p>The document focuses on three methods LlamaIndex uses to measure vector similarity:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Cosine Similarity:</strong> Measures the angle between two vectors – a smaller angle indicates higher similarity. It&#8217;s less sensitive to vector length and is the default method in LlamaIndex.</p>
</li>
<li>
<p><strong>Dot Product:</strong> Calculates similarity based on the alignment and length of vectors. Higher values indicate greater similarity, but it <strong>is</strong> sensitive to vector length, potentially biasing results towards longer documents.</p>
</li>
<li>
<p><strong>Euclidean Distance:</strong> Measures the actual distance between vector values, useful when vector dimensions represent real-world measurements.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The key difference lies in how each method approaches similarity: cosine similarity and dot product focus on <strong>direction</strong>, while Euclidean distance focuses on <strong>magnitude/distance</strong>. Understanding these differences is important for choosing the right method for a specific Retrieval-Augmented Generation (RAG) scenario.</p>
</div>
</div>
<div class="sect3">
<h4 id="_ok_but_how_does_llamaindex_generate_these_embeddings">OK, but how does LlamaIndex generate these embeddings?</h4>
<div class="paragraph">
<p>LlamaIndex defaults to using OpenAI’s <code>text-embedding-ada-002</code> model for creating text embeddings, which are crucial for tasks like semantic search. However, it offers flexibility to use alternative models due to cost, privacy, or specialization needs.</p>
</div>
<div class="paragraph">
<p><strong>Key takeaways:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Alternatives to OpenAI:</strong> LlamaIndex supports various embedding models beyond OpenAI, including local models and those from other providers.</p>
</li>
<li>
<p><strong>Hugging Face Integration:</strong>  A popular option is using models from <strong>Hugging Face</strong>, a community-driven platform for AI models (particularly in NLP).  The <code>llama-index-embeddings-huggingface</code> package enables this, with <code>BAAI/bge-small-en-v1.5</code> as a well-balanced default local model.</p>
</li>
<li>
<p><strong>Custom Models:</strong> Advanced users can create and integrate their own custom embedding models by extending LlamaIndex’s <code>BaseEmbedding</code> class.</p>
</li>
<li>
<p><strong>Further Integrations:</strong> LlamaIndex also integrates with Langchain, Azure, CohereAI, and other providers, expanding the range of available embedding models.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In essence, LlamaIndex provides a versatile system for handling text embeddings, allowing users to choose the model that best fits their requirements and constraints.</p>
</div>
</div>
<div class="sect3">
<h4 id="_how_do_i_decide_which_embedding_model_i_should_use">How do I decide which embedding model I should use?</h4>
<div class="paragraph">
<p>Choosing the right embedding model is crucial for a successful Retrieval-Augmented Generation (RAG) application, impacting performance, quality, and cost. Key considerations include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Performance:</strong> Both qualitative (semantic understanding, domain specificity) and quantitative (semantic similarity, benchmarks like <strong>MTEB Leaderboard</strong> - <a href="https://huggingface.co/spaces/mteb/leaderboard" class="bare">https://huggingface.co/spaces/mteb/leaderboard</a> are important.</p>
</li>
<li>
<p><strong>Speed &amp; Efficiency:</strong> Latency and throughput matter for real-time applications, as queries need to be embedded quickly. Consider input chunk size limitations.</p>
</li>
<li>
<p><strong>Language Support:</strong> Choose a model that supports the languages your application requires.</p>
</li>
<li>
<p><strong>Resources &amp; Cost:</strong> Balance embedding accuracy with computational costs, storage, and API usage fees.</p>
</li>
<li>
<p><strong>Accessibility:</strong> Consider availability (API vs. local install) and ease of integration.</p>
</li>
<li>
<p><strong>Privacy &amp; Connectivity:</strong> Local models offer privacy and offline functionality.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>LlamaIndex</strong> offers flexibility and supports many embedding models (see <a href="https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html#list-of-supported-embeddings" class="bare">https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html#list-of-supported-embeddings</a>.</p>
</div>
<div class="paragraph">
<p>While <strong>OpenAI’s <code>text-embedding-ada-002</code></strong> is a good default choice, benchmarking different models is recommended to optimize for specific application needs. Resources like <a href="https://blog.getzep.com/text-embedding-latency-a-semi-scientific-look/" class="bare">https://blog.getzep.com/text-embedding-latency-a-semi-scientific-look/</a> can help evaluate model performance.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_persisting_and_reusing_indexes">Persisting and reusing Indexes</h3>
<div class="exampleblock">
<div class="content">
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/ch5/sample_persist.py" target="_blank">
ch5/sample_persist.py</a>
<hr>
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/ch5/sample_persist_reload.py" target="_blank">
ch5/sample_persist_reload.py</a>
</div>
</div>
<div class="paragraph">
<p>This text discusses the importance of storing vector embeddings generated by LlamaIndex to avoid redundant computation and ensure consistent query results. Here&#8217;s a summary:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Why persist embeddings?</strong> Re-embedding documents is computationally expensive and slow. Storing embeddings allows for faster processing, lower costs, and consistent query accuracy.</p>
</li>
<li>
<p><strong>Vector Stores in LlamaIndex:</strong> LlamaIndex uses vector stores for efficient storage and retrieval of these embeddings. It defaults to in-memory storage, but offers persistence via the <code>.persist()</code> method.</p>
</li>
<li>
<p><strong>How to persist and load:</strong></p>
</li>
<li>
<p>Use <code>index.storage_context.persist(persist_dir="index_cache")</code> to save the index data to disk.</p>
</li>
<li>
<p>Use <code>StorageContext.from_defaults()</code> and <code>load_index_from_storage()</code> to reload the index from the saved directory in future sessions, avoiding re-indexing.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In essence, the text explains how to save and reload LlamaIndex indexes to disk for efficiency and consistency.</p>
</div>
<div class="sect3">
<h4 id="_understanding_the_storagecontext">Understanding the StorageContext</h4>
<div class="paragraph">
<p>The <code>StorageContext</code> in LlamaIndex is a central component for managing data storage during indexing and querying. It encompasses four key stores:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Document Store:</strong> Stores documents locally in <code>docstore.json</code>.</p>
</li>
<li>
<p><strong>Index Store:</strong> Stores index structures locally in <code>index_store.json</code>.</p>
</li>
<li>
<p><strong>Vector Stores:</strong> Manages multiple vector stores (locally in <code>vector_store.json</code> by default).</p>
</li>
<li>
<p><strong>Graph Store:</strong> Stores graph data structures in <code>graph_store.json</code>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>LlamaIndex automatically creates these local storage files when using the <code>persist()</code> method, but allows for custom persistence locations.</p>
</div>
<div class="paragraph">
<p>While basic local stores are provided, the <code>StorageContext</code> is designed to be flexible, supporting integrations with more robust solutions like AWS S3, Pinecone, and MongoDB.</p>
</div>
<div class="paragraph">
<p>The example demonstrates customizing vector storage using <strong>ChromaDB</strong>:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Install <code>chromadb</code> via pip.</p>
</li>
<li>
<p>Initialize a Chroma client and create a collection (<code>my_chroma_store</code>).</p>
</li>
<li>
<p>Create a <code>ChromaVectorStore</code> instance linked to the Chroma collection.</p>
</li>
<li>
<p>Integrate the <code>ChromaVectorStore</code> into the <code>StorageContext</code>.</p>
</li>
<li>
<p>Build an index using the customized <code>StorageContext</code>.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>This approach simplifies working with vector databases, abstracting away complexity and allowing developers to focus on application logic.  LlamaIndex offers a scalable solution, ranging from simple in-memory storage to cloud-hosted databases, with easy component swapping.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_exploring_other_index_types_in_llamaindex">Exploring other index types in LlamaIndex</h3>
<div class="sect3">
<h4 id="_the_summaryindex">The SummaryIndex</h4>
<div class="paragraph">
<p>The <code>SummaryIndex</code> is a simple and efficient indexing method in LlamaIndex, differing from the <code>VectorStoreIndex</code> by storing data in a sequential list of nodes <strong>without</strong> using embeddings or a vector store. This makes it faster and less resource-intensive.</p>
</div>
<div class="paragraph">
<p><strong>Key features and use cases:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Simple Structure:</strong> Data is stored as a list of chunks from ingested documents.</p>
</li>
<li>
<p><strong>No LLM or Embeddings:</strong> Operates locally without requiring large language models or embedding models during indexing.</p>
</li>
<li>
<p><strong>Linear Scan:</strong>  Retrieval involves scanning the list sequentially for relevant information.</p>
</li>
<li>
<p><strong>Suitable for:</strong> Documentation search, scenarios with resource constraints, or when complex semantic search isn&#8217;t necessary.</p>
</li>
<li>
<p><strong>Usage:</strong> Easily created using <code>SummaryIndex.from_documents()</code>.</p>
</li>
<li>
<p><strong>Refinement Process:</strong> Uses a "create and refine" approach during queries, building an initial response and then refining it with additional context.</p>
</li>
<li>
<p><strong>Retrievers:</strong> Compatible with different retrievers (<code>SummaryIndexRetriever</code>, <code>SummaryIndexEmbeddingRetriever</code>, <code>SummaryIndexLLMRetriever</code>) for varied search mechanisms.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In essence, the <code>SummaryIndex</code> provides a straightforward way to index and search data when speed and simplicity are prioritized over complex semantic understanding.</p>
</div>
</div>
<div class="sect3">
<h4 id="_the_documentsummaryindex">The DocumentSummaryIndex</h4>
<div class="paragraph">
<p>The <code>DocumentSummaryIndex</code> is a specialized indexing tool within LlamaIndex designed for efficient document retrieval, particularly useful for large datasets where quick access to specific documents is needed.</p>
</div>
<div class="paragraph">
<p><strong>Key Features &amp; Functionality:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Summarization:</strong> It works by summarizing each document and linking these summaries to the document&#8217;s underlying nodes.</p>
</li>
<li>
<p><strong>Efficient Retrieval:</strong>  These summaries act as a quick filter, identifying relevant documents before deeper analysis.</p>
</li>
<li>
<p><strong>Use Case:</strong> Ideal for knowledge management systems within organizations dealing with extensive documentation (reports, policies, manuals, etc.). It avoids issues with embedding-based retrieval on entire datasets with similar text chunks.</p>
</li>
<li>
<p><strong>Customization:</strong> Offers parameters to control:</p>
</li>
<li>
<p><code>response_synthesizer</code>:  How summaries are generated.</p>
</li>
<li>
<p><code>summary_query</code>: The prompt used for summarization.</p>
</li>
<li>
<p><code>show_progress</code>: Display progress bars during indexing.</p>
</li>
<li>
<p><code>embed_summaries</code>:  Embed summaries for similarity-based searches (default is <code>True</code>).</p>
</li>
<li>
<p><strong>Retrieval Methods:</strong> Supports both embedding-based and LLM-based retrievers.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Basic Usage:</strong></p>
</div>
<div class="paragraph">
<p>Creating a <code>DocumentSummaryIndex</code> involves loading documents, summarizing them, and associating the summaries with the document nodes.  The <code>get_document_summary()</code> method allows access to the generated summaries for individual documents.</p>
</div>
<div class="paragraph">
<p>In essence, the <code>DocumentSummaryIndex</code> prioritizes speed and relevance by leveraging document summaries to narrow the search space, making it a valuable tool for specific retrieval scenarios.</p>
</div>
</div>
<div class="sect3">
<h4 id="_the_keywordtableindex">The KeywordTableIndex</h4>
<div class="paragraph">
<p>The <code>KeywordTableIndex</code> in LlamaIndex is an efficient index structure designed for rapid, targeted factual lookup based on keyword matching. It functions similarly to a glossary, creating a keyword-to-node mapping for quick retrieval of relevant information.</p>
</div>
<div class="paragraph">
<p><strong>Key Features:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Keyword-Based:</strong>  Instead of relying on complex embedding spaces, it uses a straightforward keyword table.</p>
</li>
<li>
<p><strong>Efficient Search:</strong> Enables fast retrieval by directly matching keywords in queries to those in the index.</p>
</li>
<li>
<p><strong>Customizable:</strong> Offers parameters like <code>keyword_extract_template</code> (for prompt customization), <code>max_keywords_per_chunk</code> (to manage table size), and <code>use_async</code> (for performance).</p>
</li>
<li>
<p><strong>Keyword Extraction:</strong>  Extracts keywords from documents using an LLM and a defined prompt, linking them to the source text chunks.</p>
</li>
<li>
<p><strong>Retrieval Modes:</strong> Supports simple keyword matching, RAKE, and LLM-based keyword extraction/matching.</p>
</li>
<li>
<p><strong>Alternatives:</strong> Offers <code>SimpleKeywordTableIndex</code> (regex-based) and <code>RAKEKeywordTableIndex</code> (using <code>rake_nltk</code>) as LLM-free options.</p>
</li>
<li>
<p><strong>Create and Refine:</strong> Like <code>SummaryIndex</code>, it uses a create and refine approach for final response synthesis.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The index is particularly useful when precise keyword matching is crucial, and provides a versatile tool for applications requiring keyword precision.  A simple example demonstrates its ease of use, automatically extracting keywords and setting up the retrieval system.</p>
</div>
</div>
<div class="sect3">
<h4 id="_the_treeindex">The TreeIndex</h4>
<div class="paragraph">
<p>The <code>TreeIndex</code> is a hierarchical data structure within LlamaIndex designed for efficient information organization and retrieval, particularly useful for complex datasets. Unlike a flat index, it organizes data in a tree format where each node summarizes its children, created recursively using LLMs and customizable summarization prompts.</p>
</div>
<div class="paragraph">
<p><strong>Key Features &amp; Parameters:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Hierarchical Structure:</strong> Data is organized in a tree, allowing for abstraction and efficient querying.</p>
</li>
<li>
<p><strong>Customizable Parameters:</strong></p>
</li>
<li>
<p><code>summary_template</code>: Prompt for summarization during index construction.</p>
</li>
<li>
<p><code>insert_prompt</code>: Prompt for integrating new nodes into the tree.</p>
</li>
<li>
<p><code>num_children</code>: Maximum number of child nodes per node (default is 10).</p>
</li>
<li>
<p><code>build_tree</code>:  Determines if the tree is built during index construction or query time.</p>
</li>
<li>
<p><code>use_async</code>: Enables asynchronous operation for faster processing of large datasets.</p>
</li>
<li>
<p><strong>Retrieval Modes:</strong> Offers various retrieval strategies including <code>TreeSelectLeafRetriever</code>, <code>TreeSelectLeafEmbeddingRetriever</code>, <code>TreeRootRetriever</code>, and <code>TreeAllLeafRetriever</code>.</p>
</li>
<li>
<p><strong>Query Process:</strong> Queries traverse the tree, identifying relevant keywords in node summaries to pinpoint relevant leaf nodes.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Usage:</strong></p>
</div>
<div class="paragraph">
<p>The <code>TreeIndex</code> is created from documents and used with a query engine to retrieve information. A simple example demonstrates loading documents and querying the index.</p>
</div>
<div class="paragraph">
<p><strong>Drawbacks:</strong></p>
</div>
<div class="paragraph">
<p>While powerful, <code>TreeIndex</code> has potential drawbacks:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Increased Computation:</strong> Building and maintaining the tree is computationally intensive.</p>
</li>
<li>
<p><strong>Recursive Retrieval:</strong> Querying involves recursive tree traversal, which can be slow.</p>
</li>
<li>
<p><strong>Summarization Overhead:</strong> Summarizing nodes adds to the processing cost.</p>
</li>
<li>
<p><strong>Storage Requirements:</strong> Requires more storage than flat indexes.</p>
</li>
<li>
<p><strong>Maintenance:</strong> Updates and insertions can be complex.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Overall:</strong></p>
</div>
<div class="paragraph">
<p>The <code>TreeIndex</code> is a valuable tool for RAG applications dealing with large, complex datasets where context and relationships are important. However, its computational and storage costs should be carefully considered against the benefits of improved retrieval performance. It excels in scenarios needing efficient, context-aware retrieval, particularly within organizations managing hierarchical data.</p>
</div>
</div>
<div class="sect3">
<h4 id="_the_knowledgegraphindex">The KnowledgeGraphIndex</h4>
<div class="paragraph">
<p>The <code>KnowledgeGraphIndex</code> in LlamaIndex is a tool for enhancing query processing by building a <strong>knowledge graph (KG)</strong> from text data. It primarily uses an LLM to extract <strong>triplets</strong> (subject-predicate-object) from text, but allows for custom extraction functions.</p>
</div>
<div class="paragraph">
<p><strong>Key Features &amp; Benefits:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Relationship Focus:</strong> Excels at understanding complex relationships between entities and concepts, providing context-aware responses. Ideal for multifaceted questions.</p>
</li>
<li>
<p><strong>Use Cases:</strong> Suitable for applications like news aggregation, where tracking entities and their relationships over time is valuable.</p>
</li>
<li>
<p><strong>Customization:</strong> Offers several customizable parameters:</p>
</li>
<li>
<p><code>kg_triple_extract_template</code>:  Controls how triplets are identified.</p>
</li>
<li>
<p><code>max_triplets_per_chunk</code>: Limits triplets per text chunk.</p>
</li>
<li>
<p><code>graph_store</code>: Defines graph storage type.</p>
</li>
<li>
<p><code>include_embeddings</code>:  Adds embeddings for enhanced retrieval.</p>
</li>
<li>
<p><code>max_object_length</code>: Limits the length of the object in a triplet.</p>
</li>
<li>
<p><code>kg_triplet_extract_fn</code>: Allows for custom triplet extraction.</p>
</li>
<li>
<p><strong>Construction:</strong> Builds the KG by either using a default LLM-based triplet extraction method or a user-provided custom function. Embeddings can be included for each triplet.</p>
</li>
<li>
<p><strong>Querying:</strong> Utilizes three distinct retrievers (<code>KGTableRetriever</code>, <code>KnowledgeGraphRAGRetriever</code>, and a hybrid mode) to retrieve relevant information from the KG.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In essence, the <code>KnowledgeGraphIndex</code> transforms text into a structured knowledge representation, enabling more intelligent and contextually relevant query responses.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_building_indexes_on_top_of_other_indexes_with_composablegraph">Building Indexes on top of other Indexes with ComposableGraph</h3>
<div class="paragraph">
<p>The <code>ComposableGraph</code> in LlamaIndex is a method for structuring information by <strong>hierarchically stacking Indexes</strong>. It allows you to build lower-level Indexes within individual documents (like <code>TreeIndex</code>) and then aggregate those into higher-level Indexes over a collection of documents (like <code>SummaryIndex</code>).</p>
</div>
<div class="paragraph">
<p><strong>Key features and functionality:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Hierarchical Structure:</strong> Enables organization of detailed information within documents and summarization across collections.</p>
</li>
<li>
<p><strong>Construction:</strong> Built using <code>ComposableGraph.from_indices()</code>, requiring a root Index class (e.g., <code>SummaryIndex</code>), child Indexes (e.g., <code>TreeIndex</code>), and summaries for each child Index.</p>
</li>
<li>
<p><strong>Querying:</strong>  A <code>ComposableGraphQueryEngine</code> recursively traverses the hierarchy, starting from the root summary Index, to retrieve relevant information from lower-level Indexes.</p>
</li>
<li>
<p><strong>Customization:</strong> Allows for custom query engines at each Index level for tailored retrieval strategies.</p>
</li>
<li>
<p><strong>Summaries:</strong>  Summaries can be manually defined or automatically generated using queries or <code>SummaryExtractor</code>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Benefits:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Efficient retrieval of information from both high-level summaries and detailed, low-level Indexes.</p>
</li>
<li>
<p>Comprehensive understanding of complex datasets.</p>
</li>
<li>
<p>Deep, hierarchical understanding of data.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In essence, <code>ComposableGraph</code> provides a powerful way to organize and query complex information by leveraging a layered indexing approach.</p>
</div>
</div>
<div class="sect2">
<h3 id="_estimating_the_potential_cost_of_building_and_querying_indexes">Estimating the potential cost of building and querying Indexes</h3>
<div class="paragraph">
<p>This text details the potential costs and privacy concerns associated with using Indexes in LlamaIndex, primarily due to their reliance on Large Language Models (LLMs) for building and querying.</p>
</div>
<div class="paragraph">
<p><strong>Key takeaways:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Cost Considerations:</strong> Repeated LLM calls, especially during index construction (like <code>TreeIndex</code> or <code>KeywordTableIndex</code>) and embedding generation (like <code>VectorStoreIndex</code>), can quickly become expensive.</p>
</li>
<li>
<p><strong>Best Practices for Cost Reduction:</strong></p>
</li>
<li>
<p>Utilize Indexes that minimize LLM calls during building (e.g., <code>SummaryIndex</code>, <code>SimpleKeywordTableIndex</code>).</p>
</li>
<li>
<p>Employ cheaper LLM models when full accuracy isn&#8217;t essential.</p>
</li>
<li>
<p>Cache and reuse existing Indexes to avoid redundant building.</p>
</li>
<li>
<p>Optimize query parameters (e.g., <code>similarity_top_k</code>) to reduce LLM calls.</p>
</li>
<li>
<p>Use local LLM and embedding models for cost control and enhanced data privacy.</p>
</li>
<li>
<p><strong>Cost Estimation:</strong> The text provides practical examples using <code>MockLLM</code> and <code>MockEmbedding</code> with <code>TokenCountingHandler</code> to estimate LLM and embedding token usage <strong>before</strong> building and querying indexes. This allows for proactive cost management.</p>
</li>
<li>
<p><strong>RAG &amp; Smaller Models:</strong> Retrieval-Augmented Generation (RAG) enhances the performance of smaller models by providing access to external knowledge, mitigating the need for excessively large, costly models.</p>
</li>
<li>
<p><strong>Importance of Prediction:</strong>  Always estimate token usage before indexing large datasets to avoid unexpected expenses.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In essence, the document advocates for a proactive approach to cost and privacy management when using LlamaIndex Indexes, emphasizing estimation, optimization, and the potential benefits of local models.</p>
</div>
</div>
<div class="sect2">
<h3 id="_indexing_our_pits_study_materials_hands_on">Indexing our PITS study materials – hands-on</h3>
<div class="paragraph">
<p>This text details the implementation of an <code>index_builder.py</code> module for a tutoring application using LlamaIndex. The module is responsible for creating and loading indexes for efficient data retrieval.</p>
</div>
<div class="paragraph">
<p>Here&#8217;s a summary of the key points:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Two Index Types:</strong> The module creates two types of indexes: a <code>VectorStoreIndex</code> and a <code>TreeIndex</code>.</p>
</li>
<li>
<p><strong>Persistence:</strong> The code first attempts to load existing indexes from a specified storage location (<code>INDEX_STORAGE</code>). This avoids rebuilding the indexes if they already exist, saving time and resources.</p>
</li>
<li>
<p><strong>Index IDs:</strong> When multiple indexes are stored in the same location, <code>index_id</code> is used to differentiate and correctly load them.</p>
</li>
<li>
<p><strong>Building New Indexes:</strong> If the indexes are not found in storage, they are built from provided <code>nodes</code> (presumably document chunks). Each index is assigned a unique ID (<code>"vector"</code> and <code>"tree"</code>) using <code>set_index_id</code>.</p>
</li>
<li>
<p><strong>Storage:</strong> Newly created indexes are persisted to the <code>INDEX_STORAGE</code> directory for future use.</p>
</li>
<li>
<p><strong>Return Value:</strong> The <code>build_indexes</code> function returns both the <code>vector_index</code> and <code>tree_index</code> objects.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The code provides a basic implementation with potential for improvement, and the next step (covered in Chapter 6) will focus on querying the data using these indexes.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_part_3_retrieving_and_working_with_indexed_data">Part 3: Retrieving and Working with Indexed Data</h2>
<div class="sectionbody">

</div>
</div>
<div class="sect1">
<h2 id="_chapter_6_querying_our_data_part_1_context_retrieval">Chapter 6: Querying Our Data, Part 1 – Context Retrieval</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_understanding_the_basic_retrievers">Understanding the basic retrievers</h3>
<div class="paragraph">
<p>This text explains <strong>retrieval mechanisms</strong> within the LlamaIndex RAG (Retrieval-Augmented Generation) system. Here&#8217;s a summary:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Core Function:</strong> Retrievers find relevant information ("nodes") from an index to provide context for generating responses. They return results as <code>NodeWithScore</code> objects, which include a relevance score (though not all retrievers provide a score).</p>
</li>
<li>
<p><strong>Construction Methods:</strong> Retrievers can be created in two main ways:</p>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>From an Index:</strong> Using the <code>as_retriever()</code> method of an index object (e.g., <code>summary_index.as_retriever()</code>).</p>
</li>
<li>
<p><strong>Direct Instantiation:</strong> Directly creating a retriever object (e.g., <code>SummaryIndexEmbeddingRetriever(index=summary_index)</code>).</p>
</li>
</ol>
</div>
</li>
<li>
<p><strong>Upcoming Information:</strong> The text previews a detailed list of available retriever options for each index type within LlamaIndex, intended as a reference for building applications.</p>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="_the_vectorstoreindex_retrievers">The VectorStoreIndex retrievers</h4>
<div class="paragraph">
<p>This document details various retriever options available within the LlamaIndex framework for different index types, focusing on how they function and their customization options.</p>
</div>
<div class="paragraph">
<p><strong>1. VectorIndex Retrievers:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong><code>VectorIndexRetriever</code>:</strong> The default retriever for <code>VectorStoreIndex</code>, it uses vector similarity search. Key customizable parameters include:</p>
</li>
<li>
<p><code>similarity_top_k</code>: Number of top results returned.</p>
</li>
<li>
<p><code>vector_store_query_mode</code>:  Query mode for the vector store (e.g., Pinecone, OpenSearch).</p>
</li>
<li>
<p><code>filters</code>, <code>doc_ids</code>, <code>node_ids</code>:  Methods for narrowing search scope using metadata or IDs.</p>
</li>
<li>
<p><code>alpha</code>, <code>sparse_top_k</code>: Parameters for hybrid (sparse &amp; dense) search.</p>
</li>
<li>
<p><code>vector_store_kwargs</code>:  For passing specific arguments to the vector store.</p>
</li>
<li>
<p><strong><code>VectorIndexAutoRetriever</code>:</strong> A more advanced retriever that uses an LLM to automatically optimize query parameters based on content description and metadata, useful for complex or ambiguous data.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>2. SummaryIndex Retrievers:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong><code>SummaryIndexRetriever</code>:</strong> Returns <strong>all</strong> nodes in the index without filtering or sorting – useful for a complete data view.</p>
</li>
<li>
<p><strong><code>SummaryIndexEmbeddingRetriever</code>:</strong> Uses embeddings (created dynamically) to find the most relevant nodes based on similarity to the query, returning nodes with a relevance score (<code>NodeWithScore</code>).</p>
</li>
<li>
<p><strong><code>SummaryIndexLLMRetriever</code>:</strong> Leverages an LLM and a prompt to select relevant nodes.  Customizable via:</p>
</li>
<li>
<p><code>choice_select_prompt</code>: Override the default prompt.</p>
</li>
<li>
<p><code>choice_batch_size</code>: Batch size for query processing.</p>
</li>
<li>
<p><code>format_node_batch_fn</code>, <code>parse_choice_select_answer_fn</code>: Functions for formatting node batches and parsing LLM responses (including relevance score calculation).</p>
</li>
<li>
<p><code>service_context</code>: Allows customization of the LLM used.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>General Considerations:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Security:</strong> Filtering information early in the RAG process (at the retriever stage) is a secure design principle.</p>
</li>
<li>
<p><strong>Cost:</strong> Reducing the amount of information processed by the LLM (through filtering) can lower costs.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The document emphasizes choosing the appropriate retriever based on the data&#8217;s structure, the user&#8217;s familiarity with the data, and the desired level of control over the search process.</p>
</div>
</div>
<div class="sect3">
<h4 id="_the_documentsummaryindex_retrievers">The DocumentSummaryIndex retrievers</h4>
<div class="paragraph">
<p>The text details two retrieval options for a <code>DocumentSummaryIndex</code>: <code>DocumentSummaryIndexLLMRetriever</code> and <code>DocumentSummaryIndexEmbeddingRetriever</code>.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Uses an LLM to select relevant summaries from document summaries.</p>
</li>
<li>
<p>Processes queries in batches, configurable with <code>choice_batch_size</code>.</p>
</li>
<li>
<p>Allows custom prompts (<code>choice_select_prompt</code>) and functions for formatting nodes for the LLM (<code>format_node_batch_fn</code>) and parsing the LLM&#8217;s response (<code>parse_choice_select_answer_fn</code>).</p>
</li>
<li>
<p>Returns results sorted by relevance <strong>and</strong> includes a relevance score for each node.</p>
</li>
<li>
<p><strong>Note:</strong> Experimentation showed LLM-assigned relevance scores tend to be consistently high, potentially requiring prompt adjustments for nuanced differentiation.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong><code>DocumentSummaryIndexEmbeddingRetriever</code>:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Relies on embeddings to find summaries with the highest similarity to the query.</p>
</li>
<li>
<p>Requires the index to be built with <code>embed_summaries=True</code>.</p>
</li>
<li>
<p>Uses <code>similarity_top_k</code> to specify the number of summaries to return.</p>
</li>
<li>
<p><strong>Does not</strong> return a relevance score.</p>
</li>
<li>
<p>Effective for finding relevant summaries based on embedding similarity.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In essence, the LLM retriever leverages natural language understanding for more sophisticated relevance assessment (with scores), while the embedding retriever uses a faster, similarity-based approach.</p>
</div>
</div>
<div class="sect3">
<h4 id="_the_treeindex_retrievers">The TreeIndex retrievers</h4>
<div class="paragraph">
<p>This text details the <code>TreeIndex</code> in LlamaIndex, a complex index type designed for hierarchical data like filesystems or organizational charts. It&#8217;s important to note that <code>TreeIndex</code> builds a <strong>new</strong> hierarchical structure based on summaries of the original data, not simply reflecting existing hierarchies.  Querying this structure can be computationally expensive due to its recursive nature.</p>
</div>
<div class="paragraph">
<p>Here&#8217;s a breakdown of the different retrieval methods available for <code>TreeIndex</code>:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong><code>TreeSelectLeafRetriever</code> (Default):</strong> Recursively navigates the tree, using an LLM to identify the most relevant leaf nodes.  The <code>child_branch_factor</code> controls how many child nodes are considered at each level (defaults to 1). Offers customizable prompt templates for query refinement. Doesn&#8217;t return relevance scores.</p>
</li>
<li>
<p><strong><code>TreeSelectLeafEmbeddingRetriever</code>:</strong> Similar to <code>TreeSelectLeafRetriever</code>, but uses embedding similarity to select nodes instead of an LLM. Includes an <code>embed_model</code> parameter for specifying the embedding model. Doesn&#8217;t return relevance scores.</p>
</li>
<li>
<p><strong><code>TreeAllLeafRetriever</code>:</strong>  Retrieves <strong>all</strong> leaf nodes, regardless of hierarchy, and sorts them.  Fastest option, useful for ensuring no information is missed, but doesn&#8217;t provide relevance scores.</p>
</li>
<li>
<p><strong><code>TreeRootRetriever</code>:</strong> Retrieves responses directly from the root nodes of the tree, assuming answers are pre-computed and stored there.  Efficient when information is already summarized at the top level. Doesn&#8217;t return relevance scores.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Practical Use Case:</strong> The text highlights a clinical decision support system (CDSS) as a good example, where pre-computed answers to common medical questions are stored in root nodes for quick retrieval.</p>
</div>
<div class="paragraph">
<p>In essence, <code>TreeIndex</code> offers flexibility in how you navigate and retrieve information from hierarchical data, with trade-offs between speed, computational cost, and the need for relevance scoring.</p>
</div>
</div>
<div class="sect3">
<h4 id="_the_keywordtableindex_retrievers">The KeywordTableIndex retrievers</h4>
<div class="paragraph">
<p>The <code>KeywordTableIndex</code> retrieves information by first <strong>extracting keywords from a query</strong>. This extraction method varies depending on the retriever used. Once keywords are extracted, the retriever <strong>counts their frequency within the indexed nodes</strong> and <strong>sorts nodes by matching keyword count</strong> (typically descending, indicating relevance). Results are returned as <code>NodeWithScore</code> objects, though <strong>relevance scores are not directly provided by the index itself</strong>.</p>
</div>
<div class="paragraph">
<p>There are three main retriever options:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>KeywordTableGPTRetriever:</strong> Uses an LLM to identify keywords.</p>
</li>
<li>
<p><strong>KeywordTableSimpleRetriever:</strong> Uses a faster, regex-based keyword extraction method.</p>
</li>
<li>
<p><strong>KeywordTableRAKERetriever:</strong> Employs the RAKE method for keyword extraction.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Common arguments for configuring these retrievers include: <code>query_keyword_extract_template</code> (for the default retriever), <code>max_keywords_per_query</code>, and <code>num_chunks_per_query</code> to control query complexity and system performance.</p>
</div>
</div>
<div class="sect3">
<h4 id="_the_knowledgegraphindex_retrievers">The KnowledgeGraphIndex retrievers</h4>
<div class="paragraph">
<p>This text details two types of retrievers used with Knowledge Graph Indices in LlamaIndex: <code>KGTableRetriever</code> and <code>KnowledgeGraphRAGRetriever</code>. Both extract relevant information (nodes) from a knowledge graph based on user queries, which are structured as triplets (subject, predicate, object).</p>
</div>
<div class="paragraph">
<p><strong>KGTableRetriever:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Is the default retriever and operates in three modes:</p>
</li>
<li>
<p><strong>Keyword:</strong> Uses keywords from the query to find matching nodes (case-sensitive).</p>
</li>
<li>
<p><strong>Embedding:</strong> Converts the query to an embedding and finds similar nodes.</p>
</li>
<li>
<p><strong>Hybrid:</strong> Combines keyword and embedding searches for precision and semantic understanding.</p>
</li>
<li>
<p>Offers several customizable parameters to control keyword extraction, query refinement, and the amount of information retrieved (e.g., <code>max_keywords_per_query</code>, <code>similarity_top_k</code>).</p>
</li>
<li>
<p>Returns a default score of 1000 for retrieved nodes.</p>
</li>
<li>
<p>If no nodes are found, returns a placeholder node indicating "No relationships found".</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>KnowledgeGraphRAGRetriever:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Identifies key entities in the query and uses them to navigate the graph.</p>
</li>
<li>
<p>Utilizes entity extraction and synonym expansion to broaden the query context.</p>
</li>
<li>
<p>Traverses the graph to a specified depth (<code>graph_traversal_depth</code>).</p>
</li>
<li>
<p>Also operates in keyword, embedding, and hybrid modes (though as of January 2024, only keyword mode was fully implemented in v0.9.25).</p>
</li>
<li>
<p>Includes a <code>with_nl2graphquery</code> option to convert natural language queries into graph queries.</p>
</li>
<li>
<p>Offers parameters to control entity/synonym limits, expansion policies, and verbosity.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Both retrievers share the ability to customize prompts using <code>BasePromptTemplate</code> objects (detailed in a later chapter). They both aim to retrieve relevant knowledge sequences to answer user queries, balancing information quality and quantity through parameters like <code>max_knowledge_sequence</code>.</p>
</div>
</div>
<div class="sect3">
<h4 id="_efficient_use_of_retrieval_mechanisms_asynchronous_operation">Efficient use of retrieval mechanisms – asynchronous operation</h4>
<div class="paragraph">
<p>This text discusses the benefits of using <strong>asynchronous execution</strong> in LlamaIndex, as opposed to the previously used <strong>synchronous methods</strong>. While synchronous methods are simpler to understand, asynchronous operations improve <strong>performance, reduce latency, and enhance user experience</strong>—especially in applications with frequent, complex queries and large datasets.</p>
</div>
<div class="paragraph">
<p>The provided code example demonstrates how to run two retrievers in <strong>parallel</strong> using <code>asyncio.gather()</code>. Although the performance gain is minimal with a small dataset, the benefits become significant in real-world applications. The text then indicates it will move on to discussing more advanced retrieval methods.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_building_more_advanced_retrieval_mechanisms">Building more advanced retrieval mechanisms</h3>
<div class="sect3">
<h4 id="_implementing_metadata_filters">Implementing metadata filters</h4>
<div class="paragraph">
<p>This text demonstrates how to implement a retrieval system using LlamaIndex that filters results based on <strong>metadata</strong>, specifically to handle situations where the same term has different meanings depending on the user&#8217;s context (in this case, their department).</p>
</div>
<div class="paragraph">
<p>Here&#8217;s a breakdown:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>The Problem:</strong> Different departments within an organization may have differing definitions for the same concepts (e.g., "incident").</p>
</li>
<li>
<p><strong>The Solution:</strong>  Use metadata filtering to retrieve only the definition relevant to the current user&#8217;s department.</p>
</li>
<li>
<p><strong>Implementation:</strong></p>
<div class="ulist">
<ul>
<li>
<p><strong>Define User Departments:</strong> A dictionary maps users to their respective departments.</p>
</li>
<li>
<p><strong>Create Nodes with Metadata:</strong>  Text nodes are created, each containing a definition and metadata specifying the relevant department.</p>
</li>
<li>
<p><strong>Filtering Function:</strong> A function <code>show_report</code> uses <code>MetadataFilters</code> to retrieve nodes matching the user&#8217;s department.</p>
</li>
<li>
<p><strong>Retrieval:</strong> The <code>as_retriever</code> method is used with the filters to create a retriever that only returns relevant nodes.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Example:</strong>  Running the same query ("What is an incident?") for users "Alice" (Security) and "Bob" (IT) returns different definitions tailored to their departments.</p>
</li>
<li>
<p><strong>Advanced Filtering:</strong> While the default vector store in LlamaIndex only supports equality (<code>EQ</code>) filtering, more sophisticated vector stores (like Pinecone or ChromaDB) support a wider range of operators (greater than, less than, in, not in, etc.) for more complex filtering scenarios, such as access control based on clearance levels.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In essence, the text showcases a practical application of metadata filtering in LlamaIndex to achieve a form of "polymorphism" in information retrieval, delivering contextually appropriate results to different users.</p>
</div>
</div>
<div class="sect3">
<h4 id="_using_selectors_for_more_advanced_decision_logic">Using selectors for more advanced decision logic</h4>
<div class="paragraph">
<p>This text discusses the importance of <strong>selectors</strong> in advanced Retrieval-Augmented Generation (RAG) applications, particularly when dealing with diverse user queries. Because users may ask specific questions, seek general information, or request summaries/comparisons, a RAG system needs a way to dynamically choose the <strong>best</strong> retrieval method.</p>
</div>
<div class="paragraph">
<p>Selectors act as this decision-making component, implementing conditional logic to route queries to the appropriate tool (retriever, parser, index, etc.). LlamaIndex offers five types of selectors: <code>LLMSingleSelector</code>, <code>LLMMultiSelector</code>, <code>EmbeddingSingleSelector</code>, <code>PydanticSingleSelector</code>, and <code>PydanticMultiSelector</code>, which differ in how they make their selections (LLM reasoning, similarity calculations, or Pydantic objects).</p>
</div>
<div class="paragraph">
<p>The example provided demonstrates a simple <code>LLMSingleSelector</code> that uses an LLM to choose from a predefined list of options based on a user query, returning both the selected option and the reasoning behind the choice.  The text emphasizes that selectors are a generic mechanism applicable to various conditional logic scenarios within a RAG application, not just retrievers. It then introduces the concept of <code>ToolMetadata</code> as a more advanced selection method, setting the stage for further explanation.</p>
</div>
</div>
<div class="sect3">
<h4 id="_understanding_tools">Understanding tools</h4>
<div class="paragraph">
<p>This text explains how to implement an <strong>adaptive retrieval mechanism</strong> using LlamaIndex, enabling an application to intelligently choose the best retriever for a given query.</p>
</div>
<div class="paragraph">
<p>Here&#8217;s a summary of the key concepts and steps:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Agentic Functionality &amp; Tool Containers:</strong> The core idea is to use a generic container holding different functionalities (retrievers in this case) that can be selected at runtime based on context.</p>
</li>
<li>
<p><strong>LlamaHub Tools:</strong> LlamaHub provides a collection of pre-built tools for various tasks.</p>
</li>
<li>
<p><strong><code>RetrieverTool</code>:</strong> This class encapsulates a retriever and a textual description, allowing a selector to understand its purpose.</p>
</li>
<li>
<p><strong><code>RouterRetriever</code>:</strong> This object uses a selector to decide which <code>RetrieverTool</code> to use for a given query. It takes the selector and a list of <code>RetrieverTool</code> objects as input.</p>
</li>
<li>
<p><strong>Selectors (<code>PydanticMultiSelector</code>):</strong>  These determine which retriever(s) to use. <code>PydanticMultiSelector</code> can select multiple retrievers simultaneously, handling complex queries that require information from multiple sources. <code>PydanticSingleSelector</code> would only choose one.</p>
</li>
<li>
<p><strong>Implementation:</strong> The example code demonstrates creating two retrievers (one for Ancient Rome, one for dogs), wrapping them in <code>RetrieverTool</code> objects with descriptive text, and then combining them into a <code>RouterRetriever</code>.  Queries are then passed to the <code>RouterRetriever</code>, which dynamically selects the appropriate retriever based on the query&#8217;s content.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The text sets the stage for further discussion of more advanced retrieval and query engine techniques in later chapters.</p>
</div>
</div>
<div class="sect3">
<h4 id="_transforming_and_rewriting_queries">Transforming and rewriting queries</h4>
<div class="paragraph">
<p>This text introduces <code>QueryTransform</code> as a powerful tool for Retrieval-Augmented Generation (RAG) applications. It allows for the modification and rewriting of queries <strong>before</strong> they are used to search an index, improving retrieval relevance and accuracy.</p>
</div>
<div class="paragraph">
<p><strong>Key takeaways:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Purpose:</strong> To refine user queries into more effective search terms. A practical example given is a technical support chatbot where vague user descriptions can be transformed into specific technical queries.</p>
</li>
<li>
<p><strong>Variations:</strong> Several <code>QueryTransform</code> types exist, each with a specific function:</p>
<div class="ulist">
<ul>
<li>
<p><strong><code>IdentityQueryTransform</code>:</strong>  No modification – maintains default behavior.</p>
</li>
<li>
<p><strong><code>HyDEQueryTransform</code>:</strong> Generates hypothetical documents to improve relevance.</p>
</li>
<li>
<p><strong><code>DecomposeQueryTransform</code>:</strong> Breaks down complex queries into simpler subqueries.</p>
</li>
<li>
<p><strong><code>ImageOutputQueryTransform</code>:</strong> Formats results for image output (e.g., generating <code>&lt;img&gt;</code> tags).</p>
</li>
<li>
<p><strong><code>StepDecomposeQueryTransform</code>:</strong>  Decomposes queries while considering previous reasoning/context.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Example:</strong> The provided Python code demonstrates <code>DecomposeQueryTransform</code> taking a broad query ("Tell me about buildings in ancient Rome") and refining it into a more focused one ("What were some famous buildings in ancient Rome?"). This illustrates how transformation can lead to more accurate retrieval.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In essence, <code>QueryTransform</code> enhances RAG systems by bridging the gap between how users ask questions and how the index best understands and responds to them.</p>
</div>
</div>
<div class="sect3">
<h4 id="_creating_more_specific_sub_queries">Creating more specific sub-queries</h4>
<div class="paragraph">
<p>This text explains how to improve query performance in LlamaIndex by <strong>breaking down complex questions into simpler sub-queries</strong> using the <code>OpenAIQuestionGenerator</code>.</p>
</div>
<div class="paragraph">
<p>Here&#8217;s a summary of the key points:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Problem:</strong> Ambiguous or complex questions can lead to poor results from information retrieval systems.</p>
</li>
<li>
<p><strong>Solution:</strong>  <code>OpenAIQuestionGenerator</code> automatically generates more specific sub-questions from an initial query.</p>
</li>
<li>
<p><strong>How it works:</strong></p>
<div class="ulist">
<ul>
<li>
<p>It utilizes LLMs (specifically OpenAI&#8217;s by default) to understand the query and available tools.</p>
</li>
<li>
<p><code>ToolMetadata</code> is used to describe each retrieval tool (e.g., a vector index for Ancient Rome, a summary index for dogs).</p>
</li>
<li>
<p>The generator receives a list of tools and the original query, then outputs a list of <code>SubQuestion</code> objects, each containing a <code>tool_name</code> and a refined <code>sub_question</code>.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Benefits:</strong> More specific queries lead to better context for retrieval and higher-quality answers.</p>
</li>
<li>
<p><strong>Alternatives:</strong> <code>LLMQuestionGenerator</code> (allows use of any LLM) and <code>GuidanceQuestionGenerator</code> (guides query processing order) are also available.</p>
</li>
<li>
<p><strong>Next Steps:</strong> These sub-queries are used with a <code>SubQuestionQueryEngine</code> (discussed in a later chapter) to process the information.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In essence, the text demonstrates a technique for enhancing query accuracy by strategically decomposing complex requests into manageable, focused sub-questions.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_understanding_the_concepts_of_dense_and_sparse_retrieval">Understanding the concepts of dense and sparse retrieval</h3>
<div class="sect3">
<h4 id="_implementing_sparse_retrieval_in_llamaindex">Implementing sparse retrieval in LlamaIndex</h4>
<div class="paragraph">
<p>This document details sparse retrieval methods within the LlamaIndex framework, contrasting them with dense retrieval and outlining scenarios for their optimal use.</p>
</div>
<div class="paragraph">
<p><strong>Key Takeaways:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Sparse Retrieval Basics:</strong> Constructs like <code>KeywordTableIndex</code> are basic forms of sparse retrieval. LlamaIndex offers more advanced options like <code>BM25Retriever</code>, which refines TF-IDF by considering term frequency <strong>and</strong> document length for more accurate relevance scoring.  Installation requires <code>rank-bm25</code> and <code>llama-index-retrievers-bm25</code>.</p>
</li>
<li>
<p><strong>Sparse vs. Dense Retrieval:</strong></p>
<div class="ulist">
<ul>
<li>
<p><strong>Sparse Retrieval excels with:</strong> Precise queries containing specific keywords, citations, or phrases (e.g., legal documents). It efficiently handles structured data and direct references.</p>
</li>
<li>
<p><strong>Dense Retrieval excels with:</strong> Understanding semantic context and handling variations in phrasing (e.g., customer support chatbots). It&#8217;s better when users don&#8217;t use the exact keywords found in the knowledge base.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Hybrid Approach:</strong> Combining sparse and dense retrieval offers the benefits of both.  LlamaIndex&#8217;s selectors and routers can facilitate this.</p>
</li>
<li>
<p><strong>Handling Empty Results:</strong>  Retrievers can return empty results if no relevant content is found. Strategies to address this include fallback mechanisms, query expansion, and relevance scoring.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In essence, the document advocates for choosing the right retrieval method based on the nature of the data and the expected user queries, and highlights the possibility of combining both approaches for improved performance.</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_chapter_7_querying_our_data_part_2_postprocessing_and_response_synthesis">Chapter 7: Querying Our Data, Part 2 – Postprocessing and Response Synthesis</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_re_ranking_transforming_and_filtering_nodes_using_postprocessors">Re-ranking, transforming, and filtering nodes using postprocessors</h3>
<div class="sect3">
<h4 id="_similaritypostprocessor">SimilarityPostprocessor</h4>
<div class="paragraph">
<p>The <code>SimilarityPostprocessor</code> is a tool in LlamaIndex designed to refine the nodes retrieved for a query by filtering them based on a similarity score. It works by comparing each node&#8217;s similarity score to a user-defined <code>similarity_cutoff</code> threshold. Nodes with scores below this threshold (or with a score of <code>None</code>) are removed, ensuring that only the most relevant and semantically similar content is passed on to the language model for generating a response.</p>
</div>
<div class="paragraph">
<p><strong>Key features and benefits:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Improves Response Relevance:</strong> By filtering out irrelevant nodes, it increases the likelihood of the LLM producing a more focused and meaningful answer.</p>
</li>
<li>
<p><strong>Configurable Threshold:</strong> The <code>similarity_cutoff</code> parameter allows users to control the stringency of the filtering process.</p>
</li>
<li>
<p><strong>Simple Implementation:</strong>  It&#8217;s easily integrated into a retrieval pipeline, as demonstrated by the provided Python example.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Use Case:</strong></p>
</div>
<div class="paragraph">
<p>The example highlights its usefulness in customer support chatbots, where it can distinguish between highly relevant information (like return policies for damaged items) and irrelevant content (like product advertisements) when responding to user queries.</p>
</div>
<div class="paragraph">
<p>In essence, the <code>SimilarityPostprocessor</code> acts as a quality control step, ensuring that the LLM receives only the most pertinent information for accurate and effective response generation.</p>
</div>
</div>
<div class="sect3">
<h4 id="_keywordnodepostprocessor">KeywordNodePostprocessor</h4>
<div class="paragraph">
<p>The <code>KeywordNodePostprocessor</code> is a tool used in Retrieval-Augmented Generation (RAG) systems to refine node selection based on keywords. It filters nodes retrieved by a retriever, either <strong>requiring</strong> the presence of specific keywords or <strong>excluding</strong> nodes containing unwanted keywords. This enhances the relevance and accuracy of RAG responses, particularly useful for scenarios like corporate data access where sensitive information needs to be protected.</p>
</div>
<div class="paragraph">
<p><strong>Key Features &amp; Functionality:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Filtering Logic:</strong> Uses <code>required_keywords</code> (must contain) and <code>exclude_keywords</code> (must not contain) lists.</p>
</li>
<li>
<p><strong>Dependency:</strong> Requires the <code>spaCy</code> library for Natural Language Processing (NLP). Installation via <code>pip install spacy</code>.</p>
</li>
<li>
<p><strong>Input:</strong> Takes a list of <code>NodeWithScore</code> objects as input.</p>
</li>
<li>
<p><strong>Customization:</strong> Offers parameters for:</p>
<div class="ulist">
<ul>
<li>
<p><code>required_keywords</code>: Keywords that <strong>must</strong> be present.</p>
</li>
<li>
<p><code>exclude_keywords</code>: Keywords that trigger exclusion.</p>
</li>
<li>
<p><code>lang</code>: Specifies the language for <code>spaCy</code> processing (default is English - "en").</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Case Sensitivity:</strong> Keyword matching is case-sensitive; consider converting text to a consistent case for reliable results.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Practical Use Case:</strong> The provided example demonstrates filtering log entries to exclude those marked as <code>&lt;SECRET&gt;</code> or <code>&lt;RESTRICTED&gt;</code>, ensuring confidential data isn&#8217;t included in retrieval results.</p>
</div>
<div class="paragraph">
<p>In essence, <code>KeywordNodePostprocessor</code> provides a flexible and powerful way to control the content included in a RAG system&#8217;s responses, improving both relevance and security.</p>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="footer">
<div id="footer-text">
Last updated 2025-04-05 11:40:11 +0300
</div>
</div>
</body>
</html>