= Building Data-Driven Applications with LlamaIndex
:source-highlighter: coderay
:icons: font
:toc: left
:toclevels: 4
Andrei Gheorghiu

====
GitHub::
https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex
====

= Part 1: Introduction to Generative AI and LlamaIndex

== Chapter 2: LlamaIndex: The Hidden Jewel - An Introduction to the LlamaIndex Ecosystem

=== Introducing PITS – our LlamaIndex hands-on project

====
++++
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/tree/main/PITS_APP" target="_blank">
PITS_APP</a>
++++
====

The author introduces PITS, an AI tutor built with LlamaIndex, designed to provide a personalized and interactive learning experience. Users can upload study materials, after which PITS will assess the user's knowledge with a quiz and then create customized learning material, including slides and narration, divided into chapters. PITS will adapt to the user's knowledge level, answer questions, and remember the conversation context across multiple sessions. LlamaIndex will be used to understand and index the study materials, while GPT-4 will power the teaching interactions.

=== Familiarizing ourselves with the structure of the LlamaIndex code repository

====
++++
<a href="https://github.com/run-llama/llama_index" target="_blank">
https://github.com/run-llama/llama_index</a>
++++
====

The LlamaIndex framework's code, reorganized for modularity and efficiency, is structured as follows:

*   **llama-index-core:** The foundational package, providing essential framework components.
*   **llama-index-integrations:** Add-on packages for customizing the framework with specific LLMs, data loaders, embedding models, and vector store providers.
*   **llama-index-packs:** Ready-made templates developed by the community to kickstart applications.
*   **llama-index-cli:** Supports the LlamaIndex command-line interface.
*   **OTHERS:** Contains fine-tuning abstractions and experimental features.

Each subfolder within `llama-index-integrations` and `llama-index-packs` represents an individual package that can be installed via pip. For example, to use `llama_index.llms.mistralai`, you must first install the `llama-index-llms-mistralai` package. The book will list necessary packages at the beginning of each chapter.

= Part 2: Starting Your First LlamaIndex Project

== Chapter 3: Kickstarting your Journey with LlamaIndex

=== Uncovering the essential building blocks of LlamaIndex – documents, nodes, and indexes

////
This document provides an introduction to LlamaIndex and its key components for building Retrieval-Augmented Generation (RAG) applications. Here's a summary:

*   **LlamaIndex Overview:** LlamaIndex connects external data sources to LLMs by ingesting, structuring, and organizing data for efficient retrieval and querying.
*   **Documents:** Documents are containers for various types of raw data (text, PDFs, databases, APIs). They include the text itself, metadata (author, category), and a unique ID. Data loaders from LlamaHub are used to ingest data from various sources into Documents.
*   **Nodes:** Nodes are smaller, more manageable chunks of content extracted from Documents. They allow proprietary knowledge to fit within the model’s prompt limits, create semantic units of data centered around specific information, and allow the creation of relationships between Nodes. `TextNode` is a key class, containing text, character indices, templates, metadata, and relationships to other nodes. Nodes can be created manually or automatically using splitters like `TokenTextSplitter`.
*   **Node Relationships:** Nodes can be linked to each other (previous, next, parent, child, source) to enable contextual querying, track provenance, enable navigation, support knowledge graph construction, and improve index structure.
*   **Indexes:** Indexes are data structures that organize Nodes for optimized storage and retrieval. LlamaIndex supports various index types, including `SummaryIndex`, `DocumentSummaryIndex`, `VectorStoreIndex`, `TreeIndex`, `KeywordTableIndex`, `KnowledgeGraphIndex`, and `ComposableGraph`. Indexes are built from Nodes, allow insertion of new Nodes, and provide a query interface.
*   **QueryEngine:** A `QueryEngine` contains a retriever, node postprocessor, and response synthesizer. The retriever fetches relevant Nodes from the index. The node postprocessor transforms, re-ranks, or filters Nodes after they’ve been retrieved and before the final response is crafted. The response synthesizer crafts the final response using the LLM, formatting the retrieved Nodes into a prompt, generating a response, and post-processing the response.
*   **RAG Workflow:** The complete RAG workflow involves loading data as Documents, parsing Documents into Nodes, building an index from Nodes, running queries over the index to retrieve relevant Nodes, and synthesizing the final response.
*
////

==== Documents

LlamaIndex uses `Document` objects to contain and structure raw data from various sources like PDFs, databases, or APIs. A `Document` holds the text content, a unique ID, and metadata (additional information) for more specific queries. Documents can be created manually or, more commonly, generated in bulk using data loaders from LlamaHub, which supports various data formats and sources. An example is provided using the `WikipediaReader` to load data from Wikipedia articles into `Document` objects. The next step is converting these raw `Document` objects into a format that LLMs can process, which is where Nodes come in.

==== Nodes

Nodes are smaller, manageable chunks of content extracted from Documents, addressing prompt size limits by allowing selection of relevant information. They create semantic units of data centered around specific information and allow the creation of relationships between Nodes. In LlamaIndex, the `TextNode` class is a main focus, with attributes like `text`, `start_char_idx`, `end_char_idx`, `text_template`, `metadata_template`, `metadata_seperator`, and `metadata`. Nodes inherit Document-level metadata but can also be individually customized.

==== Manually creating the Node objects

The provided code demonstrates how to manually create `TextNode` objects from a `Document` object in LlamaIndex. It involves slicing the document's text and assigning it to individual nodes. Each node is automatically assigned a unique ID, but this can be customized. This manual approach offers full control over the node's text and metadata.

==== Automatically extracting Nodes from Documents using splitters

The `TokenTextSplitter` in LlamaIndex is a tool for chunking documents into nodes, which is important for RAG workflows. It splits text into chunks of whole sentences with a default overlap to maintain context. The splitter can be customized with parameters like `chunk_size` and `chunk_overlap`. The example shows how to use `TokenTextSplitter` on a `Document` object, splitting the text into nodes and inheriting metadata from the original document. A warning is triggered if the metadata is too large, leaving less room for the actual content text. The next chapter will cover more text-splitting and node-parsing techniques available in LlamaIndex.

==== Nodes don’t like to be alone – they crave relationships

This content explains how to manually create relationships between nodes in LlamaIndex, focusing on the "previous" and "next" relationships to maintain order within a document. It highlights that LlamaIndex can automatically create these relationships during node parsing. Additionally, it introduces other relationship types like "SOURCE," "PARENT," and "CHILD," which are useful for tracking the origin of nodes and representing hierarchical structures within the data. The content concludes by posing the question of why these relationships are important, setting the stage for further discussion on their utility.

==== Why are relationships important?

Creating relationships between Nodes in LlamaIndex enhances querying by providing more context, tracking provenance, enabling navigation, supporting knowledge graph construction, and improving index structure. These relationships augment Nodes with contextual connections, leading to more expressive querying and complex index topologies. After structuring raw data into queryable Nodes, the next step is to organize them into efficient indexes.

==== Indexes

The passage explains the concept of indexing in LlamaIndex, which is crucial for organizing data for retrieval-augmented generation (RAG). Indexing transforms messy data into structured knowledge that AI can use effectively. LlamaIndex supports various index types, including `SummaryIndex`, `DocumentSummaryIndex`, `VectorStoreIndex`, `TreeIndex`, `KeywordTableIndex`, `KnowledgeGraphIndex`, and `ComposableGraph`, each with its own strengths and trade-offs. All index types share common features like building the index, inserting new nodes, and querying the index. A `SummaryIndex` example is provided, illustrating its creation and function as a simple list-based data structure that organizes nodes in order.

==== Are we there yet?

The text discusses how to retrieve answers from an index using retrievers and response synthesizers. It uses a Lionel Messi index as an example, querying "What is Messi's hometown?" The summary index retrieves all nodes to synthesize a response with full context.

==== How does this actually work under the hood?

The `QueryEngine` in LlamaIndex retrieves relevant Nodes from an index using a retriever, which fetches and ranks them. A node postprocessor then transforms, re-ranks, or filters these Nodes. Finally, a response synthesizer formulates an LLM prompt with the query and Node context, generates a response, and post-processes it into a natural language answer. The `index.as_query_engine()` creates a complete query engine with default components. The overall process involves loading data, parsing it into Nodes, building an index, querying the index, and synthesizing a response. Different index types like `SummaryIndex`, `TreeIndex`, and `KeywordIndex` impact performance and use cases, and the index structure defines the data management logic.

=== Starting our PITS project – hands-on exercise


====
++++
<a href="PITS_APP/global_settings.py" target="_blank">
PITS_APP/global_settings.py</a>
++++

---
++++
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/PITS_APP/session_functions.py" target="_blank">
PITS_APP/session_functions.py</a>
++++

---
++++
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/PITS_APP/logging_functions.py" target="_blank">
PITS_APP/logging_functions.py</a>
++++
====

The chapter introduces the hands-on development of the PITS project, emphasizing a modular code structure for clarity and ease of understanding. The project is built using Python and integrates with LlamaIndex, with a focus on creating a learning application. The author provides a disclaimer that the current implementation lacks certain features, such as authentication and error handling, which can be improved upon later.

A detailed overview of the Python source code files is provided, including their functions:

- **app.py**: Main entry point for the Streamlit app.
- **document_uploader.py**: Manages document ingestion and indexing.
- **training_material_builder.py**: Creates learning materials based on user knowledge.
- **training_interface.py**: Displays teaching content and facilitates user interaction.
- **quiz_builder.py**: Generates quizzes based on user knowledge.
- **quiz_interface.py**: Administers quizzes and evaluates user performance.
- **conversation_engine.py**: Manages user interactions and maintains conversational context.
- **storage_manager.py**: Handles file operations for session states and user uploads.
- **session_functions.py**: Manages session state saving, loading, and deletion.
- **logging_functions.py**: Records user interactions and application events.
- **global_settings.py**: Contains application configurations and settings.
- **user_onboarding.py**: Manages user onboarding processes.
- **index_builder.py**: Builds indexes for the application.

The chapter also highlights the importance of the YAML package for session management and provides installation instructions. It delves into the `global_settings.py`, `session_functions.py`, and `logging_functions.py` modules, explaining their roles in managing configurations, session states, and logging user actions, respectively. The author emphasizes the necessity of logging for debugging and monitoring the application. The chapter concludes with a promise of further coding in subsequent chapters.

== Chapter 4: Ingesting Data into Our RAG Workflow

=== Ingesting data via LlamaHub

This section emphasizes the importance of data ingestion and processing in a RAG workflow, highlighting common challenges and potential solutions.

**Key Challenges:**

1.  **Data Quality:** The quality of the RAG output depends on the quality of the input data. Cleaning, deduplicating, and removing redundant, ambiguous, biased, incomplete, or outdated information is crucial.
2.  **Data Dynamics:** Knowledge repositories evolve, requiring a system for regularly updating content to incorporate new information and remove outdated data.
3.  **Data Variety:** Data comes in various formats, and a RAG system should handle them all. While LlamaIndex offers many data loaders, automated ingestion can be challenging. LlamaParse is introduced as a solution for automated data ingestion and processing.

The section then transitions to discussing data ingestion using LlamaHub data loaders.

=== An overview of LlamaHub

LlamaHub is a library of integrations, including over 180 data connectors (also known as data readers or data loaders), that allow seamless integration of external data with LlamaIndex. These connectors extract data from various sources like databases, APIs, files, and websites, converting it into LlamaIndex `Document` objects, saving you from writing custom parsers. LlamaIndex's modular architecture means these integrations aren't included in the core installation, requiring separate installation of the corresponding package. These readers may also utilize specialized libraries and tools tailored to each data type. The LlamaHub website lists all available readers with documentation and samples. The source code for the readers can be found in the `llama-index-integrations/readers` subfolder of the Llama-index GitHub repository. Before using a data reader, make sure to install any additional dependencies required by the specific connector.

=== Using the LlamaHub data loaders to ingest content

==== Ingesting data from a web page

====
++++
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/ch4/sample_reader_SimpleWebPageReader.py" target="_blank">
ch4/sample_reader_SimpleWebPageReader.py</a>
++++
====

The `SimpleWebPageReader` in LlamaIndex extracts text content from web pages. It requires the `llama-index-readers-web` package to be installed. The reader fetches content from URLs, converts HTML to plain text (if specified and if the `html2text` package is installed), and attaches metadata using a custom function if provided. The content, URL, and metadata are then encapsulated in a `Document` object. While effective for simple web pages, it may not be suitable for complex, interactive websites. It simplifies the process of ingesting and structuring basic web content, allowing developers to focus on building RAG applications.

==== Ingesting data from a database

====
++++
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/ch4/sample_reader_DatabaseReader.py" target="_blank">
ch4/sample_reader_DatabaseReader.py</a>
++++
====

This text discusses using databases for efficient data management and introduces the `DatabaseReader` connector in LlamaIndex for querying various database systems. It explains how to install the connector, connect to a database (using a URI, SQLAlchemy Engine, or credentials), execute a SQL query, and convert the results into LlamaIndex Document objects. The text provides an example using an SQLite database and points to the official documentation for a more general example. It also highlights the ease of use of LlamaHub readers, mentioning the wide variety of supported data formats and hinting at more efficient methods for ingesting multiple documents in the next section.

==== Bulk-ingesting data from sources with multiple file formats

====
++++
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/ch4/sample_reader_SimpleDirectoryReader.py" target="_blank">
ch4/sample_reader_SimpleDirectoryReader.py</a>
++++
====

This document discusses two methods for loading data into LlamaIndex for use in Retrieval-Augmented Generation (RAG) systems.

1.  **SimpleDirectoryReader**: This is a simple and easy-to-use reader that can ingest multiple data formats (PDFs, Word docs, text files, CSVs) from a directory or a list of files. It automatically detects the file type and uses the appropriate reader to extract the content.
2.  **LlamaParse**: This is a more advanced parsing service that is part of the LlamaCloud enterprise platform. It is designed for complex file formats and uses multi-modal capabilities and LLM intelligence to provide high-quality document parsing. It allows users to provide natural language instructions to guide the parsing process and offers a JSON output mode for structured data. It can be used in combination with `SimpleDirectoryReader` for bulk ingestion. It supports a wide range of file types and offers a free tier. It is a paid service, so users should review the privacy policy before submitting proprietary data.

=== Parsing the documents into nodes

==== Understanding the simple text splitters

====
++++
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/ch4/sample_splitter_TokenTextSplitter.py" target="_blank">
ch4/sample_splitter_TokenTextSplitter.py</a>
++++

---
++++
<a href="https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/node_parser/text/token.py" target="_blank">
llama-index-core/llama_index/core/node_parser/text/token.py</a>
++++

---
++++
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/ch4/sample_splitter_CodeSplitter.py" target="_blank">
ch4/sample_splitter_CodeSplitter.py</a>
++++

---
++++
<a href="https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/node_parser/text/code.py" target="_blank">
llama-index-core/llama_index/core/node_parser/text/code.py</a>
++++

====

This text discusses text splitters in LlamaIndex, which break down documents into smaller pieces at the raw text level. It provides code examples and explanations for three specific text splitters:

1.  **SentenceSplitter:** Splits text while maintaining sentence boundaries, creating nodes containing groups of sentences.
2.  **TokenTextSplitter:** Splits text at the token level, respecting sentence boundaries. Key parameters include `chunk_size` (max tokens per chunk), `chunk_overlap` (token overlap between chunks), `separator` (primary token boundary), and `backup_separators` (additional splitting points).
3.  **CodeSplitter:** Designed for source code, splitting based on programming language using an abstract syntax tree (AST) to keep related statements together. Requires installing `tree_sitter` and `tree_sitter_languages`. Key parameters include `language` (programming language), `chunk_lines` (lines per chunk), `chunk_lines_overlap` (line overlap), and `max_chars` (max characters per chunk).


==== Using more advanced node parsers

====
++++
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/ch4/sample_parser_SentenceWindowNodeParser.py" target="_blank">
ch4/sample_parser_SentenceWindowNodeParser.py</a>
++++

---
++++
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/ch4/sample_parser_LangchainNodeParser.py" target="_blank">
ch4/sample_parser_LangchainNodeParser.py</a>
++++

---
++++
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/ch4/sample_parser_SimpleFileNodeParser.py" target="_blank">
ch4/sample_parser_SimpleFileNodeParser.py</a>
++++

---
++++
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/ch4/sample_parser_HTMLNodeParser.py" target="_blank">
ch4/sample_parser_HTMLNodeParser.py</a>
++++

---
++++
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/ch4/sample_parser_MarkdownNodeParser.py" target="_blank">
ch4/sample_parser_MarkdownNodeParser.py</a>
++++

---
++++
<a href="https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex/blob/main/ch4/sample_parser_JSONNodeParser.py" target="_blank">
ch4/sample_parser_JSONNodeParser.py</a>
++++
====

This text discusses advanced tools in LlamaIndex for chunking text into nodes, focusing on `NodeParser` and its derived classes. Key aspects include:

*   **NodeParser Basics:** All node parsers inherit from the `NodeParser` class, which allows customization of `include_metadata`, `Include_prev_next_rel`, and `Callback_manager`.
*   **SentenceWindowNodeParser:** Splits text into sentences and includes a window of surrounding sentences in the metadata.
*   **LangchainNodeParser:** Integrates Langchain text splitters into LlamaIndex.
*   **SimpleFileNodeParser:** Automatically selects a node parser based on the file type.
*   **HTMLNodeParser:** Parses HTML files using Beautiful Soup, converting them into nodes based on HTML tags.
*   **MarkdownNodeParser:** Processes markdown text, creating nodes for each header and incorporating the header hierarchy into the metadata.
*   **JSONNodeParser:** Processes structured data in JSON format.
