= Learning LangChain: Building AI and LLM Applications with LangChain and LangGraph
:source-highlighter: coderay
:icons: font
:toc: left
:toclevels: 4
Mayo Oshin

== Chapter 1. LLM Fundamentals with LangChain

=== Getting Set Up with LangChain

The chapter recommends setting up LangChain on your computer to follow along with the examples. First, create an OpenAI account and generate an API key from the OpenAI website. LangChain supports both Python and JavaScript, and the book provides equivalent code examples in both languages.

For Python users:

- Ensure Python is installed.
- Optionally install Jupyter Notebook (`pip install notebook`).
- Install LangChain and related libraries via pip.
- Set the OpenAI API key in your terminal environment.
- Launch Jupyter Notebook to run the examples.

For JavaScript users:

- Set the OpenAI API key in your terminal environment.
- Install Node.js if needed.
- Install LangChain and related packages via npm.
- Save code examples as `.js` files and run them using Node.js.

This setup ensures you can run the provided LangChain code examples smoothly throughout the book.

=== Using LLMs in LangChain

====
++++
<a href="https://github.com/langchain-ai/learning-langchain/blob/master/ch1/py/a-llm.py" target="_blank">
ch1/py/a-llm.py</a>
++++

---
++++
<a href="https://github.com/langchain-ai/learning-langchain/blob/master/ch1/py/b-chat.py" target="_blank">
ch1/py/b-chat.py</a>
++++

---
++++
<a href="https://github.com/langchain-ai/learning-langchain/blob/master/ch1/py/c-system.py" target="_blank">
ch1/py/c-system.py</a>
++++
====

This document details how to interact with Large Language Models (LLMs) using the LangChain framework, focusing on two primary interfaces: **LLMs** and **Chat Models**.

**LLM Interface:** This interface takes a simple string prompt, sends it to the LLM provider (like OpenAI), and returns the model's prediction.  Key parameters for configuration include `model` (specifying the underlying LLM), `temperature` (controlling output randomness/creativity), and `max_tokens` (limiting output length/cost).

**Chat Model Interface:** This interface is designed for conversational interactions. It differentiates messages by `role`:

* **System:** Instructions for the model.
* **User:** The user's input/query.
* **Assistant:** The model's response.

LangChain provides message classes like `HumanMessage`, `AIMessage`, `SystemMessage`, and `ChatMessage` to manage these roles. Using `SystemMessage` allows pre-configuring the AI's behavior, ensuring more predictable responses.

**Key Takeaways:**

* **LangChain simplifies LLM interaction.** It provides consistent interfaces regardless of the underlying provider.
* **Choosing between LLM and Chat Model depends on the application.**  Simple prompts use the LLM interface, while conversations benefit from the Chat Model interface.
* **Parameters like `temperature` and `max_tokens` are crucial for controlling LLM behavior and cost.**
* **`SystemMessage` is powerful for guiding the model's responses.**



The document includes code examples in both Python and JavaScript demonstrating how to use both interfaces with OpenAI's `gpt-3.5-turbo` model.

=== Making LLM Prompts Reusable

The content explains how prompt instructions shape the output of language models by providing context and guiding responses. It demonstrates creating detailed prompts with dynamic inputs (like context and question) using LangChain’s `PromptTemplate` and `ChatPromptTemplate` interfaces in both Python and JavaScript. These templates allow placeholders (e.g., `{context}`, `{question}`) to be filled at runtime, enabling flexible prompt construction.

Examples show:

- Defining a prompt template with placeholders.
- Invoking the template with specific context and question values.
- Passing the formatted prompt to an LLM (OpenAI) to generate answers.
- Using `ChatPromptTemplate` for chat-based interactions with role-based messages (`system`, `human`).

The key takeaway is that LangChain simplifies building reusable, dynamic prompts that can be integrated with LLMs for question answering or chat applications, ensuring prompts are structured and adaptable to user inputs.

== Chapter 2. RAG Part I: Indexing Your Data

=== Splitting Your Text into Chunks

The content explains how LangChain's `RecursiveCharacterTextSplitter` helps split large texts into semantically meaningful chunks by recursively splitting text using a prioritized list of separators (paragraphs, lines, words) to respect chunk size limits. It outputs chunks as `Document` objects with metadata and position info.

Key points:

- Default separators: paragraphs (`\n\n`), lines (`\n`), words (space).
- Splitting starts with largest separator and moves to smaller ones if chunks exceed size.
- Supports chunk size and overlap to maintain context.
- Can split raw text strings or documents loaded from files.
- Specialized splitting for code and Markdown using language-specific separators to keep semantic units (e.g., function bodies) intact.
- LangChain provides built-in separators for many languages (Python, JS, Markdown, HTML).
- `from_language` method creates splitter instances tailored to specific languages.
- `create_documents` method splits raw text strings into documents, optionally attaching metadata per chunk.
- Metadata is preserved and attached to each chunk, useful for tracking source or provenance.

Examples show usage in Python and JavaScript for plain text, Python code, and Markdown, demonstrating how chunks align with natural text/code boundaries and how metadata is propagated.

=== Generating Text Embeddings

The content explains how LangChain's `Embeddings` class interfaces with text embedding models (like OpenAI, Cohere, Hugging Face) to convert text into vector representations. It provides two methods: one for embedding multiple documents (list of strings) and one for embedding a single query string. Examples in Python and JavaScript demonstrate embedding multiple documents at once for efficiency, returning lists of numeric vectors.

An end-to-end example shows how to:

1. Load documents using document loaders (e.g., `TextLoader`).
2. Split large documents into smaller chunks with text splitters (e.g., `RecursiveCharacterTextSplitter`).
3. Generate embeddings for each chunk using an embeddings model (e.g., `OpenAIEmbeddings`).

The example code is provided in both Python and JavaScript. After generating embeddings, the next step is to store them in a vector store database for further use.

=== Storing Embeddings in a Vector Store

The chapter explains vector stores—databases optimized for storing vectors and performing similarity calculations like cosine similarity, especially for unstructured data such as text and images. Unlike traditional structured-data databases, vector stores support CRUD and search operations on vector embeddings, enabling AI-powered applications like querying large documents.

There are many vector store providers, each with different features (multitenancy, filtering, performance, cost, scalability). However, vector stores are relatively new, can be complex to manage, and add system complexity.

To address this, PostgreSQL supports vector storage via the `pgvector` extension, allowing users to manage both traditional relational data and vector embeddings in one familiar database.

The setup involves running a Docker container with Postgres + pgvector, then connecting via a connection string.

Examples in Python and JavaScript show how to:

- Load and split documents into chunks
- Generate embeddings using OpenAIEmbeddings (or other models)
- Store embeddings and documents in PGVector (Postgres)
- Perform similarity searches to retrieve relevant documents
- Add new documents with metadata and UUIDs
- Delete documents by ID

The process includes embedding queries, searching for nearest vectors in Postgres, and returning documents sorted by similarity.

This integration simplifies vector search by leveraging a popular relational database, reducing the need for separate vector store infrastructure while enabling scalable AI applications.

=== Tracking Changes to Your Documents

The content explains how LangChain's indexing API helps manage vector stores with frequently changing data by avoiding costly re-indexing and duplicate embeddings. It uses a `RecordManager` class to track documents via hashes, write times, and source IDs. The API supports cleanup modes to control deletion of outdated documents:

- `None`: no automatic cleanup.
- `Incremental`: deletes previous versions if content changes.
- `Full`: deletes previous versions and any documents not in the current batch.

Examples in Python and JavaScript demonstrate setting up a Postgres-backed vector store and record manager, creating documents, and indexing them with incremental cleanup to prevent duplicates. When documents are modified, the API replaces old versions sharing the same source ID. This approach keeps the vector store synchronized efficiently by only updating changed documents.

=== Indexing Optimization

==== MultiVectorRetriever

The document explains a method to handle documents containing both text and tables for retrieval-augmented generation (RAG) without losing table data. Instead of embedding raw text chunks (which can omit tables), it proposes a two-level indexing approach:

1. **Summarize table elements** using an LLM, generating summaries that include an ID referencing the full raw table.
2. **Store summaries in a vector store** for efficient similarity search.
3. **Store full raw tables separately** in a document store (docstore) keyed by the summary IDs.
4. When a query retrieves a summary, **fetch the full referenced raw table** from the docstore and pass it as context to the LLM for answer synthesis.

This decoupling allows retrieval of concise summaries for fast search while preserving access to complete table data for accurate answers.

The document provides detailed Python and JavaScript code examples demonstrating:

- Loading and splitting documents into chunks.
- Using an LLM to generate summaries of chunks.
- Indexing summaries in a vector store (Postgres PGVector).
- Storing original chunks in an in-memory docstore.
- Using a `MultiVectorRetriever` to first retrieve summaries by similarity, then fetch full original chunks by ID.
- Querying the retriever to get relevant full context documents for downstream LLM prompting.

This approach ensures that tables and other complex document structures are not lost during chunking and embedding, enabling richer and more accurate retrieval and answer synthesis.

==== ColBERT Optimizing Embeddings

The text discusses a challenge with embedding models that compress documents into fixed-length vectors, which can include irrelevant or redundant content and cause hallucinations in LLM outputs. A more granular approach involves generating contextual embeddings for each token in both the query and document, scoring token-level similarities, and summing maximum similarity scores to rank documents. The ColBERT embedding model implements this solution effectively.

An example Python workflow using the RAGatouille library demonstrates how to:

- Retrieve full Wikipedia page text via API,
- Index the document with ColBERT embeddings,
- Perform similarity search queries,
- And integrate with LangChain retrievers for improved document retrieval.

Using ColBERT in this way enhances the relevance of documents retrieved as context for large language models, reducing hallucinations and improving output quality.

== Chapter 3. RAG Part II: Chatting with Your Data

=== Introducing Retrieval-Augmented Generation

The text explains Retrieval-Augmented Generation (RAG), a technique that improves the accuracy of large language models (LLMs) by providing them with up-to-date external context. Without RAG, LLMs rely solely on pretrained data, which can be outdated, leading to incorrect answers—as illustrated by ChatGPT incorrectly naming France as the latest FIFA World Cup winner instead of Argentina (2022). By appending relevant, current information (e.g., from Wikipedia) as context to the prompt, the LLM can generate accurate responses. However, manually adding such context is impractical for real-world applications, highlighting the need for automated systems that retrieve and supply relevant information dynamically to LLMs during generation.

==== Retrieving Relevant Documents

The content explains the three core stages of a Retrieval-Augmented Generation (RAG) system for AI applications:

1. **Indexing**: Preprocess external data by loading documents, splitting them into chunks, embedding these chunks into vector representations, and storing them in a vector store for efficient retrieval. Code examples in Python and JavaScript demonstrate loading a text file, splitting it, embedding chunks using OpenAI embeddings, and storing them in a PostgreSQL-backed vector store (PGVector).

2. **Retrieval**: When a user query is received, it is converted into embeddings and compared against stored embeddings using similarity metrics (e.g., cosine similarity) to find the most relevant document chunks. The vector store provides an `as_retriever` method to abstract query embedding and similarity search. The number of documents retrieved can be controlled by a parameter `k` to balance relevance, performance, and prompt size. Code examples show how to retrieve relevant documents using this method.

3. **Generation**: The final stage involves combining the original user prompt with the retrieved relevant documents to form a comprehensive prompt that is sent to the language model for generating a prediction or answer.

Figures referenced illustrate the flow of indexing and retrieval processes, including similarity calculations using structures like Hierarchical Navigable Small World (HNSW) graphs.

Overall, the chapter emphasizes practical implementation of RAG with LangChain libraries, highlighting the importance of efficient indexing, controlled retrieval, and prompt synthesis for effective AI applications.

==== Generating LLM Predictions Using Relevant Documents

====
++++
<a href="https://github.com/langchain-ai/learning-langchain/blob/master/ch3/py/a-basic-rag.py" target="_blank">
ch3/py/a-basic-rag.py</a>
++++
====

The content explains how to build a Retrieval-Augmented Generation (RAG) system by integrating relevant documents retrieved from a vector store into a prompt for a large language model (LLM) to generate informed answers. It provides Python and JavaScript code examples demonstrating:

- Creating a dynamic prompt template with context and question variables.
- Using a retriever to fetch relevant documents based on a user query.
- Composing a chain that pipes the prompt output into the LLM.
- Invoking the chain with retrieved documents and the user question to generate a final answer.

Further, it shows how to encapsulate this retrieval and generation logic into a reusable function or runnable (`qa`) that takes a question, fetches documents, formats the prompt, and returns the answer—optionally including the retrieved documents for inspection.

The text highlights that while this basic RAG system works for personal use, production-ready systems require addressing challenges such as handling variable user input quality, routing queries across multiple data sources, translating natural language to query languages, and optimizing indexing (embedding and text splitting).

Finally, it previews upcoming discussion on research-backed strategies to optimize RAG system accuracy, summarized in an accompanying figure.

=== Query Transformation

==== Rewrite-Retrieve-Read

The Rewrite-Retrieve-Read (RRR) strategy improves question answering by first prompting a large language model (LLM) to rewrite a user's poorly phrased or distracted query into a clearer, more focused search query. This rewritten query is then used to retrieve relevant documents, which are subsequently passed along with the original question to the LLM to generate an answer.

An example shows that a distracted user query containing irrelevant details leads to a failure in retrieving useful information and thus no answer. By contrast, applying the RRR approach—where the LLM rewrites the query before retrieval—results in fetching relevant documents and producing a meaningful answer.

This method can be implemented in various programming languages (Python, JavaScript) and works with any retrieval system, including vector stores or web search tools. The main trade-off is increased latency due to the extra LLM call required for rewriting the query.


==== Multi-Query Retrieval

The multi-query retrieval strategy enhances information retrieval by generating multiple related queries from a user's initial question using an LLM. These queries are run in parallel against a data source, and the retrieved documents are combined and deduplicated to form a comprehensive context. This approach helps overcome limitations of single-query retrieval, especially when answers require multiple perspectives.

Key points include:

- Using a prompt template to generate several query variations from the original question.
- Running all queries in parallel with a `.batch` method to retrieve relevant documents.
- Deduplicating documents by their content to avoid repetition.
- Combining the retrieved documents as context for a final prompt to the LLM to generate a comprehensive answer.
- Encapsulating the multi-query retrieval logic in a standalone chain (`retrieval_chain`) for modularity and ease of integration.

Code examples in Python and JavaScript illustrate:

- Generating multiple queries.
- Parallel retrieval and deduplication of documents.
- Constructing a prompt with combined context.
- Producing the final answer using the LLM.

This strategy is particularly useful for complex questions requiring diverse information sources and can be integrated seamlessly into existing QA workflows.

==== RAG-Fusion

The RAG-Fusion strategy enhances multi-query retrieval by adding a final reranking step using the Reciprocal Rank Fusion (RRF) algorithm. RRF combines ranked document lists from multiple queries into a single unified ranking by scoring documents based on their positions across all lists, effectively promoting the most relevant documents to the top. This approach handles differences in score scales across queries and allows lower-ranked documents to influence the final ranking through a tunable parameter k.

The process involves:

1. Generating multiple search queries from a single user query using a language model prompt.
2. Retrieving relevant documents for each query.
3. Applying the RRF algorithm to fuse and rerank these documents into one consolidated list.
4. Using the reranked documents as context to answer the original question with a language model.

Code examples in Python and JavaScript demonstrate how to implement query generation, document retrieval, RRF reranking, and final question answering in a pipeline. RAG-Fusion improves retrieval by capturing diverse user intents, handling complex queries, and broadening document coverage to enable serendipitous discovery.
