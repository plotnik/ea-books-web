= Learning LangChain: Building AI and LLM Applications with LangChain and LangGraph
:source-highlighter: coderay
:icons: font
:toc: left
:toclevels: 4
Mayo Oshin

== Chapter 1. LLM Fundamentals with LangChain

=== Getting Set Up with LangChain

The chapter recommends setting up LangChain on your computer to follow along with the examples. First, create an OpenAI account and generate an API key from the OpenAI website. LangChain supports both Python and JavaScript, and the book provides equivalent code examples in both languages.

For Python users:

- Ensure Python is installed.
- Optionally install Jupyter Notebook (`pip install notebook`).
- Install LangChain and related libraries via pip.
- Set the OpenAI API key in your terminal environment.
- Launch Jupyter Notebook to run the examples.

For JavaScript users:

- Set the OpenAI API key in your terminal environment.
- Install Node.js if needed.
- Install LangChain and related packages via npm.
- Save code examples as `.js` files and run them using Node.js.

This setup ensures you can run the provided LangChain code examples smoothly throughout the book.

=== Using LLMs in LangChain

====
++++
<a href="https://github.com/langchain-ai/learning-langchain/blob/master/ch1/py/a-llm.py" target="_blank">
ch1/py/a-llm.py</a>
++++

---
++++
<a href="https://github.com/langchain-ai/learning-langchain/blob/master/ch1/py/b-chat.py" target="_blank">
ch1/py/b-chat.py</a>
++++

---
++++
<a href="https://github.com/langchain-ai/learning-langchain/blob/master/ch1/py/c-system.py" target="_blank">
ch1/py/c-system.py</a>
++++
====

This document details how to interact with Large Language Models (LLMs) using the LangChain framework, focusing on two primary interfaces: **LLMs** and **Chat Models**.

**LLM Interface:** This interface takes a simple string prompt, sends it to the LLM provider (like OpenAI), and returns the model's prediction.  Key parameters for configuration include `model` (specifying the underlying LLM), `temperature` (controlling output randomness/creativity), and `max_tokens` (limiting output length/cost).

**Chat Model Interface:** This interface is designed for conversational interactions. It differentiates messages by `role`:

* **System:** Instructions for the model.
* **User:** The user's input/query.
* **Assistant:** The model's response.

LangChain provides message classes like `HumanMessage`, `AIMessage`, `SystemMessage`, and `ChatMessage` to manage these roles. Using `SystemMessage` allows pre-configuring the AI's behavior, ensuring more predictable responses.

**Key Takeaways:**

* **LangChain simplifies LLM interaction.** It provides consistent interfaces regardless of the underlying provider.
* **Choosing between LLM and Chat Model depends on the application.**  Simple prompts use the LLM interface, while conversations benefit from the Chat Model interface.
* **Parameters like `temperature` and `max_tokens` are crucial for controlling LLM behavior and cost.**
* **`SystemMessage` is powerful for guiding the model's responses.**



The document includes code examples in both Python and JavaScript demonstrating how to use both interfaces with OpenAI's `gpt-3.5-turbo` model.

=== Making LLM Prompts Reusable

The content explains how prompt instructions shape the output of language models by providing context and guiding responses. It demonstrates creating detailed prompts with dynamic inputs (like context and question) using LangChain’s `PromptTemplate` and `ChatPromptTemplate` interfaces in both Python and JavaScript. These templates allow placeholders (e.g., `{context}`, `{question}`) to be filled at runtime, enabling flexible prompt construction.

Examples show:

- Defining a prompt template with placeholders.
- Invoking the template with specific context and question values.
- Passing the formatted prompt to an LLM (OpenAI) to generate answers.
- Using `ChatPromptTemplate` for chat-based interactions with role-based messages (`system`, `human`).

The key takeaway is that LangChain simplifies building reusable, dynamic prompts that can be integrated with LLMs for question answering or chat applications, ensuring prompts are structured and adaptable to user inputs.

== Chapter 2. RAG Part I: Indexing Your Data

=== Splitting Your Text into Chunks

The content explains how LangChain's `RecursiveCharacterTextSplitter` helps split large texts into semantically meaningful chunks by recursively splitting text using a prioritized list of separators (paragraphs, lines, words) to respect chunk size limits. It outputs chunks as `Document` objects with metadata and position info.

Key points:

- Default separators: paragraphs (`\n\n`), lines (`\n`), words (space).
- Splitting starts with largest separator and moves to smaller ones if chunks exceed size.
- Supports chunk size and overlap to maintain context.
- Can split raw text strings or documents loaded from files.
- Specialized splitting for code and Markdown using language-specific separators to keep semantic units (e.g., function bodies) intact.
- LangChain provides built-in separators for many languages (Python, JS, Markdown, HTML).
- `from_language` method creates splitter instances tailored to specific languages.
- `create_documents` method splits raw text strings into documents, optionally attaching metadata per chunk.
- Metadata is preserved and attached to each chunk, useful for tracking source or provenance.

Examples show usage in Python and JavaScript for plain text, Python code, and Markdown, demonstrating how chunks align with natural text/code boundaries and how metadata is propagated.

=== Generating Text Embeddings

The content explains how LangChain's `Embeddings` class interfaces with text embedding models (like OpenAI, Cohere, Hugging Face) to convert text into vector representations. It provides two methods: one for embedding multiple documents (list of strings) and one for embedding a single query string. Examples in Python and JavaScript demonstrate embedding multiple documents at once for efficiency, returning lists of numeric vectors.

An end-to-end example shows how to:

1. Load documents using document loaders (e.g., `TextLoader`).
2. Split large documents into smaller chunks with text splitters (e.g., `RecursiveCharacterTextSplitter`).
3. Generate embeddings for each chunk using an embeddings model (e.g., `OpenAIEmbeddings`).

The example code is provided in both Python and JavaScript. After generating embeddings, the next step is to store them in a vector store database for further use.

=== Storing Embeddings in a Vector Store

The chapter explains vector stores—databases optimized for storing vectors and performing similarity calculations like cosine similarity, especially for unstructured data such as text and images. Unlike traditional structured-data databases, vector stores support CRUD and search operations on vector embeddings, enabling AI-powered applications like querying large documents.

There are many vector store providers, each with different features (multitenancy, filtering, performance, cost, scalability). However, vector stores are relatively new, can be complex to manage, and add system complexity.

To address this, PostgreSQL supports vector storage via the `pgvector` extension, allowing users to manage both traditional relational data and vector embeddings in one familiar database.

The setup involves running a Docker container with Postgres + pgvector, then connecting via a connection string.

Examples in Python and JavaScript show how to:

- Load and split documents into chunks
- Generate embeddings using OpenAIEmbeddings (or other models)
- Store embeddings and documents in PGVector (Postgres)
- Perform similarity searches to retrieve relevant documents
- Add new documents with metadata and UUIDs
- Delete documents by ID

The process includes embedding queries, searching for nearest vectors in Postgres, and returning documents sorted by similarity.

This integration simplifies vector search by leveraging a popular relational database, reducing the need for separate vector store infrastructure while enabling scalable AI applications.

=== Tracking Changes to Your Documents

The content explains how LangChain's indexing API helps manage vector stores with frequently changing data by avoiding costly re-indexing and duplicate embeddings. It uses a `RecordManager` class to track documents via hashes, write times, and source IDs. The API supports cleanup modes to control deletion of outdated documents:

- `None`: no automatic cleanup.
- `Incremental`: deletes previous versions if content changes.
- `Full`: deletes previous versions and any documents not in the current batch.

Examples in Python and JavaScript demonstrate setting up a Postgres-backed vector store and record manager, creating documents, and indexing them with incremental cleanup to prevent duplicates. When documents are modified, the API replaces old versions sharing the same source ID. This approach keeps the vector store synchronized efficiently by only updating changed documents.

=== Indexing Optimization

==== MultiVectorRetriever

The document explains a method to handle documents containing both text and tables for retrieval-augmented generation (RAG) without losing table data. Instead of embedding raw text chunks (which can omit tables), it proposes a two-level indexing approach:

1. **Summarize table elements** using an LLM, generating summaries that include an ID referencing the full raw table.
2. **Store summaries in a vector store** for efficient similarity search.
3. **Store full raw tables separately** in a document store (docstore) keyed by the summary IDs.
4. When a query retrieves a summary, **fetch the full referenced raw table** from the docstore and pass it as context to the LLM for answer synthesis.

This decoupling allows retrieval of concise summaries for fast search while preserving access to complete table data for accurate answers.

The document provides detailed Python and JavaScript code examples demonstrating:

- Loading and splitting documents into chunks.
- Using an LLM to generate summaries of chunks.
- Indexing summaries in a vector store (Postgres PGVector).
- Storing original chunks in an in-memory docstore.
- Using a `MultiVectorRetriever` to first retrieve summaries by similarity, then fetch full original chunks by ID.
- Querying the retriever to get relevant full context documents for downstream LLM prompting.

This approach ensures that tables and other complex document structures are not lost during chunking and embedding, enabling richer and more accurate retrieval and answer synthesis.

==== ColBERT Optimizing Embeddings

The text discusses a challenge with embedding models that compress documents into fixed-length vectors, which can include irrelevant or redundant content and cause hallucinations in LLM outputs. A more granular approach involves generating contextual embeddings for each token in both the query and document, scoring token-level similarities, and summing maximum similarity scores to rank documents. The ColBERT embedding model implements this solution effectively.

An example Python workflow using the RAGatouille library demonstrates how to:

- Retrieve full Wikipedia page text via API,
- Index the document with ColBERT embeddings,
- Perform similarity search queries,
- And integrate with LangChain retrievers for improved document retrieval.

Using ColBERT in this way enhances the relevance of documents retrieved as context for large language models, reducing hallucinations and improving output quality.

