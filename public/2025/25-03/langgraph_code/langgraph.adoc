= Learning LangChain: Building AI and LLM Applications with LangChain and LangGraph
:source-highlighter: coderay
:icons: font
:toc: left
:toclevels: 4
Mayo Oshin

== Chapter 1. LLM Fundamentals with LangChain

=== Getting Set Up with LangChain

The chapter recommends setting up LangChain on your computer to follow along with the examples. First, create an OpenAI account and generate an API key from the OpenAI website. LangChain supports both Python and JavaScript, and the book provides equivalent code examples in both languages.

For Python users:

- Ensure Python is installed.
- Optionally install Jupyter Notebook (`pip install notebook`).
- Install LangChain and related libraries via pip.
- Set the OpenAI API key in your terminal environment.
- Launch Jupyter Notebook to run the examples.

For JavaScript users:

- Set the OpenAI API key in your terminal environment.
- Install Node.js if needed.
- Install LangChain and related packages via npm.
- Save code examples as `.js` files and run them using Node.js.

This setup ensures you can run the provided LangChain code examples smoothly throughout the book.

=== Using LLMs in LangChain

====
++++
<a href="https://github.com/langchain-ai/learning-langchain/blob/master/ch1/py/a-llm.py" target="_blank">
ch1/py/a-llm.py</a>
++++

---
++++
<a href="https://github.com/langchain-ai/learning-langchain/blob/master/ch1/py/b-chat.py" target="_blank">
ch1/py/b-chat.py</a>
++++

---
++++
<a href="https://github.com/langchain-ai/learning-langchain/blob/master/ch1/py/c-system.py" target="_blank">
ch1/py/c-system.py</a>
++++
====

This document details how to interact with Large Language Models (LLMs) using the LangChain framework, focusing on two primary interfaces: **LLMs** and **Chat Models**.

**LLM Interface:** This interface takes a simple string prompt, sends it to the LLM provider (like OpenAI), and returns the model's prediction.  Key parameters for configuration include `model` (specifying the underlying LLM), `temperature` (controlling output randomness/creativity), and `max_tokens` (limiting output length/cost).

**Chat Model Interface:** This interface is designed for conversational interactions. It differentiates messages by `role`:

* **System:** Instructions for the model.
* **User:** The user's input/query.
* **Assistant:** The model's response.

LangChain provides message classes like `HumanMessage`, `AIMessage`, `SystemMessage`, and `ChatMessage` to manage these roles. Using `SystemMessage` allows pre-configuring the AI's behavior, ensuring more predictable responses.

**Key Takeaways:**

* **LangChain simplifies LLM interaction.** It provides consistent interfaces regardless of the underlying provider.
* **Choosing between LLM and Chat Model depends on the application.**  Simple prompts use the LLM interface, while conversations benefit from the Chat Model interface.
* **Parameters like `temperature` and `max_tokens` are crucial for controlling LLM behavior and cost.**
* **`SystemMessage` is powerful for guiding the model's responses.**



The document includes code examples in both Python and JavaScript demonstrating how to use both interfaces with OpenAI's `gpt-3.5-turbo` model.

=== Making LLM Prompts Reusable

The content explains how prompt instructions shape the output of language models by providing context and guiding responses. It demonstrates creating detailed prompts with dynamic inputs (like context and question) using LangChainâ€™s `PromptTemplate` and `ChatPromptTemplate` interfaces in both Python and JavaScript. These templates allow placeholders (e.g., `{context}`, `{question}`) to be filled at runtime, enabling flexible prompt construction.

Examples show:

- Defining a prompt template with placeholders.
- Invoking the template with specific context and question values.
- Passing the formatted prompt to an LLM (OpenAI) to generate answers.
- Using `ChatPromptTemplate` for chat-based interactions with role-based messages (`system`, `human`).

The key takeaway is that LangChain simplifies building reusable, dynamic prompts that can be integrated with LLMs for question answering or chat applications, ensuring prompts are structured and adaptable to user inputs.

== Chapter 2. RAG Part I: Indexing Your Data

=== Splitting Your Text into Chunks

The content explains how LangChain's `RecursiveCharacterTextSplitter` helps split large texts into semantically meaningful chunks by recursively splitting text using a prioritized list of separators (paragraphs, lines, words) to respect chunk size limits. It outputs chunks as `Document` objects with metadata and position info.

Key points:

- Default separators: paragraphs (`\n\n`), lines (`\n`), words (space).
- Splitting starts with largest separator and moves to smaller ones if chunks exceed size.
- Supports chunk size and overlap to maintain context.
- Can split raw text strings or documents loaded from files.
- Specialized splitting for code and Markdown using language-specific separators to keep semantic units (e.g., function bodies) intact.
- LangChain provides built-in separators for many languages (Python, JS, Markdown, HTML).
- `from_language` method creates splitter instances tailored to specific languages.
- `create_documents` method splits raw text strings into documents, optionally attaching metadata per chunk.
- Metadata is preserved and attached to each chunk, useful for tracking source or provenance.

Examples show usage in Python and JavaScript for plain text, Python code, and Markdown, demonstrating how chunks align with natural text/code boundaries and how metadata is propagated.

=== Generating Text Embeddings

The content explains how LangChain's `Embeddings` class interfaces with text embedding models (like OpenAI, Cohere, Hugging Face) to convert text into vector representations. It provides two methods: one for embedding multiple documents (list of strings) and one for embedding a single query string. Examples in Python and JavaScript demonstrate embedding multiple documents at once for efficiency, returning lists of numeric vectors.

An end-to-end example shows how to:

1. Load documents using document loaders (e.g., `TextLoader`).
2. Split large documents into smaller chunks with text splitters (e.g., `RecursiveCharacterTextSplitter`).
3. Generate embeddings for each chunk using an embeddings model (e.g., `OpenAIEmbeddings`).

The example code is provided in both Python and JavaScript. After generating embeddings, the next step is to store them in a vector store database for further use.