= LangSmith Cheat Sheet ğŸš€
:source-highlighter: pygments
:icons: font
:toc: right
:toclevels: 4

====
LangSmith Docs::
https://docs.smith.langchain.com/

LangChain Cheat Sheet ğŸš€::
link:../../../2023/23-12/langchain_code/lc_cs.html[2023/23-12/langchain_code/lc_cs.html]
====

=== 1Â Â·Â QuickÂ Start

[source,bash]
----
#Â Install the Python SDK
pip install langsmith    # or: npm i @langchain/smith

#Â MinimumÂ environmentÂ vars
export LANGCHAIN_TRACING_V2="true"       # enable tracing
export LANGCHAIN_API_KEY="skâ€‘..."        # get from smith.langchain.com
export LANGCHAIN_PROJECT="myâ€‘project"    # logical bucket for traces
#Â Optional
export LANGCHAIN_ENDPOINT="https://api.smith.langchain.com"
export LANGCHAIN_HIDE_INPUTS="true"      # redact PII
----

'''''

=== 2Â Â·Â CoreÂ Concepts

[cols=",",options="header",]
|===
|Concept |WhatÂ itÂ is
|*Project* |Topâ€‘level bucket that groups runs/traces.

|*RunÂ /Â Span* |Single invocation (chain, LLM, tool) recorded with
metadata.

|*Dataset* |Collection of inputÂ â†’Â expected output pairs, used for eval &
fineâ€‘tuning.

|*Evaluation* |Programmatic or LLMâ€‘asâ€‘judge scoring of a chain/LLM on a
dataset.

|*Monitor* |Dashboard card that tracks latency, cost, quality, etc.,
over time.

|*PromptÂ Canvas* |GUI for prompt versioning, diffing, and collaboration.
|===

'''''

=== 3Â Â·Â TracingÂ &Â Observability

==== PythonÂ (oneâ€‘off)

[source,python]
----
from langchain.callbacks import tracing_v2_enabled

with tracing_v2_enabled(project_name="demo"):
Â Â Â Â result = chain.invoke({"input": "Hi"})
----

==== JS/TSÂ (global)

[source,ts]
----
import { traceable } from "@langchain/core/tracing";
await traceable.run(chain, { input: "Hello" }, { projectName: "demo" });
----

*Tips*

* Tag runs: `+chain.invoke(..., tags=["prod","user123"])+`
* Add custom metadata: `+run.extra = {"order_id": 42}+`.
* Log user feedback:
`+client.create_feedback(run_id, score=1, comment="ğŸ‘")+`.

'''''

=== 4Â Â·Â CLIÂ Reference Â (npmÂ iÂ -gÂ langsmith)

[source,bash]
----
langsmith login                         # OAuth or API key
langsmith projects list                 # show all projects
langsmith logs -p my-project            # tail live traces
langsmith dataset create -n eval1       # make dataset from runs
langsmith eval run -d eval1 --chain app.py --evals qa embed_distance faithfulness
----

'''''

=== 5Â Â·Â PythonÂ ClientÂ Snippets

[source,python]
----
from langsmith import Client
client = Client()

# Read a run
a_run = client.read_run("<run-id>")

# Create dataset + examples
set = client.create_dataset("orders-test", description="Edge cases")
client.create_example(
Â Â Â Â inputs={"query": "Where's my order?"},
Â Â Â Â outputs={"answer": "Your order ships tomorrow."},
Â Â Â Â dataset_id=set.id,
)

# Evaluate chain on dataset
client.run_on_dataset(
Â Â Â Â dataset_name="orders-test",
Â Â Â Â llm_or_chain_factory=lambda: chain,
Â Â Â Â evaluation="qa",
)
----

'''''

=== 6Â Â·Â JS/TSÂ Snippets

[source,ts]
----
import { Client } from "@langchain/smith";
const client = new Client();

const run = await client.readRun(runId);
----

'''''

=== 7Â Â·Â EvaluatorsÂ DeepÂ Dive

==== Builtâ€‘in evaluators

[cols=",,,",options="header",]
|===
|Evaluator |Metric |IdealÂ UseÂ Cases |KeyÂ Kwargs
|*qa* |LLMâ€‘asâ€‘judge (0â€“1) correctness vs reference |Q&A, summarization,
generation quality |`+model_name+`, `+threshold+`

|*string_distance* |Levenshtein / Rougeâ€‘L / BLEU |Deterministic text
comparison |`+distance+`

|*embed_distance* |Cosine similarity in embedding space |Semantic
similarity, retrieval |`+embedding_model+`

|*faithfulness* |Contextâ€‘grounded hallucination score |RAG answers,
citation compliance |`+context_key+`

|*json_diff* |Structural diff of JSON |Tool responses & structured data
|`+ignore_paths+`

|*toxicity* |PerspectiveÂ API score |Content safety & moderation
|`+threshold+`
|===

'''''

==== Running evaluators

[source,python]
----
client.run_on_dataset(
Â Â Â Â dataset_name="orders-test",
Â Â Â Â llm_or_chain_factory=lambda: chain,
Â Â Â Â evaluation=["qa", "embed_distance", "faithfulness"],
Â Â Â Â max_examples=100,
)
----

CLI equivalent:

[source,bash]
----
langsmith eval run -d orders-test --chain app.py --evals qa embed_distance faithfulness --max-examples 100
----

'''''

==== Custom evaluator skeleton

[source,python]
----
from langsmith.evaluation import StringEvaluator, EvalResult
import re

class RegexPassFail(StringEvaluator):
Â Â Â Â def __init__(self, pattern: str):
Â Â Â Â Â Â Â Â self.pattern = re.compile(pattern)

Â Â Â Â def _score_string(self, prediction: str, reference: str | None = None) -> EvalResult:
Â Â Â Â Â Â Â Â ok = bool(self.pattern.search(prediction))
Â Â Â Â Â Â Â Â return EvalResult(score=int(ok), comment="âœ…" if ok else "âŒ")

client.register_evaluator("regex_pass", RegexPassFail(r"order_\\d+"))
----

'''''

==== Interpreting results

* *score*: floatÂ 0â€“1; `+None+` means skipped or errored.
* *comment*: model or evaluator reasoningâ€”view in _Evals_ tab.
* Dashboards automatically aggregate p50, p90, passâ€‘rate.

'''''

==== Tips & tricks

* Start with string/embedding metrics; layer LLM judges for samples that
pass.
* Weight metrics to blend: `+--weights qa:0.7 faithfulness:0.3+`.
* Cache expensive LLM calls by setting
`+LANGCHAIN_CACHE="redis://host:6379"+`.
* Publish evaluators to workspace registry â†’ reuse via UI â€œAdd Metricâ€.

'''''

=== 8Â Â·Â MonitoringÂ Dashboards

* Go to *Monitor â†’ Charts* in any project.
* Windows: 1h â€¢ 24h â€¢ 7d.
* Charts: Token usage, latency p95, errorÂ rate, evaluation score.

'''''

=== 9Â Â·Â SecurityÂ &Â Selfâ€‘Hosting

[cols=",",options="header",]
|===
|Option |Purpose
|`+LANGCHAIN_REDACT="field1,field2"+` |Masks sensitive JSON keys.
|`+LANGCHAIN_PUBLIC="true"+` |Share a readâ€‘only project link.
|*Selfâ€‘host* |Deploy OSS stack via Dockerâ€‘compose ().
|===

'''''

=== 10Â Â·Â TroubleshootingÂ FAQ

[cols=",",options="header",]
|===
|Symptom |Fix
|No runs appear |Verify `+LANGCHAIN_TRACING_V2+` and project name.

|403Â errors |Check API key scopes & workspace membership.

|Duplicate traces |Remove legacy callback handlers or disable DEBUG
logs.
|===

'''''

=== 11Â Â·Â Resources

* Docs & QuickÂ Start â†’ _docs.smith.langchain.com_
* Evaluator library â†’ _docs.smith.langchain.com/evaluation_
* GitHub SDK â†’ _github.com/langchain-ai/langsmith_
* Community Help â†’ _discord.gg/langchain_

'''''

