<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Asciidoctor 2.0.23">
<meta name="author" content="Mayo Oshin">
<title>Learning LangChain: Building AI and LLM Applications with LangChain and LangGraph</title>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700">
<style>
/*! Asciidoctor default stylesheet | MIT License | https://asciidoctor.org */
/* Uncomment the following line when using as a custom stylesheet */
/* @import "https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700"; */
html{font-family:sans-serif;-webkit-text-size-adjust:100%}
a{background:none}
a:focus{outline:thin dotted}
a:active,a:hover{outline:0}
h1{font-size:2em;margin:.67em 0}
b,strong{font-weight:bold}
abbr{font-size:.9em}
abbr[title]{cursor:help;border-bottom:1px dotted #dddddf;text-decoration:none}
dfn{font-style:italic}
hr{height:0}
mark{background:#ff0;color:#000}
code,kbd,pre,samp{font-family:monospace;font-size:1em}
pre{white-space:pre-wrap}
q{quotes:"\201C" "\201D" "\2018" "\2019"}
small{font-size:80%}
sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}
sup{top:-.5em}
sub{bottom:-.25em}
img{border:0}
svg:not(:root){overflow:hidden}
figure{margin:0}
audio,video{display:inline-block}
audio:not([controls]){display:none;height:0}
fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}
legend{border:0;padding:0}
button,input,select,textarea{font-family:inherit;font-size:100%;margin:0}
button,input{line-height:normal}
button,select{text-transform:none}
button,html input[type=button],input[type=reset],input[type=submit]{-webkit-appearance:button;cursor:pointer}
button[disabled],html input[disabled]{cursor:default}
input[type=checkbox],input[type=radio]{padding:0}
button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}
textarea{overflow:auto;vertical-align:top}
table{border-collapse:collapse;border-spacing:0}
*,::before,::after{box-sizing:border-box}
html,body{font-size:100%}
body{background:#fff;color:rgba(0,0,0,.8);padding:0;margin:0;font-family:"Noto Serif","DejaVu Serif",serif;line-height:1;position:relative;cursor:auto;-moz-tab-size:4;-o-tab-size:4;tab-size:4;word-wrap:anywhere;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased}
a:hover{cursor:pointer}
img,object,embed{max-width:100%;height:auto}
object,embed{height:100%}
img{-ms-interpolation-mode:bicubic}
.left{float:left!important}
.right{float:right!important}
.text-left{text-align:left!important}
.text-right{text-align:right!important}
.text-center{text-align:center!important}
.text-justify{text-align:justify!important}
.hide{display:none}
img,object,svg{display:inline-block;vertical-align:middle}
textarea{height:auto;min-height:50px}
select{width:100%}
.subheader,.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{line-height:1.45;color:#7a2518;font-weight:400;margin-top:0;margin-bottom:.25em}
div,dl,dt,dd,ul,ol,li,h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6,pre,form,p,blockquote,th,td{margin:0;padding:0}
a{color:#2156a5;text-decoration:underline;line-height:inherit}
a:hover,a:focus{color:#1d4b8f}
a img{border:0}
p{line-height:1.6;margin-bottom:1.25em;text-rendering:optimizeLegibility}
p aside{font-size:.875em;line-height:1.35;font-style:italic}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{font-family:"Open Sans","DejaVu Sans",sans-serif;font-weight:300;font-style:normal;color:#ba3925;text-rendering:optimizeLegibility;margin-top:1em;margin-bottom:.5em;line-height:1.0125em}
h1 small,h2 small,h3 small,#toctitle small,.sidebarblock>.content>.title small,h4 small,h5 small,h6 small{font-size:60%;color:#e99b8f;line-height:0}
h1{font-size:2.125em}
h2{font-size:1.6875em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.375em}
h4,h5{font-size:1.125em}
h6{font-size:1em}
hr{border:solid #dddddf;border-width:1px 0 0;clear:both;margin:1.25em 0 1.1875em}
em,i{font-style:italic;line-height:inherit}
strong,b{font-weight:bold;line-height:inherit}
small{font-size:60%;line-height:inherit}
code{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;font-weight:400;color:rgba(0,0,0,.9)}
ul,ol,dl{line-height:1.6;margin-bottom:1.25em;list-style-position:outside;font-family:inherit}
ul,ol{margin-left:1.5em}
ul li ul,ul li ol{margin-left:1.25em;margin-bottom:0}
ul.circle{list-style-type:circle}
ul.disc{list-style-type:disc}
ul.square{list-style-type:square}
ul.circle ul:not([class]),ul.disc ul:not([class]),ul.square ul:not([class]){list-style:inherit}
ol li ul,ol li ol{margin-left:1.25em;margin-bottom:0}
dl dt{margin-bottom:.3125em;font-weight:bold}
dl dd{margin-bottom:1.25em}
blockquote{margin:0 0 1.25em;padding:.5625em 1.25em 0 1.1875em;border-left:1px solid #ddd}
blockquote,blockquote p{line-height:1.6;color:rgba(0,0,0,.85)}
@media screen and (min-width:768px){h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2}
h1{font-size:2.75em}
h2{font-size:2.3125em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.6875em}
h4{font-size:1.4375em}}
table{background:#fff;margin-bottom:1.25em;border:1px solid #dedede;word-wrap:normal}
table thead,table tfoot{background:#f7f8f7}
table thead tr th,table thead tr td,table tfoot tr th,table tfoot tr td{padding:.5em .625em .625em;font-size:inherit;color:rgba(0,0,0,.8);text-align:left}
table tr th,table tr td{padding:.5625em .625em;font-size:inherit;color:rgba(0,0,0,.8)}
table tr.even,table tr.alt{background:#f8f8f7}
table thead tr th,table tfoot tr th,table tbody tr td,table tr td,table tfoot tr td{line-height:1.6}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2;word-spacing:-.05em}
h1 strong,h2 strong,h3 strong,#toctitle strong,.sidebarblock>.content>.title strong,h4 strong,h5 strong,h6 strong{font-weight:400}
.center{margin-left:auto;margin-right:auto}
.stretch{width:100%}
.clearfix::before,.clearfix::after,.float-group::before,.float-group::after{content:" ";display:table}
.clearfix::after,.float-group::after{clear:both}
:not(pre).nobreak{word-wrap:normal}
:not(pre).nowrap{white-space:nowrap}
:not(pre).pre-wrap{white-space:pre-wrap}
:not(pre):not([class^=L])>code{font-size:.9375em;font-style:normal!important;letter-spacing:0;padding:.1em .5ex;word-spacing:-.15em;background:#f7f7f8;border-radius:4px;line-height:1.45;text-rendering:optimizeSpeed}
pre{color:rgba(0,0,0,.9);font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;line-height:1.45;text-rendering:optimizeSpeed}
pre code,pre pre{color:inherit;font-size:inherit;line-height:inherit}
pre>code{display:block}
pre.nowrap,pre.nowrap pre{white-space:pre;word-wrap:normal}
em em{font-style:normal}
strong strong{font-weight:400}
.keyseq{color:rgba(51,51,51,.8)}
kbd{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;display:inline-block;color:rgba(0,0,0,.8);font-size:.65em;line-height:1.45;background:#f7f7f7;border:1px solid #ccc;border-radius:3px;box-shadow:0 1px 0 rgba(0,0,0,.2),inset 0 0 0 .1em #fff;margin:0 .15em;padding:.2em .5em;vertical-align:middle;position:relative;top:-.1em;white-space:nowrap}
.keyseq kbd:first-child{margin-left:0}
.keyseq kbd:last-child{margin-right:0}
.menuseq,.menuref{color:#000}
.menuseq b:not(.caret),.menuref{font-weight:inherit}
.menuseq{word-spacing:-.02em}
.menuseq b.caret{font-size:1.25em;line-height:.8}
.menuseq i.caret{font-weight:bold;text-align:center;width:.45em}
b.button::before,b.button::after{position:relative;top:-1px;font-weight:400}
b.button::before{content:"[";padding:0 3px 0 2px}
b.button::after{content:"]";padding:0 2px 0 3px}
p a>code:hover{color:rgba(0,0,0,.9)}
#header,#content,#footnotes,#footer{width:100%;margin:0 auto;max-width:62.5em;*zoom:1;position:relative;padding-left:.9375em;padding-right:.9375em}
#header::before,#header::after,#content::before,#content::after,#footnotes::before,#footnotes::after,#footer::before,#footer::after{content:" ";display:table}
#header::after,#content::after,#footnotes::after,#footer::after{clear:both}
#content{margin-top:1.25em}
#content::before{content:none}
#header>h1:first-child{color:rgba(0,0,0,.85);margin-top:2.25rem;margin-bottom:0}
#header>h1:first-child+#toc{margin-top:8px;border-top:1px solid #dddddf}
#header>h1:only-child{border-bottom:1px solid #dddddf;padding-bottom:8px}
#header .details{border-bottom:1px solid #dddddf;line-height:1.45;padding-top:.25em;padding-bottom:.25em;padding-left:.25em;color:rgba(0,0,0,.6);display:flex;flex-flow:row wrap}
#header .details span:first-child{margin-left:-.125em}
#header .details span.email a{color:rgba(0,0,0,.85)}
#header .details br{display:none}
#header .details br+span::before{content:"\00a0\2013\00a0"}
#header .details br+span.author::before{content:"\00a0\22c5\00a0";color:rgba(0,0,0,.85)}
#header .details br+span#revremark::before{content:"\00a0|\00a0"}
#header #revnumber{text-transform:capitalize}
#header #revnumber::after{content:"\00a0"}
#content>h1:first-child:not([class]){color:rgba(0,0,0,.85);border-bottom:1px solid #dddddf;padding-bottom:8px;margin-top:0;padding-top:1rem;margin-bottom:1.25rem}
#toc{border-bottom:1px solid #e7e7e9;padding-bottom:.5em}
#toc>ul{margin-left:.125em}
#toc ul.sectlevel0>li>a{font-style:italic}
#toc ul.sectlevel0 ul.sectlevel1{margin:.5em 0}
#toc ul{font-family:"Open Sans","DejaVu Sans",sans-serif;list-style-type:none}
#toc li{line-height:1.3334;margin-top:.3334em}
#toc a{text-decoration:none}
#toc a:active{text-decoration:underline}
#toctitle{color:#7a2518;font-size:1.2em}
@media screen and (min-width:768px){#toctitle{font-size:1.375em}
body.toc2{padding-left:15em;padding-right:0}
body.toc2 #header>h1:nth-last-child(2){border-bottom:1px solid #dddddf;padding-bottom:8px}
#toc.toc2{margin-top:0!important;background:#f8f8f7;position:fixed;width:15em;left:0;top:0;border-right:1px solid #e7e7e9;border-top-width:0!important;border-bottom-width:0!important;z-index:1000;padding:1.25em 1em;height:100%;overflow:auto}
#toc.toc2 #toctitle{margin-top:0;margin-bottom:.8rem;font-size:1.2em}
#toc.toc2>ul{font-size:.9em;margin-bottom:0}
#toc.toc2 ul ul{margin-left:0;padding-left:1em}
#toc.toc2 ul.sectlevel0 ul.sectlevel1{padding-left:0;margin-top:.5em;margin-bottom:.5em}
body.toc2.toc-right{padding-left:0;padding-right:15em}
body.toc2.toc-right #toc.toc2{border-right-width:0;border-left:1px solid #e7e7e9;left:auto;right:0}}
@media screen and (min-width:1280px){body.toc2{padding-left:20em;padding-right:0}
#toc.toc2{width:20em}
#toc.toc2 #toctitle{font-size:1.375em}
#toc.toc2>ul{font-size:.95em}
#toc.toc2 ul ul{padding-left:1.25em}
body.toc2.toc-right{padding-left:0;padding-right:20em}}
#content #toc{border:1px solid #e0e0dc;margin-bottom:1.25em;padding:1.25em;background:#f8f8f7;border-radius:4px}
#content #toc>:first-child{margin-top:0}
#content #toc>:last-child{margin-bottom:0}
#footer{max-width:none;background:rgba(0,0,0,.8);padding:1.25em}
#footer-text{color:hsla(0,0%,100%,.8);line-height:1.44}
#content{margin-bottom:.625em}
.sect1{padding-bottom:.625em}
@media screen and (min-width:768px){#content{margin-bottom:1.25em}
.sect1{padding-bottom:1.25em}}
.sect1:last-child{padding-bottom:0}
.sect1+.sect1{border-top:1px solid #e7e7e9}
#content h1>a.anchor,h2>a.anchor,h3>a.anchor,#toctitle>a.anchor,.sidebarblock>.content>.title>a.anchor,h4>a.anchor,h5>a.anchor,h6>a.anchor{position:absolute;z-index:1001;width:1.5ex;margin-left:-1.5ex;display:block;text-decoration:none!important;visibility:hidden;text-align:center;font-weight:400}
#content h1>a.anchor::before,h2>a.anchor::before,h3>a.anchor::before,#toctitle>a.anchor::before,.sidebarblock>.content>.title>a.anchor::before,h4>a.anchor::before,h5>a.anchor::before,h6>a.anchor::before{content:"\00A7";font-size:.85em;display:block;padding-top:.1em}
#content h1:hover>a.anchor,#content h1>a.anchor:hover,h2:hover>a.anchor,h2>a.anchor:hover,h3:hover>a.anchor,#toctitle:hover>a.anchor,.sidebarblock>.content>.title:hover>a.anchor,h3>a.anchor:hover,#toctitle>a.anchor:hover,.sidebarblock>.content>.title>a.anchor:hover,h4:hover>a.anchor,h4>a.anchor:hover,h5:hover>a.anchor,h5>a.anchor:hover,h6:hover>a.anchor,h6>a.anchor:hover{visibility:visible}
#content h1>a.link,h2>a.link,h3>a.link,#toctitle>a.link,.sidebarblock>.content>.title>a.link,h4>a.link,h5>a.link,h6>a.link{color:#ba3925;text-decoration:none}
#content h1>a.link:hover,h2>a.link:hover,h3>a.link:hover,#toctitle>a.link:hover,.sidebarblock>.content>.title>a.link:hover,h4>a.link:hover,h5>a.link:hover,h6>a.link:hover{color:#a53221}
details,.audioblock,.imageblock,.literalblock,.listingblock,.stemblock,.videoblock{margin-bottom:1.25em}
details{margin-left:1.25rem}
details>summary{cursor:pointer;display:block;position:relative;line-height:1.6;margin-bottom:.625rem;outline:none;-webkit-tap-highlight-color:transparent}
details>summary::-webkit-details-marker{display:none}
details>summary::before{content:"";border:solid transparent;border-left:solid;border-width:.3em 0 .3em .5em;position:absolute;top:.5em;left:-1.25rem;transform:translateX(15%)}
details[open]>summary::before{border:solid transparent;border-top:solid;border-width:.5em .3em 0;transform:translateY(15%)}
details>summary::after{content:"";width:1.25rem;height:1em;position:absolute;top:.3em;left:-1.25rem}
.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{text-rendering:optimizeLegibility;text-align:left;font-family:"Noto Serif","DejaVu Serif",serif;font-size:1rem;font-style:italic}
table.tableblock.fit-content>caption.title{white-space:nowrap;width:0}
.paragraph.lead>p,#preamble>.sectionbody>[class=paragraph]:first-of-type p{font-size:1.21875em;line-height:1.6;color:rgba(0,0,0,.85)}
.admonitionblock>table{border-collapse:separate;border:0;background:none;width:100%}
.admonitionblock>table td.icon{text-align:center;width:80px}
.admonitionblock>table td.icon img{max-width:none}
.admonitionblock>table td.icon .title{font-weight:bold;font-family:"Open Sans","DejaVu Sans",sans-serif;text-transform:uppercase}
.admonitionblock>table td.content{padding-left:1.125em;padding-right:1.25em;border-left:1px solid #dddddf;color:rgba(0,0,0,.6);word-wrap:anywhere}
.admonitionblock>table td.content>:last-child>:last-child{margin-bottom:0}
.exampleblock>.content{border:1px solid #e6e6e6;margin-bottom:1.25em;padding:1.25em;background:#fff;border-radius:4px}
.sidebarblock{border:1px solid #dbdbd6;margin-bottom:1.25em;padding:1.25em;background:#f3f3f2;border-radius:4px}
.sidebarblock>.content>.title{color:#7a2518;margin-top:0;text-align:center}
.exampleblock>.content>:first-child,.sidebarblock>.content>:first-child{margin-top:0}
.exampleblock>.content>:last-child,.exampleblock>.content>:last-child>:last-child,.exampleblock>.content .olist>ol>li:last-child>:last-child,.exampleblock>.content .ulist>ul>li:last-child>:last-child,.exampleblock>.content .qlist>ol>li:last-child>:last-child,.sidebarblock>.content>:last-child,.sidebarblock>.content>:last-child>:last-child,.sidebarblock>.content .olist>ol>li:last-child>:last-child,.sidebarblock>.content .ulist>ul>li:last-child>:last-child,.sidebarblock>.content .qlist>ol>li:last-child>:last-child{margin-bottom:0}
.literalblock pre,.listingblock>.content>pre{border-radius:4px;overflow-x:auto;padding:1em;font-size:.8125em}
@media screen and (min-width:768px){.literalblock pre,.listingblock>.content>pre{font-size:.90625em}}
@media screen and (min-width:1280px){.literalblock pre,.listingblock>.content>pre{font-size:1em}}
.literalblock pre,.listingblock>.content>pre:not(.highlight),.listingblock>.content>pre[class=highlight],.listingblock>.content>pre[class^="highlight "]{background:#f7f7f8}
.literalblock.output pre{color:#f7f7f8;background:rgba(0,0,0,.9)}
.listingblock>.content{position:relative}
.listingblock code[data-lang]::before{display:none;content:attr(data-lang);position:absolute;font-size:.75em;top:.425rem;right:.5rem;line-height:1;text-transform:uppercase;color:inherit;opacity:.5}
.listingblock:hover code[data-lang]::before{display:block}
.listingblock.terminal pre .command::before{content:attr(data-prompt);padding-right:.5em;color:inherit;opacity:.5}
.listingblock.terminal pre .command:not([data-prompt])::before{content:"$"}
.listingblock pre.highlightjs{padding:0}
.listingblock pre.highlightjs>code{padding:1em;border-radius:4px}
.listingblock pre.prettyprint{border-width:0}
.prettyprint{background:#f7f7f8}
pre.prettyprint .linenums{line-height:1.45;margin-left:2em}
pre.prettyprint li{background:none;list-style-type:inherit;padding-left:0}
pre.prettyprint li code[data-lang]::before{opacity:1}
pre.prettyprint li:not(:first-child) code[data-lang]::before{display:none}
table.linenotable{border-collapse:separate;border:0;margin-bottom:0;background:none}
table.linenotable td[class]{color:inherit;vertical-align:top;padding:0;line-height:inherit;white-space:normal}
table.linenotable td.code{padding-left:.75em}
table.linenotable td.linenos,pre.pygments .linenos{border-right:1px solid;opacity:.35;padding-right:.5em;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}
pre.pygments span.linenos{display:inline-block;margin-right:.75em}
.quoteblock{margin:0 1em 1.25em 1.5em;display:table}
.quoteblock:not(.excerpt)>.title{margin-left:-1.5em;margin-bottom:.75em}
.quoteblock blockquote,.quoteblock p{color:rgba(0,0,0,.85);font-size:1.15rem;line-height:1.75;word-spacing:.1em;letter-spacing:0;font-style:italic;text-align:justify}
.quoteblock blockquote{margin:0;padding:0;border:0}
.quoteblock blockquote::before{content:"\201c";float:left;font-size:2.75em;font-weight:bold;line-height:.6em;margin-left:-.6em;color:#7a2518;text-shadow:0 1px 2px rgba(0,0,0,.1)}
.quoteblock blockquote>.paragraph:last-child p{margin-bottom:0}
.quoteblock .attribution{margin-top:.75em;margin-right:.5ex;text-align:right}
.verseblock{margin:0 1em 1.25em}
.verseblock pre{font-family:"Open Sans","DejaVu Sans",sans-serif;font-size:1.15rem;color:rgba(0,0,0,.85);font-weight:300;text-rendering:optimizeLegibility}
.verseblock pre strong{font-weight:400}
.verseblock .attribution{margin-top:1.25rem;margin-left:.5ex}
.quoteblock .attribution,.verseblock .attribution{font-size:.9375em;line-height:1.45;font-style:italic}
.quoteblock .attribution br,.verseblock .attribution br{display:none}
.quoteblock .attribution cite,.verseblock .attribution cite{display:block;letter-spacing:-.025em;color:rgba(0,0,0,.6)}
.quoteblock.abstract blockquote::before,.quoteblock.excerpt blockquote::before,.quoteblock .quoteblock blockquote::before{display:none}
.quoteblock.abstract blockquote,.quoteblock.abstract p,.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{line-height:1.6;word-spacing:0}
.quoteblock.abstract{margin:0 1em 1.25em;display:block}
.quoteblock.abstract>.title{margin:0 0 .375em;font-size:1.15em;text-align:center}
.quoteblock.excerpt>blockquote,.quoteblock .quoteblock{padding:0 0 .25em 1em;border-left:.25em solid #dddddf}
.quoteblock.excerpt,.quoteblock .quoteblock{margin-left:0}
.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{color:inherit;font-size:1.0625rem}
.quoteblock.excerpt .attribution,.quoteblock .quoteblock .attribution{color:inherit;font-size:.85rem;text-align:left;margin-right:0}
p.tableblock:last-child{margin-bottom:0}
td.tableblock>.content{margin-bottom:1.25em;word-wrap:anywhere}
td.tableblock>.content>:last-child{margin-bottom:-1.25em}
table.tableblock,th.tableblock,td.tableblock{border:0 solid #dedede}
table.grid-all>*>tr>*{border-width:1px}
table.grid-cols>*>tr>*{border-width:0 1px}
table.grid-rows>*>tr>*{border-width:1px 0}
table.frame-all{border-width:1px}
table.frame-ends{border-width:1px 0}
table.frame-sides{border-width:0 1px}
table.frame-none>colgroup+*>:first-child>*,table.frame-sides>colgroup+*>:first-child>*{border-top-width:0}
table.frame-none>:last-child>:last-child>*,table.frame-sides>:last-child>:last-child>*{border-bottom-width:0}
table.frame-none>*>tr>:first-child,table.frame-ends>*>tr>:first-child{border-left-width:0}
table.frame-none>*>tr>:last-child,table.frame-ends>*>tr>:last-child{border-right-width:0}
table.stripes-all>*>tr,table.stripes-odd>*>tr:nth-of-type(odd),table.stripes-even>*>tr:nth-of-type(even),table.stripes-hover>*>tr:hover{background:#f8f8f7}
th.halign-left,td.halign-left{text-align:left}
th.halign-right,td.halign-right{text-align:right}
th.halign-center,td.halign-center{text-align:center}
th.valign-top,td.valign-top{vertical-align:top}
th.valign-bottom,td.valign-bottom{vertical-align:bottom}
th.valign-middle,td.valign-middle{vertical-align:middle}
table thead th,table tfoot th{font-weight:bold}
tbody tr th{background:#f7f8f7}
tbody tr th,tbody tr th p,tfoot tr th,tfoot tr th p{color:rgba(0,0,0,.8);font-weight:bold}
p.tableblock>code:only-child{background:none;padding:0}
p.tableblock{font-size:1em}
ol{margin-left:1.75em}
ul li ol{margin-left:1.5em}
dl dd{margin-left:1.125em}
dl dd:last-child,dl dd:last-child>:last-child{margin-bottom:0}
li p,ul dd,ol dd,.olist .olist,.ulist .ulist,.ulist .olist,.olist .ulist{margin-bottom:.625em}
ul.checklist,ul.none,ol.none,ul.no-bullet,ol.no-bullet,ol.unnumbered,ul.unstyled,ol.unstyled{list-style-type:none}
ul.no-bullet,ol.no-bullet,ol.unnumbered{margin-left:.625em}
ul.unstyled,ol.unstyled{margin-left:0}
li>p:empty:only-child::before{content:"";display:inline-block}
ul.checklist>li>p:first-child{margin-left:-1em}
ul.checklist>li>p:first-child>.fa-square-o:first-child,ul.checklist>li>p:first-child>.fa-check-square-o:first-child{width:1.25em;font-size:.8em;position:relative;bottom:.125em}
ul.checklist>li>p:first-child>input[type=checkbox]:first-child{margin-right:.25em}
ul.inline{display:flex;flex-flow:row wrap;list-style:none;margin:0 0 .625em -1.25em}
ul.inline>li{margin-left:1.25em}
.unstyled dl dt{font-weight:400;font-style:normal}
ol.arabic{list-style-type:decimal}
ol.decimal{list-style-type:decimal-leading-zero}
ol.loweralpha{list-style-type:lower-alpha}
ol.upperalpha{list-style-type:upper-alpha}
ol.lowerroman{list-style-type:lower-roman}
ol.upperroman{list-style-type:upper-roman}
ol.lowergreek{list-style-type:lower-greek}
.hdlist>table,.colist>table{border:0;background:none}
.hdlist>table>tbody>tr,.colist>table>tbody>tr{background:none}
td.hdlist1,td.hdlist2{vertical-align:top;padding:0 .625em}
td.hdlist1{font-weight:bold;padding-bottom:1.25em}
td.hdlist2{word-wrap:anywhere}
.literalblock+.colist,.listingblock+.colist{margin-top:-.5em}
.colist td:not([class]):first-child{padding:.4em .75em 0;line-height:1;vertical-align:top}
.colist td:not([class]):first-child img{max-width:none}
.colist td:not([class]):last-child{padding:.25em 0}
.thumb,.th{line-height:0;display:inline-block;border:4px solid #fff;box-shadow:0 0 0 1px #ddd}
.imageblock.left{margin:.25em .625em 1.25em 0}
.imageblock.right{margin:.25em 0 1.25em .625em}
.imageblock>.title{margin-bottom:0}
.imageblock.thumb,.imageblock.th{border-width:6px}
.imageblock.thumb>.title,.imageblock.th>.title{padding:0 .125em}
.image.left,.image.right{margin-top:.25em;margin-bottom:.25em;display:inline-block;line-height:0}
.image.left{margin-right:.625em}
.image.right{margin-left:.625em}
a.image{text-decoration:none;display:inline-block}
a.image object{pointer-events:none}
sup.footnote,sup.footnoteref{font-size:.875em;position:static;vertical-align:super}
sup.footnote a,sup.footnoteref a{text-decoration:none}
sup.footnote a:active,sup.footnoteref a:active,#footnotes .footnote a:first-of-type:active{text-decoration:underline}
#footnotes{padding-top:.75em;padding-bottom:.75em;margin-bottom:.625em}
#footnotes hr{width:20%;min-width:6.25em;margin:-.25em 0 .75em;border-width:1px 0 0}
#footnotes .footnote{padding:0 .375em 0 .225em;line-height:1.3334;font-size:.875em;margin-left:1.2em;margin-bottom:.2em}
#footnotes .footnote a:first-of-type{font-weight:bold;text-decoration:none;margin-left:-1.05em}
#footnotes .footnote:last-of-type{margin-bottom:0}
#content #footnotes{margin-top:-.625em;margin-bottom:0;padding:.75em 0}
div.unbreakable{page-break-inside:avoid}
.big{font-size:larger}
.small{font-size:smaller}
.underline{text-decoration:underline}
.overline{text-decoration:overline}
.line-through{text-decoration:line-through}
.aqua{color:#00bfbf}
.aqua-background{background:#00fafa}
.black{color:#000}
.black-background{background:#000}
.blue{color:#0000bf}
.blue-background{background:#0000fa}
.fuchsia{color:#bf00bf}
.fuchsia-background{background:#fa00fa}
.gray{color:#606060}
.gray-background{background:#7d7d7d}
.green{color:#006000}
.green-background{background:#007d00}
.lime{color:#00bf00}
.lime-background{background:#00fa00}
.maroon{color:#600000}
.maroon-background{background:#7d0000}
.navy{color:#000060}
.navy-background{background:#00007d}
.olive{color:#606000}
.olive-background{background:#7d7d00}
.purple{color:#600060}
.purple-background{background:#7d007d}
.red{color:#bf0000}
.red-background{background:#fa0000}
.silver{color:#909090}
.silver-background{background:#bcbcbc}
.teal{color:#006060}
.teal-background{background:#007d7d}
.white{color:#bfbfbf}
.white-background{background:#fafafa}
.yellow{color:#bfbf00}
.yellow-background{background:#fafa00}
span.icon>.fa{cursor:default}
a span.icon>.fa{cursor:inherit}
.admonitionblock td.icon [class^="fa icon-"]{font-size:2.5em;text-shadow:1px 1px 2px rgba(0,0,0,.5);cursor:default}
.admonitionblock td.icon .icon-note::before{content:"\f05a";color:#19407c}
.admonitionblock td.icon .icon-tip::before{content:"\f0eb";text-shadow:1px 1px 2px rgba(155,155,0,.8);color:#111}
.admonitionblock td.icon .icon-warning::before{content:"\f071";color:#bf6900}
.admonitionblock td.icon .icon-caution::before{content:"\f06d";color:#bf3400}
.admonitionblock td.icon .icon-important::before{content:"\f06a";color:#bf0000}
.conum[data-value]{display:inline-block;color:#fff!important;background:rgba(0,0,0,.8);border-radius:50%;text-align:center;font-size:.75em;width:1.67em;height:1.67em;line-height:1.67em;font-family:"Open Sans","DejaVu Sans",sans-serif;font-style:normal;font-weight:bold}
.conum[data-value] *{color:#fff!important}
.conum[data-value]+b{display:none}
.conum[data-value]::after{content:attr(data-value)}
pre .conum[data-value]{position:relative;top:-.125em}
b.conum *{color:inherit!important}
.conum:not([data-value]):empty{display:none}
dt,th.tableblock,td.content,div.footnote{text-rendering:optimizeLegibility}
h1,h2,p,td.content,span.alt,summary{letter-spacing:-.01em}
p strong,td.content strong,div.footnote strong{letter-spacing:-.005em}
p,blockquote,dt,td.content,td.hdlist1,span.alt,summary{font-size:1.0625rem}
p{margin-bottom:1.25rem}
.sidebarblock p,.sidebarblock dt,.sidebarblock td.content,p.tableblock{font-size:1em}
.exampleblock>.content{background:#fffef7;border-color:#e0e0dc;box-shadow:0 1px 4px #e0e0dc}
.print-only{display:none!important}
@page{margin:1.25cm .75cm}
@media print{*{box-shadow:none!important;text-shadow:none!important}
html{font-size:80%}
a{color:inherit!important;text-decoration:underline!important}
a.bare,a[href^="#"],a[href^="mailto:"]{text-decoration:none!important}
a[href^="http:"]:not(.bare)::after,a[href^="https:"]:not(.bare)::after{content:"(" attr(href) ")";display:inline-block;font-size:.875em;padding-left:.25em}
abbr[title]{border-bottom:1px dotted}
abbr[title]::after{content:" (" attr(title) ")"}
pre,blockquote,tr,img,object,svg{page-break-inside:avoid}
thead{display:table-header-group}
svg{max-width:100%}
p,blockquote,dt,td.content{font-size:1em;orphans:3;widows:3}
h2,h3,#toctitle,.sidebarblock>.content>.title{page-break-after:avoid}
#header,#content,#footnotes,#footer{max-width:none}
#toc,.sidebarblock,.exampleblock>.content{background:none!important}
#toc{border-bottom:1px solid #dddddf!important;padding-bottom:0!important}
body.book #header{text-align:center}
body.book #header>h1:first-child{border:0!important;margin:2.5em 0 1em}
body.book #header .details{border:0!important;display:block;padding:0!important}
body.book #header .details span:first-child{margin-left:0!important}
body.book #header .details br{display:block}
body.book #header .details br+span::before{content:none!important}
body.book #toc{border:0!important;text-align:left!important;padding:0!important;margin:0!important}
body.book #toc,body.book #preamble,body.book h1.sect0,body.book .sect1>h2{page-break-before:always}
.listingblock code[data-lang]::before{display:block}
#footer{padding:0 .9375em}
.hide-on-print{display:none!important}
.print-only{display:block!important}
.hide-for-print{display:none!important}
.show-for-print{display:inherit!important}}
@media amzn-kf8,print{#header>h1:first-child{margin-top:1.25rem}
.sect1{padding:0!important}
.sect1+.sect1{border:0}
#footer{background:none}
#footer-text{color:rgba(0,0,0,.6);font-size:.9em}}
@media amzn-kf8{#header,#content,#footnotes,#footer{padding:0}}
</style>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
</head>
<body class="article toc2 toc-left">
<div id="header">
<h1>Learning LangChain: Building AI and LLM Applications with LangChain and LangGraph</h1>
<div class="details">
<span id="author" class="author">Mayo Oshin</span><br>
</div>
<div id="toc" class="toc2">
<div id="toctitle">Table of Contents</div>
<ul class="sectlevel1">
<li><a href="#_chapter_1_llm_fundamentals_with_langchain">Chapter 1. LLM Fundamentals with LangChain</a>
<ul class="sectlevel2">
<li><a href="#_getting_set_up_with_langchain">Getting Set Up with LangChain</a></li>
<li><a href="#_using_llms_in_langchain">Using LLMs in LangChain</a></li>
<li><a href="#_making_llm_prompts_reusable">Making LLM Prompts Reusable</a></li>
</ul>
</li>
<li><a href="#_chapter_2_rag_part_i_indexing_your_data">Chapter 2. RAG Part I: Indexing Your Data</a>
<ul class="sectlevel2">
<li><a href="#_splitting_your_text_into_chunks">Splitting Your Text into Chunks</a></li>
<li><a href="#_generating_text_embeddings">Generating Text Embeddings</a></li>
<li><a href="#_storing_embeddings_in_a_vector_store">Storing Embeddings in a Vector Store</a></li>
<li><a href="#_tracking_changes_to_your_documents">Tracking Changes to Your Documents</a></li>
<li><a href="#_indexing_optimization">Indexing Optimization</a>
<ul class="sectlevel3">
<li><a href="#_multivectorretriever">MultiVectorRetriever</a></li>
<li><a href="#_colbert_optimizing_embeddings">ColBERT Optimizing Embeddings</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#_chapter_3_rag_part_ii_chatting_with_your_data">Chapter 3. RAG Part II: Chatting with Your Data</a>
<ul class="sectlevel2">
<li><a href="#_introducing_retrieval_augmented_generation">Introducing Retrieval-Augmented Generation</a>
<ul class="sectlevel3">
<li><a href="#_retrieving_relevant_documents">Retrieving Relevant Documents</a></li>
<li><a href="#_generating_llm_predictions_using_relevant_documents">Generating LLM Predictions Using Relevant Documents</a></li>
</ul>
</li>
<li><a href="#_query_transformation">Query Transformation</a>
<ul class="sectlevel3">
<li><a href="#_rewrite_retrieve_read">Rewrite-Retrieve-Read</a></li>
<li><a href="#_multi_query_retrieval">Multi-Query Retrieval</a></li>
<li><a href="#_rag_fusion">RAG-Fusion</a></li>
<li><a href="#_hypothetical_document_embeddings">Hypothetical Document Embeddings</a></li>
</ul>
</li>
<li><a href="#_query_routing">Query Routing</a>
<ul class="sectlevel3">
<li><a href="#_logical_routing">Logical Routing</a></li>
<li><a href="#_semantic_routing">Semantic Routing</a></li>
</ul>
</li>
<li><a href="#_query_construction">Query Construction</a>
<ul class="sectlevel3">
<li><a href="#_text_to_metadata_filter">Text-to-Metadata Filter</a></li>
<li><a href="#_text_to_sql">Text-to-SQL</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#_chapter_4_using_langgraph_to_add_memory_to_your_chatbot">Chapter 4. Using LangGraph to Add Memory to Your Chatbot</a>
<ul class="sectlevel2">
<li><a href="#_building_a_chatbot_memory_system">Building a Chatbot Memory System</a></li>
<li><a href="#_introducing_langgraph">Introducing LangGraph</a></li>
<li><a href="#_creating_a_stategraph">Creating a StateGraph</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="content">
<div class="sect1">
<h2 id="_chapter_1_llm_fundamentals_with_langchain">Chapter 1. LLM Fundamentals with LangChain</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_getting_set_up_with_langchain">Getting Set Up with LangChain</h3>
<div class="paragraph">
<p>The chapter recommends setting up LangChain on your computer to follow along with the examples. First, create an OpenAI account and generate an API key from the OpenAI website. LangChain supports both Python and JavaScript, and the book provides equivalent code examples in both languages.</p>
</div>
<div class="paragraph">
<p>For Python users:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Ensure Python is installed.</p>
</li>
<li>
<p>Optionally install Jupyter Notebook (<code>pip install notebook</code>).</p>
</li>
<li>
<p>Install LangChain and related libraries via pip.</p>
</li>
<li>
<p>Set the OpenAI API key in your terminal environment.</p>
</li>
<li>
<p>Launch Jupyter Notebook to run the examples.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>For JavaScript users:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Set the OpenAI API key in your terminal environment.</p>
</li>
<li>
<p>Install Node.js if needed.</p>
</li>
<li>
<p>Install LangChain and related packages via npm.</p>
</li>
<li>
<p>Save code examples as <code>.js</code> files and run them using Node.js.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>This setup ensures you can run the provided LangChain code examples smoothly throughout the book.</p>
</div>
</div>
<div class="sect2">
<h3 id="_using_llms_in_langchain">Using LLMs in LangChain</h3>
<div class="exampleblock">
<div class="content">
<a href="https://github.com/langchain-ai/learning-langchain/blob/master/ch1/py/a-llm.py" target="_blank">
ch1/py/a-llm.py</a>
<hr>
<a href="https://github.com/langchain-ai/learning-langchain/blob/master/ch1/py/b-chat.py" target="_blank">
ch1/py/b-chat.py</a>
<hr>
<a href="https://github.com/langchain-ai/learning-langchain/blob/master/ch1/py/c-system.py" target="_blank">
ch1/py/c-system.py</a>
</div>
</div>
<div class="paragraph">
<p>This document details how to interact with Large Language Models (LLMs) using the LangChain framework, focusing on two primary interfaces: <strong>LLMs</strong> and <strong>Chat Models</strong>.</p>
</div>
<div class="paragraph">
<p><strong>LLM Interface:</strong> This interface takes a simple string prompt, sends it to the LLM provider (like OpenAI), and returns the model&#8217;s prediction.  Key parameters for configuration include <code>model</code> (specifying the underlying LLM), <code>temperature</code> (controlling output randomness/creativity), and <code>max_tokens</code> (limiting output length/cost).</p>
</div>
<div class="paragraph">
<p><strong>Chat Model Interface:</strong> This interface is designed for conversational interactions. It differentiates messages by <code>role</code>:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>System:</strong> Instructions for the model.</p>
</li>
<li>
<p><strong>User:</strong> The user&#8217;s input/query.</p>
</li>
<li>
<p><strong>Assistant:</strong> The model&#8217;s response.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>LangChain provides message classes like <code>HumanMessage</code>, <code>AIMessage</code>, <code>SystemMessage</code>, and <code>ChatMessage</code> to manage these roles. Using <code>SystemMessage</code> allows pre-configuring the AI&#8217;s behavior, ensuring more predictable responses.</p>
</div>
<div class="paragraph">
<p><strong>Key Takeaways:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>LangChain simplifies LLM interaction.</strong> It provides consistent interfaces regardless of the underlying provider.</p>
</li>
<li>
<p><strong>Choosing between LLM and Chat Model depends on the application.</strong>  Simple prompts use the LLM interface, while conversations benefit from the Chat Model interface.</p>
</li>
<li>
<p><strong>Parameters like <code>temperature</code> and <code>max_tokens</code> are crucial for controlling LLM behavior and cost.</strong></p>
</li>
<li>
<p><strong><code>SystemMessage</code> is powerful for guiding the model&#8217;s responses.</strong></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The document includes code examples in both Python and JavaScript demonstrating how to use both interfaces with OpenAI&#8217;s <code>gpt-3.5-turbo</code> model.</p>
</div>
</div>
<div class="sect2">
<h3 id="_making_llm_prompts_reusable">Making LLM Prompts Reusable</h3>
<div class="paragraph">
<p>The content explains how prompt instructions shape the output of language models by providing context and guiding responses. It demonstrates creating detailed prompts with dynamic inputs (like context and question) using LangChain’s <code>PromptTemplate</code> and <code>ChatPromptTemplate</code> interfaces in both Python and JavaScript. These templates allow placeholders (e.g., <code>{context}</code>, <code>{question}</code>) to be filled at runtime, enabling flexible prompt construction.</p>
</div>
<div class="paragraph">
<p>Examples show:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Defining a prompt template with placeholders.</p>
</li>
<li>
<p>Invoking the template with specific context and question values.</p>
</li>
<li>
<p>Passing the formatted prompt to an LLM (OpenAI) to generate answers.</p>
</li>
<li>
<p>Using <code>ChatPromptTemplate</code> for chat-based interactions with role-based messages (<code>system</code>, <code>human</code>).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The key takeaway is that LangChain simplifies building reusable, dynamic prompts that can be integrated with LLMs for question answering or chat applications, ensuring prompts are structured and adaptable to user inputs.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_chapter_2_rag_part_i_indexing_your_data">Chapter 2. RAG Part I: Indexing Your Data</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_splitting_your_text_into_chunks">Splitting Your Text into Chunks</h3>
<div class="paragraph">
<p>The content explains how LangChain&#8217;s <code>RecursiveCharacterTextSplitter</code> helps split large texts into semantically meaningful chunks by recursively splitting text using a prioritized list of separators (paragraphs, lines, words) to respect chunk size limits. It outputs chunks as <code>Document</code> objects with metadata and position info.</p>
</div>
<div class="paragraph">
<p>Key points:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Default separators: paragraphs (<code>\n\n</code>), lines (<code>\n</code>), words (space).</p>
</li>
<li>
<p>Splitting starts with largest separator and moves to smaller ones if chunks exceed size.</p>
</li>
<li>
<p>Supports chunk size and overlap to maintain context.</p>
</li>
<li>
<p>Can split raw text strings or documents loaded from files.</p>
</li>
<li>
<p>Specialized splitting for code and Markdown using language-specific separators to keep semantic units (e.g., function bodies) intact.</p>
</li>
<li>
<p>LangChain provides built-in separators for many languages (Python, JS, Markdown, HTML).</p>
</li>
<li>
<p><code>from_language</code> method creates splitter instances tailored to specific languages.</p>
</li>
<li>
<p><code>create_documents</code> method splits raw text strings into documents, optionally attaching metadata per chunk.</p>
</li>
<li>
<p>Metadata is preserved and attached to each chunk, useful for tracking source or provenance.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Examples show usage in Python and JavaScript for plain text, Python code, and Markdown, demonstrating how chunks align with natural text/code boundaries and how metadata is propagated.</p>
</div>
</div>
<div class="sect2">
<h3 id="_generating_text_embeddings">Generating Text Embeddings</h3>
<div class="paragraph">
<p>The content explains how LangChain&#8217;s <code>Embeddings</code> class interfaces with text embedding models (like OpenAI, Cohere, Hugging Face) to convert text into vector representations. It provides two methods: one for embedding multiple documents (list of strings) and one for embedding a single query string. Examples in Python and JavaScript demonstrate embedding multiple documents at once for efficiency, returning lists of numeric vectors.</p>
</div>
<div class="paragraph">
<p>An end-to-end example shows how to:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Load documents using document loaders (e.g., <code>TextLoader</code>).</p>
</li>
<li>
<p>Split large documents into smaller chunks with text splitters (e.g., <code>RecursiveCharacterTextSplitter</code>).</p>
</li>
<li>
<p>Generate embeddings for each chunk using an embeddings model (e.g., <code>OpenAIEmbeddings</code>).</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>The example code is provided in both Python and JavaScript. After generating embeddings, the next step is to store them in a vector store database for further use.</p>
</div>
</div>
<div class="sect2">
<h3 id="_storing_embeddings_in_a_vector_store">Storing Embeddings in a Vector Store</h3>
<div class="paragraph">
<p>The chapter explains vector stores—databases optimized for storing vectors and performing similarity calculations like cosine similarity, especially for unstructured data such as text and images. Unlike traditional structured-data databases, vector stores support CRUD and search operations on vector embeddings, enabling AI-powered applications like querying large documents.</p>
</div>
<div class="paragraph">
<p>There are many vector store providers, each with different features (multitenancy, filtering, performance, cost, scalability). However, vector stores are relatively new, can be complex to manage, and add system complexity.</p>
</div>
<div class="paragraph">
<p>To address this, PostgreSQL supports vector storage via the <code>pgvector</code> extension, allowing users to manage both traditional relational data and vector embeddings in one familiar database.</p>
</div>
<div class="paragraph">
<p>The setup involves running a Docker container with Postgres + pgvector, then connecting via a connection string.</p>
</div>
<div class="paragraph">
<p>Examples in Python and JavaScript show how to:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Load and split documents into chunks</p>
</li>
<li>
<p>Generate embeddings using OpenAIEmbeddings (or other models)</p>
</li>
<li>
<p>Store embeddings and documents in PGVector (Postgres)</p>
</li>
<li>
<p>Perform similarity searches to retrieve relevant documents</p>
</li>
<li>
<p>Add new documents with metadata and UUIDs</p>
</li>
<li>
<p>Delete documents by ID</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The process includes embedding queries, searching for nearest vectors in Postgres, and returning documents sorted by similarity.</p>
</div>
<div class="paragraph">
<p>This integration simplifies vector search by leveraging a popular relational database, reducing the need for separate vector store infrastructure while enabling scalable AI applications.</p>
</div>
</div>
<div class="sect2">
<h3 id="_tracking_changes_to_your_documents">Tracking Changes to Your Documents</h3>
<div class="paragraph">
<p>The content explains how LangChain&#8217;s indexing API helps manage vector stores with frequently changing data by avoiding costly re-indexing and duplicate embeddings. It uses a <code>RecordManager</code> class to track documents via hashes, write times, and source IDs. The API supports cleanup modes to control deletion of outdated documents:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>None</code>: no automatic cleanup.</p>
</li>
<li>
<p><code>Incremental</code>: deletes previous versions if content changes.</p>
</li>
<li>
<p><code>Full</code>: deletes previous versions and any documents not in the current batch.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Examples in Python and JavaScript demonstrate setting up a Postgres-backed vector store and record manager, creating documents, and indexing them with incremental cleanup to prevent duplicates. When documents are modified, the API replaces old versions sharing the same source ID. This approach keeps the vector store synchronized efficiently by only updating changed documents.</p>
</div>
</div>
<div class="sect2">
<h3 id="_indexing_optimization">Indexing Optimization</h3>
<div class="sect3">
<h4 id="_multivectorretriever">MultiVectorRetriever</h4>
<div class="paragraph">
<p>The document explains a method to handle documents containing both text and tables for retrieval-augmented generation (RAG) without losing table data. Instead of embedding raw text chunks (which can omit tables), it proposes a two-level indexing approach:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Summarize table elements</strong> using an LLM, generating summaries that include an ID referencing the full raw table.</p>
</li>
<li>
<p><strong>Store summaries in a vector store</strong> for efficient similarity search.</p>
</li>
<li>
<p><strong>Store full raw tables separately</strong> in a document store (docstore) keyed by the summary IDs.</p>
</li>
<li>
<p>When a query retrieves a summary, <strong>fetch the full referenced raw table</strong> from the docstore and pass it as context to the LLM for answer synthesis.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>This decoupling allows retrieval of concise summaries for fast search while preserving access to complete table data for accurate answers.</p>
</div>
<div class="paragraph">
<p>The document provides detailed Python and JavaScript code examples demonstrating:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Loading and splitting documents into chunks.</p>
</li>
<li>
<p>Using an LLM to generate summaries of chunks.</p>
</li>
<li>
<p>Indexing summaries in a vector store (Postgres PGVector).</p>
</li>
<li>
<p>Storing original chunks in an in-memory docstore.</p>
</li>
<li>
<p>Using a <code>MultiVectorRetriever</code> to first retrieve summaries by similarity, then fetch full original chunks by ID.</p>
</li>
<li>
<p>Querying the retriever to get relevant full context documents for downstream LLM prompting.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>This approach ensures that tables and other complex document structures are not lost during chunking and embedding, enabling richer and more accurate retrieval and answer synthesis.</p>
</div>
</div>
<div class="sect3">
<h4 id="_colbert_optimizing_embeddings">ColBERT Optimizing Embeddings</h4>
<div class="paragraph">
<p>The text discusses a challenge with embedding models that compress documents into fixed-length vectors, which can include irrelevant or redundant content and cause hallucinations in LLM outputs. A more granular approach involves generating contextual embeddings for each token in both the query and document, scoring token-level similarities, and summing maximum similarity scores to rank documents. The ColBERT embedding model implements this solution effectively.</p>
</div>
<div class="paragraph">
<p>An example Python workflow using the RAGatouille library demonstrates how to:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Retrieve full Wikipedia page text via API,</p>
</li>
<li>
<p>Index the document with ColBERT embeddings,</p>
</li>
<li>
<p>Perform similarity search queries,</p>
</li>
<li>
<p>And integrate with LangChain retrievers for improved document retrieval.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Using ColBERT in this way enhances the relevance of documents retrieved as context for large language models, reducing hallucinations and improving output quality.</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_chapter_3_rag_part_ii_chatting_with_your_data">Chapter 3. RAG Part II: Chatting with Your Data</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_introducing_retrieval_augmented_generation">Introducing Retrieval-Augmented Generation</h3>
<div class="paragraph">
<p>The text explains Retrieval-Augmented Generation (RAG), a technique that improves the accuracy of large language models (LLMs) by providing them with up-to-date external context. Without RAG, LLMs rely solely on pretrained data, which can be outdated, leading to incorrect answers—as illustrated by ChatGPT incorrectly naming France as the latest FIFA World Cup winner instead of Argentina (2022). By appending relevant, current information (e.g., from Wikipedia) as context to the prompt, the LLM can generate accurate responses. However, manually adding such context is impractical for real-world applications, highlighting the need for automated systems that retrieve and supply relevant information dynamically to LLMs during generation.</p>
</div>
<div class="sect3">
<h4 id="_retrieving_relevant_documents">Retrieving Relevant Documents</h4>
<div class="paragraph">
<p>The content explains the three core stages of a Retrieval-Augmented Generation (RAG) system for AI applications:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Indexing</strong>: Preprocess external data by loading documents, splitting them into chunks, embedding these chunks into vector representations, and storing them in a vector store for efficient retrieval. Code examples in Python and JavaScript demonstrate loading a text file, splitting it, embedding chunks using OpenAI embeddings, and storing them in a PostgreSQL-backed vector store (PGVector).</p>
</li>
<li>
<p><strong>Retrieval</strong>: When a user query is received, it is converted into embeddings and compared against stored embeddings using similarity metrics (e.g., cosine similarity) to find the most relevant document chunks. The vector store provides an <code>as_retriever</code> method to abstract query embedding and similarity search. The number of documents retrieved can be controlled by a parameter <code>k</code> to balance relevance, performance, and prompt size. Code examples show how to retrieve relevant documents using this method.</p>
</li>
<li>
<p><strong>Generation</strong>: The final stage involves combining the original user prompt with the retrieved relevant documents to form a comprehensive prompt that is sent to the language model for generating a prediction or answer.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Figures referenced illustrate the flow of indexing and retrieval processes, including similarity calculations using structures like Hierarchical Navigable Small World (HNSW) graphs.</p>
</div>
<div class="paragraph">
<p>Overall, the chapter emphasizes practical implementation of RAG with LangChain libraries, highlighting the importance of efficient indexing, controlled retrieval, and prompt synthesis for effective AI applications.</p>
</div>
</div>
<div class="sect3">
<h4 id="_generating_llm_predictions_using_relevant_documents">Generating LLM Predictions Using Relevant Documents</h4>
<div class="exampleblock">
<div class="content">
<a href="https://github.com/langchain-ai/learning-langchain/blob/master/ch3/py/a-basic-rag.py" target="_blank">
ch3/py/a-basic-rag.py</a>
</div>
</div>
<div class="paragraph">
<p>The content explains how to build a Retrieval-Augmented Generation (RAG) system by integrating relevant documents retrieved from a vector store into a prompt for a large language model (LLM) to generate informed answers. It provides Python and JavaScript code examples demonstrating:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Creating a dynamic prompt template with context and question variables.</p>
</li>
<li>
<p>Using a retriever to fetch relevant documents based on a user query.</p>
</li>
<li>
<p>Composing a chain that pipes the prompt output into the LLM.</p>
</li>
<li>
<p>Invoking the chain with retrieved documents and the user question to generate a final answer.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Further, it shows how to encapsulate this retrieval and generation logic into a reusable function or runnable (<code>qa</code>) that takes a question, fetches documents, formats the prompt, and returns the answer—optionally including the retrieved documents for inspection.</p>
</div>
<div class="paragraph">
<p>The text highlights that while this basic RAG system works for personal use, production-ready systems require addressing challenges such as handling variable user input quality, routing queries across multiple data sources, translating natural language to query languages, and optimizing indexing (embedding and text splitting).</p>
</div>
<div class="paragraph">
<p>Finally, it previews upcoming discussion on research-backed strategies to optimize RAG system accuracy, summarized in an accompanying figure.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_query_transformation">Query Transformation</h3>
<div class="sect3">
<h4 id="_rewrite_retrieve_read">Rewrite-Retrieve-Read</h4>
<div class="paragraph">
<p>The Rewrite-Retrieve-Read (RRR) strategy improves question answering by first prompting a large language model (LLM) to rewrite a user&#8217;s poorly phrased or distracted query into a clearer, more focused search query. This rewritten query is then used to retrieve relevant documents, which are subsequently passed along with the original question to the LLM to generate an answer.</p>
</div>
<div class="paragraph">
<p>An example shows that a distracted user query containing irrelevant details leads to a failure in retrieving useful information and thus no answer. By contrast, applying the RRR approach—where the LLM rewrites the query before retrieval—results in fetching relevant documents and producing a meaningful answer.</p>
</div>
<div class="paragraph">
<p>This method can be implemented in various programming languages (Python, JavaScript) and works with any retrieval system, including vector stores or web search tools. The main trade-off is increased latency due to the extra LLM call required for rewriting the query.</p>
</div>
</div>
<div class="sect3">
<h4 id="_multi_query_retrieval">Multi-Query Retrieval</h4>
<div class="paragraph">
<p>The multi-query retrieval strategy enhances information retrieval by generating multiple related queries from a user&#8217;s initial question using an LLM. These queries are run in parallel against a data source, and the retrieved documents are combined and deduplicated to form a comprehensive context. This approach helps overcome limitations of single-query retrieval, especially when answers require multiple perspectives.</p>
</div>
<div class="paragraph">
<p>Key points include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Using a prompt template to generate several query variations from the original question.</p>
</li>
<li>
<p>Running all queries in parallel with a <code>.batch</code> method to retrieve relevant documents.</p>
</li>
<li>
<p>Deduplicating documents by their content to avoid repetition.</p>
</li>
<li>
<p>Combining the retrieved documents as context for a final prompt to the LLM to generate a comprehensive answer.</p>
</li>
<li>
<p>Encapsulating the multi-query retrieval logic in a standalone chain (<code>retrieval_chain</code>) for modularity and ease of integration.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Code examples in Python and JavaScript illustrate:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Generating multiple queries.</p>
</li>
<li>
<p>Parallel retrieval and deduplication of documents.</p>
</li>
<li>
<p>Constructing a prompt with combined context.</p>
</li>
<li>
<p>Producing the final answer using the LLM.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>This strategy is particularly useful for complex questions requiring diverse information sources and can be integrated seamlessly into existing QA workflows.</p>
</div>
</div>
<div class="sect3">
<h4 id="_rag_fusion">RAG-Fusion</h4>
<div class="paragraph">
<p>The RAG-Fusion strategy enhances multi-query retrieval by adding a final reranking step using the Reciprocal Rank Fusion (RRF) algorithm. RRF combines ranked document lists from multiple queries into a single unified ranking by scoring documents based on their positions across all lists, effectively promoting the most relevant documents to the top. This approach handles differences in score scales across queries and allows lower-ranked documents to influence the final ranking through a tunable parameter k.</p>
</div>
<div class="paragraph">
<p>The process involves:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Generating multiple search queries from a single user query using a language model prompt.</p>
</li>
<li>
<p>Retrieving relevant documents for each query.</p>
</li>
<li>
<p>Applying the RRF algorithm to fuse and rerank these documents into one consolidated list.</p>
</li>
<li>
<p>Using the reranked documents as context to answer the original question with a language model.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Code examples in Python and JavaScript demonstrate how to implement query generation, document retrieval, RRF reranking, and final question answering in a pipeline. RAG-Fusion improves retrieval by capturing diverse user intents, handling complex queries, and broadening document coverage to enable serendipitous discovery.</p>
</div>
</div>
<div class="sect3">
<h4 id="_hypothetical_document_embeddings">Hypothetical Document Embeddings</h4>
<div class="paragraph">
<p>The document explains the Hypothetical Document Embeddings (HyDE) technique for improving document retrieval in RAG (Retrieval-Augmented Generation) systems. HyDE works by first generating a hypothetical document that answers the user&#8217;s query using a large language model (LLM). This generated passage is then embedded and used to retrieve relevant documents based on vector similarity, as it tends to be closer in embedding space to relevant documents than the original query.</p>
</div>
<div class="paragraph">
<p>The process involves:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Defining a prompt to generate the hypothetical document from the query.</p>
</li>
<li>
<p>Passing the generated document to a retriever to find similar documents in a vector store.</p>
</li>
<li>
<p>Using the retrieved documents as context in a final prompt to the LLM to produce the answer.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Code examples in Python and JavaScript illustrate how to implement each step using LangChain and OpenAI APIs.</p>
</div>
<div class="paragraph">
<p>The document also discusses query transformation strategies, which involve rewriting or decomposing the original user query to improve retrieval. Techniques include removing irrelevant text, grounding queries with conversation history, broadening the search with related queries, and breaking complex questions into simpler ones. The choice of rewriting strategy depends on the use case.</p>
</div>
<div class="paragraph">
<p>Finally, the text hints at the next topic: routing queries to retrieve relevant data from multiple sources to build a robust RAG system.</p>
</div>
<div class="paragraph">
<p>Reference: Gao et al., “Precise Zero-Shot Dense Retrieval Without Relevance Labels,” arXiv, 2022.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_query_routing">Query Routing</h3>
<div class="sect3">
<h4 id="_logical_routing">Logical Routing</h4>
<div class="paragraph">
<p>The text explains the concept of <strong>logical routing</strong> in LLM applications, where the model is given knowledge of multiple data sources and decides which one to use based on the user&#8217;s query. This is achieved using function-calling models like GPT-3.5 Turbo, which generate structured outputs conforming to a predefined schema to classify queries.</p>
</div>
<div class="paragraph">
<p>Key points include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Defining a schema (e.g., with Python&#8217;s Pydantic or JavaScript&#8217;s Zod) that specifies possible data sources (like "python_docs" or "js_docs").</p>
</li>
<li>
<p>Using a prompt that instructs the LLM to route queries based on the programming language referenced.</p>
</li>
<li>
<p>Invoking the LLM to produce JSON outputs indicating the chosen data source.</p>
</li>
<li>
<p>Passing the LLM&#8217;s output into further logic (e.g., a function) to select the appropriate processing chain.</p>
</li>
<li>
<p>Implementing resilience by using case-insensitive substring matching rather than exact string comparisons to handle slight deviations in LLM output.</p>
</li>
<li>
<p>Logical routing is ideal when there is a fixed set of data sources (vector stores, databases, APIs) to choose from for accurate information retrieval.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Overall, logical routing leverages structured function calls to enable LLMs to reason about and select the most relevant data source for a given query, improving the accuracy and reliability of multi-source applications.</p>
</div>
</div>
<div class="sect3">
<h4 id="_semantic_routing">Semantic Routing</h4>
<div class="paragraph">
<p>The content explains <strong>semantic routing</strong>, a method that routes user queries to the most relevant data source by embedding both the query and various prompt templates, then using vector similarity (cosine similarity) to select the closest matching prompt. This approach contrasts with logical routing by leveraging semantic meaning rather than fixed rules.</p>
</div>
<div class="paragraph">
<p>Two example prompts are given—one for physics questions and one for math questions. Both prompts are embedded using OpenAI embeddings. When a user query arrives, it is embedded and compared to the prompt embeddings to find the most similar prompt. The selected prompt is then used to generate a response via a language model (ChatOpenAI).</p>
</div>
<div class="paragraph">
<p>Code examples in Python and JavaScript illustrate this process:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Embedding prompts and queries</p>
</li>
<li>
<p>Calculating cosine similarity</p>
</li>
<li>
<p>Selecting the best prompt based on similarity</p>
</li>
<li>
<p>Passing the routed prompt to a chat model for answering</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The section concludes by transitioning to the next topic: transforming natural language queries into the query language of the target data source in retrieval-augmented generation (RAG) systems.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_query_construction">Query Construction</h3>
<div class="sect3">
<h4 id="_text_to_metadata_filter">Text-to-Metadata Filter</h4>
<div class="paragraph">
<p>The content explains how to perform metadata-filtered vector searches using LangChain&#8217;s <code>SelfQueryRetriever</code>. When embedding data into a vector store, metadata key-value pairs can be attached to vectors. Later, queries can specify filters on this metadata to narrow search results.</p>
</div>
<div class="paragraph">
<p>LangChain’s <code>SelfQueryRetriever</code> simplifies this by leveraging an LLM to convert natural language queries into structured metadata filters and semantic search queries. Users define the metadata schema (fields with names, descriptions, and types), which is included in the prompt to the LLM.</p>
</div>
<div class="paragraph">
<p>The retriever workflow is:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>The LLM receives the user query plus metadata schema and generates a rewritten search query plus metadata filters.</p>
</li>
<li>
<p>The filters are parsed and translated into the vector store’s filter format.</p>
</li>
<li>
<p>A similarity search is performed on the vector store, applying the metadata filters to restrict results.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Code examples in Python and JavaScript demonstrate defining metadata fields (e.g., genre, year, director, rating), initializing the retriever with an LLM and database, and invoking it with a natural language query like “What’s a highly rated (above 8.5) science fiction film?”</p>
</div>
<div class="paragraph">
<p>This approach enables combining semantic search with precise metadata filtering automatically derived from user queries.</p>
</div>
</div>
<div class="sect3">
<h4 id="_text_to_sql">Text-to-SQL</h4>
<div class="paragraph">
<p>The text discusses strategies for translating natural language queries into SQL using large language models (LLMs), emphasizing the importance of grounding SQL generation with accurate database descriptions and few-shot examples. Key points include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Database Description:</strong> Providing the LLM with detailed <code>CREATE TABLE</code> statements (including column names and types) and sample rows helps it generate accurate SQL queries.</p>
</li>
<li>
<p><strong>Few-shot Examples:</strong> Including example question-to-SQL pairs in the prompt improves query generation accuracy by guiding the model on expected outputs.</p>
</li>
<li>
<p><strong>Implementation:</strong> Code examples in Python and JavaScript demonstrate how to create a chain that converts user questions into SQL queries and executes them on a database (e.g., SQLite with the Chinook sample database) using LangChain and OpenAI’s GPT-4.</p>
</li>
<li>
<p><strong>Security Considerations:</strong> Running LLM-generated SQL queries directly on production databases poses risks. Recommended precautions include:</p>
</li>
<li>
<p>Using read-only database users.</p>
</li>
<li>
<p>Restricting access to only necessary tables.</p>
</li>
<li>
<p>Implementing query timeouts to prevent resource exhaustion.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The text highlights that security for LLM-driven database querying is an evolving area requiring ongoing attention.</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_chapter_4_using_langgraph_to_add_memory_to_your_chatbot">Chapter 4. Using LangGraph to Add Memory to Your Chatbot</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_building_a_chatbot_memory_system">Building a Chatbot Memory System</h3>
<div class="paragraph">
<p>The text discusses two fundamental design decisions for building a robust chatbot memory system: how to store state and how to query it. A simple approach is to store the entire chat history as a list of messages, updating it by appending new messages after each turn, and including this history in the prompt to the model. This method enables context-aware responses, as demonstrated with example code in Python and JavaScript using LangChain, where previous conversation messages are passed to the model to answer follow-up questions effectively.</p>
</div>
<div class="paragraph">
<p>However, while this simple memory system works, scaling it for production introduces challenges such as ensuring atomic updates of memory after each interaction, storing memories in durable storage like databases, controlling which and how many messages are retained and used, and enabling inspection and modification of the stored state outside of LLM calls. The text hints at introducing better tooling to address these challenges in subsequent sections.</p>
</div>
</div>
<div class="sect2">
<h3 id="_introducing_langgraph">Introducing LangGraph</h3>
<div class="paragraph">
<p>The chapter introduces LangGraph, an open-source library by LangChain designed to build multiactor, multistep, stateful cognitive architectures called "graphs." These graphs enable complex applications by coordinating multiple specialized actors (e.g., LLM prompts, search engines) working together, much like a team of specialists collaborating.</p>
</div>
<div class="paragraph">
<p>Key concepts explained include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Multiactor:</strong> Multiple actors (nodes) collaborate, passing work among themselves (edges), requiring a coordination layer to define actors, manage handoffs, and schedule execution deterministically.</p>
</li>
<li>
<p><strong>Multistep:</strong> Interactions occur over multiple discrete steps, tracking the order and frequency of actor calls, with each handoff scheduling the next computation step until completion.</p>
</li>
<li>
<p><strong>Stateful:</strong> A central shared state tracks data across steps, allowing snapshotting, pausing/resuming execution, error recovery, and human-in-the-loop controls.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>A LangGraph graph consists of:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>State:</strong> Data input and modified during execution.</p>
</li>
<li>
<p><strong>Nodes:</strong> Functions representing steps that receive and update state.</p>
</li>
<li>
<p><strong>Edges:</strong> Connections between nodes, either fixed or conditional, defining execution flow.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>LangGraph provides visualization and debugging tools and supports scalable production deployment. Installation instructions for Python and JavaScript are given.</p>
</div>
<div class="paragraph">
<p>To demonstrate LangGraph, the chapter plans to build a simple chatbot that responds directly to user messages, illustrating core LangGraph concepts with a single LLM call.</p>
</div>
</div>
<div class="sect2">
<h3 id="_creating_a_stategraph">Creating a StateGraph</h3>
<div class="paragraph">
<p>The content explains how to build a simple chatbot using LangGraph by defining a state, adding a node for an LLM call, connecting nodes with edges, and running the graph.</p>
</div>
<div class="paragraph">
<p>Key steps:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Define the Graph State</strong></p>
<div class="ulist">
<ul>
<li>
<p>The state is a schema describing the data the graph manages.</p>
</li>
<li>
<p>Here, the state has a single key <code>messages</code>, a list of chat messages.</p>
</li>
<li>
<p>A reducer function (<code>add_messages</code>) is used to append new messages instead of overwriting the list.</p>
</li>
<li>
<p>This is shown in both Python (using <code>Annotated</code> and <code>TypedDict</code>) and JavaScript (using <code>Annotation</code> with a reducer).</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Add a Chatbot Node</strong></p>
<div class="ulist">
<ul>
<li>
<p>Nodes represent units of work, typically functions.</p>
</li>
<li>
<p>The <code>chatbot</code> node takes the current state, calls an LLM (<code>ChatOpenAI</code>), and returns new messages to append.</p>
</li>
<li>
<p>This updates the state by adding the LLM’s response to the <code>messages</code> list.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Add Edges to Define Execution Flow</strong></p>
<div class="ulist">
<ul>
<li>
<p>Edges connect nodes and define the start and end of the graph execution.</p>
</li>
<li>
<p>The graph starts at <code>START</code>, runs the <code>chatbot</code> node, then ends at <code>END</code>.</p>
</li>
<li>
<p>The graph is compiled into a runnable object with <code>invoke</code> and <code>stream</code> methods.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Visualize the Graph</strong></p>
<div class="ulist">
<ul>
<li>
<p>The graph can be visualized using a Mermaid diagram.</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Run the Graph</strong></p>
<div class="ulist">
<ul>
<li>
<p>Input is provided as a dictionary/object matching the state shape (e.g., <code>{"messages": [HumanMessage('hi!')]}</code>).</p>
</li>
<li>
<p>The <code>stream()</code> method runs the graph and streams state updates after each step.</p>
</li>
<li>
<p>Output shows the chatbot’s AI message appended to the messages list.</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>Overall, this example demonstrates how to build a simple chatbot workflow with LangGraph by defining state, nodes, edges, and running the graph with streaming output.</p>
</div>
</div>
</div>
</div>
</div>
<div id="footer">
<div id="footer-text">
Last updated 2025-05-05 10:07:33 +0300
</div>
</div>
</body>
</html>